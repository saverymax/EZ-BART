{
    "CD004161-0": {
        "query": "How does risperidone depot compare with other depot antipsychotics in people with schizophrenia?",
        "document": "Background Description of the condition Schizophrenia is a major, often chronic, psychiatric disease that close to seven people in every 1000 will be affected by at some point during their lifetime ( McGrath 2008 ). Antipsychotic drugs are effective for treating acute episodes and for preventing relapse ( Davis 1977 ; Davis 1986 ). These drugs are usually given orally, but compliance is poor and ranges from 20% to 89% with an average of 50% ( Fenton 1997 ; Young 1986 ). This means that, on average, half the patients treated with these drugs will not comply with prescribed medication. This is probably due to a combination of various factors such as the erosion of insight that accompanies psychotic illnesses, adverse effects and human nature. Description of the intervention Long\u2010acting depot antipsychotics, given by injection into the muscle, should be helpful in increasing compliance with medication. In studies comparing one depot with another, attrition rates are markedly lower than in studies comparing oral preparations, but in trials comparing an oral with a depot preparation, there are no differences in the attrition rates between groups ( Adams 2001 ). This is likely to be due to a limitation in the design of the relevant studies, as people participating in randomised trials are more likely to be compliant. This is an area where 'real world' or 'pragmatic' randomised trials are indicated. The newer generation of antipsychotics, often called atypical, seem to cause less of the movement disorders associated with older drugs. This group of compounds may be equally clinically effective ( Small 1997 ), and be more acceptable to people with schizophrenia, than older drugs such as haloperidol ( Leucht 1999 ; Marder 1994 ; Tollefson 1997 ) although this is disputed ( Geddes 2000 ). Atypical drugs have gained popularity amongst clinicians but, along with their cost, a lack of a depot preparation has been cited as a significant obstacle to their frequent use ( Sarfati 1999 ). Risperidone is an atypical antipsychotic, first made available for the care of those with schizophrenia in 1986. Since then clinical trials have been conducted to evaluate its efficacy and safety and studies have indicated that it may be superior to older drugs ( Marder 1994 ). When oral risperidone is compared with haloperidol, it appears to have marginal benefits in terms of clinical improvement and is less likely to cause movement disorders ( Hunter 2003 ). Risperidone is the first newer drug to be available in a long\u2010acting injection formulation. How the intervention might work Risperidone is one of the new or second\u2010generation \"atypical\" antipsychotics, developed in the late 1980s. It is known to block dopamine D2 and 5HT2 (serotonin) receptors in the brain, with a high ratio of 5HT2 to D2 blockade. It also blocks alpha1 and alpha2 adrenoceptors, H1 receptors and has no effect on beta adrenoceptors, muscarinic cholinoceptors or peptidergic receptors ( Janssen 1988 ). The depot formulation of risperidone has unmodified risperidone encapsulated in biodegradable polymer microspheres, which are then suspended in an aqueous solution. Once the microspheres are injected into the muscle, the polymers begin to degrade and the drug is released at a set rate. This takes place over the course of several weeks, with the highest plasma concentration occurring approximately one month after injection ( Ramstack 2003 ). Why it is important to do this review In terms of the costs of schizophrenia, this was estimated at about \u00a36.7 billion in England in 2004/05, of which the direct costs were \u00a32 million while the indirect costs accounted for the rest ( Mangalore 2007 ). The cost of risperidone (depot) itself is expensive compared to other typical antipsychotics, at \u00a3142.76 for a 50 mg vial. The maximum monthly dose of risperidone (depot) is 100 mg per month, which costs \u00a3285.52 per month ( BNF 2012 ). These newer, atypical antipsychotics in comparison are more expensive than typical antipsychotics, with olanzapine available at \u00a313.11 for 28 x 5 mg tablets, and clozapine (Clozaril) at \u00a321.56 for 28 x 100 mg tablets. It is important to complement the clinical effectiveness of risperidone (depot) with its cost\u2010effectiveness. Davies et al. ( Davies 2007 ) conducted a study on cost\u2010effectiveness of first\u2010generation antipsychotics (i.e. flupentixol, trifluoperazine, chlorpromazine) and the second\u2010generation antipsychotics (i.e. risperidone, olanzapine, amisulpiride). The study findings argue that there is no evidence to suggest that atypical (second\u2010generation) antipsychotics are more cost\u2010effective than typical (first\u2010generation) antipsychotics.",
        "summary": "When risperidone depot (25 to 50 mg every two weeks or mean two\u2010weekly dose of 29.8 mg) was compared with paliperidone palmitate depot (25 to 150 mg monthly or mean monthly dose of 115.8 mg) in adults with schizophrenia, fewer people withdrew from treatment due to lack of efficacy (on average, 307 versus 361 per 1000 people; low\u2010quality evidence) or experienced any adverse effect beyond 26 weeks (on average, 217 versus 293 per 1000 people) with risperidone. However, more people experienced tremor at 13 to 26 weeks (on average, 179 versus 105 per 1000 people) and hyperkinesia beyond 26 weeks (on average, 101 versus 61 per 1000 people) and required medication for extrapyramidal symptoms beyond 26 weeks (on average, 178 versus 122 per 1000 people; moderate\u2010quality evidence). No differences were detected between the two depot therapies in terms of change or improvement in mental state, change in general functioning or mortality. None of the trials reported on relapse, change in behavior, successful suicide, time to relapse, hospitalization or serious adverse effects. The trials that compared risperidone depot with fluphenazine decanoate or haloperidol decanoate depot were too small to provide clinically meaningful results."
    },
    "CD003817": {
        "query": "What are the effects of exercise in people who are overweight or obese?",
        "document": "Background Description of the condition Overweight and obesity are conditions of excess body fat ( NHMRC 1997 ). The World Health Organisation (WHO) defines weight status according to body mass index (BMI), the ratio of weight (in kilograms) divided by height (in metres squared). A BMI of 20 to 25.9 defines normal weight, 25 to 29.9 defines overweight and equal to or greater than 30 defines obesity ( WHO 2003 ). Overweight and obesity are a major public health problem with more than one billion adults overweight globally, 300 million of which have obesity ( WHO 2006 ). Increased consumption of more energy\u2010dense, nutrient\u2010poor foods with high levels of sugar and saturated fats, combined with reduced physical activity, have led to the increase in prevalence ( WHO 2003 ). Overweight and obesity pose a major risk for serious chronic diseases, including type 2 diabetes, cardiovascular disease, hypertension and stroke, and certain forms of cancer. Effective weight management for individuals and groups with overweight and obesity involves a range of strategies including reducing energy intake through dietary change and increasing energy expenditure by increasing physical activity levels ( WHO 2003 ). Description of the intervention Although evidence supporting the efficacy of exercise to achieve weight loss is disappointing, studies do support the efficacy of exercise to prevent weight gain. A number of large studies, including the Reno diet\u2010heart study, the \"First National Health Nutrition and Examination Survey\" (NHANES\u20101) and the Canada Fitness survey have found a negative association between physical activity and weight gain ( Foreyt 1995 ; Williamson 1993 ; Tremblay 1986 ; Tremblay 1990 ). These studies were large\u2010scale cross\u2010sectional and longitudinal studies. The impact of a number of vigorous and non\u2010vigorous leisure activities on weight was examined. People who were habitually more active were found to be less obese. Therefore increasing physical activity, both exercise and habitual activity, may have a role in preventing obesity, preventing worsening of already established obesity, and reducing body mass in obese people. How the intervention might work Even if exercise does not result in weight loss, it confers significant health benefits to people with overweight and obesity. Blood lipid profiles associated with increased risk of coronary heart disease are a common metabolic feature of obesity. Since the early 1980s there has been increasing evidence that central fat accumulation has an adverse action on lipids, resulting in elevated triglycerides and very\u2010low\u2010density lipoproteins and low levels of high\u2010density lipoproteins ( Despres 1994 ). Exercise, with or without weight loss, improves plasma lipoprotein status, in particular, increasing high\u2010density lipoproteins therefore may be of particular benefit to people who are abdominally obese even if no weight is lost by exercising. Similarly, large cross\u2010sectional studies demonstrate reduction in blood pressure in those who regularly exercise, compared with sedentary persons, irrespective of weight ( Montoye 1972 ; Sandvik 1993 ). The large cohort Harvard alumni study, showed that those who engaged in regular vigorous leisure activities had a 33 percent lower risk (relative risk reduction) of developing hypertension and 41 percent reduction (relative risk reduction) in mortality from coronary heart disease over 20 years ( Paffenbarger 1983 ). Exercise interventions ideally should be used in the context of a multi\u2010component weight loss program to gain their maximum benefit. Diet and exercise combined with psychological interventions comprise an intuitively powerful weight loss program ( NHLBI 1998 ). However, in spite of the increased comprehensiveness of weight loss programs and improvements in patient education, understanding of the role of diet and exercise in weight loss, psychological interventions, and improved pharmacotherapies for weight reduction, results of weight loss trials have continued to remain disappointing ( Liao 2000 ). There are still major gaps in our understanding of the roles of diet, exercise, and psychological therapies in weight reduction. Also, achieving long\u2010term modification of food intake and food type by the obese individual without creating decreases in energy expenditure associated with dieting, and dealing with relapse to pre\u2010intervention diet and exercise behaviours are ongoing challenges ( Brownell 1986 ). Studies examining the magnitude of weight loss achievable with exercise have shown disappointing results. Garrow and Summerbell, in a meta\u2010analysis of 28 studies of exercise and weight loss, concluded that weight lost in exercise programs without caloric restriction is small and usually ranges from 2 to 7 kg ( Garrow 1995 ). Ballor and Keesey, in an earlier meta\u2010analysis, also found that weight loss associated with exercise was modest ( Ballor 1991 ). However, considerable research has been performed in the area since these meta\u2010analyses were performed. This review aimed to clarify the effect of exercise on body weight and health in people with overweight and obesity, using high quality criteria to assess and summarise the evidence.",
        "summary": "There is some randomized controlled trial data suggesting that exercise with or without diet offers modest benefits for weight loss in overweight or obese people. The average weight loss noted in people doing exercise compared with no treatment was \u22122.03 kg (95% CI \u22122.82 kg to \u22121.23 kg) at 1 year and in people doing exercise plus diet compared with diet alone the average weight loss was \u22120.56 kg (95% CI \u22120.76kg to \u22120.36kg) at 3 months to 1 year. Exercise alone may be less effective than diet alone at 3 months to 1 year (mean difference \u22123.61 kg, 95% CI \u22122.95kg to \u22124.26 kg in people dieting. There is not enough randomized controlled trial evidence to comment on improvement in blood pressure and lowering of cardiovascular events with exercise in overweight and obese people. There were no data on quality of life. Assessments of higher intensity exercise found mixed results but suggest that there may only be a weight loss benefit in higher intensity over lower intensity exercise in people who do not make any dietary changes."
    },
    "CD008214": {
        "query": "How does intravitreal bevacizumab affect outcomes after vitrectomy for proliferative diabetic retinopathy?",
        "document": "Background Description of the condition Pars plana vitrectomy is an established and successful treatment for the complications of proliferative diabetic retinopathy (PDR) ( Ho 1992 ; McLeod 1991 ). Up to 10% of people presenting with PDR require the treatment within one year ( Kaiser 2000 ). The most common indication for surgery is non\u2010clearing vitreous haemorrhage ( Ho 1992 ; McLeod 1991 ). Unfortunately, postoperative vitreous cavity haemorrhage (POVCH) is a significant complication occurring in approximately 20% to 30% of cases, although the reported range is large: 5% to 80% of cases ( Benson 1988 ; Blankenship 1986 ; Liggett 1987 ; Novak 1984 ; Tolentino 1989 ; Virata 2001 ; Yorston 2008 ). Although there can be overlap, POVCH occurs in two main forms. Early POVCH, being present from the first few postoperative days and delaying visual recovery by non\u2010clearance. Late POVCH, occurring later during follow\u2010up, commonly at two to six months postoperatively, after a postoperative period during which the vitreous cavity was clear. Early POVCH, being present from the first few postoperative days and delaying visual recovery by non\u2010clearance. Late POVCH, occurring later during follow\u2010up, commonly at two to six months postoperatively, after a postoperative period during which the vitreous cavity was clear. Persistent haemorrhage can result from operative and postoperative oozing of the remnants of new vessels or dissected tissue, or directly from the sclerostomies used to perform surgery. It can also occur from clot lysis in the first few postoperative days. Leaching of red blood cells can occur from retained old haemorrhages in residual anterior vitreous, causing apparent POVCH ( McLeod 1991 ; Novak 1984 ; Tolentino 1989 ). Recurrent haemorrhage can result from late haemorrhage from dissected tissue, recurrent traction on residual new vessels, or indeed postoperative new vessel growth in the posterior retina. Recent studies have shown that a common cause of recurrent haemorrhage is new anterior vessel growth at the inner sclerostomy sites associated with fibrous traction ( Bhende 2000 ; Hershberger 2004 ; Hotta 2000 ; Kreiger 1993 ; Sawa 2000 ; Steel 2008 ; West 2000 ). Termed \u2018entry site neovascularisation' (ESNV) ( McLeod 2000 ), this is thought to be an aberrant wound\u2010healing response related to the presence of retinal and pars plana ischaemia ( Kreiger 1993 ; Yeh 2005 ). The presence of ESNV is difficult to observe clinically because of the extreme anterior location, but can be confirmed at the time of revision surgery with deep scleral indentation or endoscopic techniques ( West 2000 ). It can also be localised, with anterior segment high\u2010resolution ultrasonography of the inner sclerostomy sites ( Bhende 2000 ; Hershberger 2004 ; Hotta 2000 ; Steel 2008 ). People with POVCH suffer a delay in their visual recovery which, in some cases, results in a worse level of visual acuity than preoperatively. Intraocular pressure can become raised from trabecular meshwork obstruction. If maculopathy is present, then additional laser treatment cannot be applied, with the risk of worsening foveal function in the long term. The initial treatment for POVCH is observation ( Tolentino 1989 ). Spontaneous clearance occurs in many cases. Because they are no longer trapped in the gel structure of the vitreous, red blood cells can circulate and clear more freely from the vitreous cavity. Clearance is related to the amount of haemorrhage, its frequency, and the degree of communication between anterior and posterior segments, allowing red blood cells to enter the anterior chamber and be cleared via the trabecular meshwork ( McLeod 2000 ). Non\u2010clearing POVCH necessitates revision surgery in approximately one\u2010third to one\u2010half of those who experience POVCH and approximately 10% of all patients undergoing surgery ( Blankenship 1986 ; Blumenkranz 1986 ; Brown 1992 ; Han 1991 ; Martin 1992 ; Novak 1984 ; Schachat 1983 ; Tolentino 1989 ). Non\u2010clearing POVCH is treated with surgery to remove haemorrhage and to treat any underlying cause that may have been identified. The timing of the surgery will depend on a variety of factors, including social situation, fellow\u2010eye status, maculopathy needing treatment, intraocular pressure, etc. In some patients, spontaneous clearing or revision surgery is followed by further haemorrhage, frustrating visual rehabilitation further, especially if this occurs in the better eye. Description of interventions to reduce the incidence of POVCH and how the interventions may work At the time of the initial vitrectomy surgery, several strategies may have been used to prevent POVCH and avoid the need for repeat surgery. We can divide these into two main groups. Established surgical interventions that are generally regarded as standard clinical practice. Ensuring adequate haemostasis at the time of vitrectomy with laser or intraocular bipolar coagulation to reduce postoperative oozing of dissected blood vessels. Removal of peripheral haemorrhagic vitreous to reduce leaching of sequestered red blood cells postoperatively into the vitreous cavity. Identification and removal of all posterior vitreoretinal traction. Vitreoschisis is also known to occur in people with PDR, and identification of this and dissection in the true vitreoretinal plane are important in order to avoid recurrent traction and postoperative bleeding from neovascular tissue ( Schwatz 1996 ). Applying supplementary posterior panretinal photocoagulation if required. Other strategies to prevent POVCH that have been reported but not routinely adopted. Established surgical interventions that are generally regarded as standard clinical practice. Ensuring adequate haemostasis at the time of vitrectomy with laser or intraocular bipolar coagulation to reduce postoperative oozing of dissected blood vessels. Removal of peripheral haemorrhagic vitreous to reduce leaching of sequestered red blood cells postoperatively into the vitreous cavity. Identification and removal of all posterior vitreoretinal traction. Vitreoschisis is also known to occur in people with PDR, and identification of this and dissection in the true vitreoretinal plane are important in order to avoid recurrent traction and postoperative bleeding from neovascular tissue ( Schwatz 1996 ). Applying supplementary posterior panretinal photocoagulation if required. Other strategies to prevent POVCH that have been reported but not routinely adopted. Applying additional anterior retinal photocoagulation up to the ora serrata to ablate retro\u2010oral ischaemic retina and reduce the production of postoperative neovascular growth factors ( Liggett 1987 ; Mason 1978 ; Yeh 2005 ). Direct treatment to the sclerostomy sites themselves with either cryotherapy or laser ( Yeh 2005 ). This is thought to reduce the occurrence of entry site neovascularisation by inhibiting cellular migration through the sclerostomy wounds and causing focal atrophy of the ciliary epithelia. Removal of Wieger\u2019s ligament and thorough anterior vitrectomy, especially around the inner sclerostomy wounds ( McLeod 2000 ; McLeod 2003 ). This may be effective by: increasing the egress of red blood cells and growth factors from the posterior segment to the anterior segment for rapid clearance of any POVCH; reducing any putative concentration of growth factors around the sclerostomy sites themselves; removing the vitreous scaffold along which anterior new vessels could grow. Applying additional anterior retinal photocoagulation up to the ora serrata to ablate retro\u2010oral ischaemic retina and reduce the production of postoperative neovascular growth factors ( Liggett 1987 ; Mason 1978 ; Yeh 2005 ). Direct treatment to the sclerostomy sites themselves with either cryotherapy or laser ( Yeh 2005 ). This is thought to reduce the occurrence of entry site neovascularisation by inhibiting cellular migration through the sclerostomy wounds and causing focal atrophy of the ciliary epithelia. Removal of Wieger\u2019s ligament and thorough anterior vitrectomy, especially around the inner sclerostomy wounds ( McLeod 2000 ; McLeod 2003 ). This may be effective by: increasing the egress of red blood cells and growth factors from the posterior segment to the anterior segment for rapid clearance of any POVCH; reducing any putative concentration of growth factors around the sclerostomy sites themselves; removing the vitreous scaffold along which anterior new vessels could grow. Some clinicians advocate simultaneous or even pre\u2010emptive cataract surgery to achieve these aims and reduce the risk of POVCH ( Schiff 2007 ). Agents with physical actions thought to possibly reduce the rate of POVCH inserted into the eye during surgery, such as air ( Joondeph 1989 ), gas ( Koutsandrea 2001 ; Yang 2007 ), or viscoelastic substances ( Packer 1989 ). Agents with physical actions thought to possibly reduce the rate of POVCH inserted into the eye during surgery, such as air ( Joondeph 1989 ), gas ( Koutsandrea 2001 ; Yang 2007 ), or viscoelastic substances ( Packer 1989 ). The vascular endothelial growth factor (VEGF) inhibitor bevacizumab has been used preoperatively to reduce vascular proliferation and reduce the vascularity of neovascular tissue ( Romano 2009a ; Yang 2008 ). The vascular endothelial growth factor (VEGF) inhibitor bevacizumab has been used preoperatively to reduce vascular proliferation and reduce the vascularity of neovascular tissue ( Romano 2009a ; Yang 2008 ). Triamcinolone has been used intraoperatively by intraocular injection to reduce inflammation and vascular proliferation ( Faghihi 2008 ). Bevacizumab has been used intraoperatively to reduce vascular proliferation following surgery ( Romano 2009b ). Triamcinolone has been used intraoperatively by intraocular injection to reduce inflammation and vascular proliferation ( Faghihi 2008 ). Bevacizumab has been used intraoperatively to reduce vascular proliferation following surgery ( Romano 2009b ). Oral tranexamic acid, which inhibits fibrinolysis and hence clot dissolution, administered to the patient postoperatively ( Laatikainen 1987 ; Ramezani 2005 ). Oral tranexamic acid, which inhibits fibrinolysis and hence clot dissolution, administered to the patient postoperatively ( Laatikainen 1987 ; Ramezani 2005 ). Why it is important to do this review Diabetes mellitus is an increasing health problem. It is estimated that 6% (3.2 million) of the UK population are currently diabetic and that this will increase to 5 million by 2025. Ten per cent (10 billion UK pounds) of the total NHS budget is spent treating the acute costs of diabetes and its complications and this is predicted to increase to 17 billion by 2035 ( Diabetes UK 2014 ). Diabetic retinopathy is one of the leading cause of blindness in the working age group in the UK accounting for 14.4% of blind and partially sighted registrations ( Liew 2014 ). Within 20 years of diagnosis nearly all people with Type 1 and almost two thirds of people with Type 2 diabetes have some degree of retinopathy ( Scanlon 2008 ) and after 15 years, 30% and 10% of type 1 and type 2 diabetics respectively develop PDR ( Klein 1984a ; Klein 1984b ). These patients are at risk of severe visual loss resulting from the complications of PDR. A study of patients at a large eye unit in the USA suggested that approximately 10% of patients presenting with PDR require vitrectomy surgery within one year ( Kaiser 2000 ). Estimates based upon data from our region ( Vaideanu 2014 ) suggest that approximately 4000 vitrectomies for the complications of PDR are currently performed annually in the UK. If 10% require revision surgery, this equates to 400 patients per annum in the UK. POVCH after vitrectomy surgery is certainly a significant problem and its occurrence is distressing for patients as well as delaying visual recovery. Repeat interventions expose the patient to further operative risk and anxiety and add to the overall cost of care. The use of anti\u2010VEGF treatment is becoming increasingly common for many ophthalmic conditions and has significant revenue consequences. The result of anti\u2010VEGF use, both preoperatively and intraoperatively, to reduce the occurrence of POVCH is uncertain, and a review of the literature will aid in optimum patient management and design of future studies. The use of anti\u2010VEGF treatment is becoming increasingly common for many ophthalmic conditions and has significant revenue consequences. However, the effectiveness of anti\u2010VEGF use, both preoperatively and intraoperatively, in reducing the occurrence of POVCH is uncertain. A review of the literature will aid in optimum patient management and in the design of future studies.",
        "summary": "There is high\u2010quality evidence that intravitreal bevacizumab (IVB), given pre\u2010 or peri\u2010operatively reduces the rate of vitreous hemorrhage in the first 4 weeks after a first vitrectomy for diabetic vitreous hemorrhage (on average 109 versus 310 with post\u2010operative hemorrhage per 1000 people). This was reflected in a reduced need to perform vitreous wash out for a re\u2010bleed with IVB (15 versus 70 per 1000 people). This would be important in improving the rate of visual rehabilitation and reducing the cost and complications involved in repeat surgery. An effect of IVB on late vitreous hemorrhage and vision (\u22656 months) was not demonstrated, but the numbers of participants in the analyses was much smaller, and the evidence low\u2010quality. There was no apparent increase in the rate of serious complications related to IVB, but event rates were low and the trials too small to detect differences between groups even if these were present. Only retinal detachment was reported consistently across trials, but the evidence for this outcome was low\u2010quality. The groups included were fairly heterogeneous in terms of age and activity of proliferative retinopathy and it is possible that one subgroup may benefit more than another. Trials involving IVB were the only ones that met the inclusion criteria for the review. Trails of other VEGF inhibitors are on\u2010going and will further inform us on the benefit of these drugs."
    },
    "CD006469": {
        "query": "In women with metastatic and recurrent cervical cancer, how do different types and combinations of chemotherapy affect outcomes?",
        "document": "Background Description of the condition Cervical carcinoma arises in the uterine cervix and 60% to 70% of cases are squamous cell carcinomas, approximately 25% to 30% are adenocarcinomas or adenosquamous carcinomas, with a very small number of rarer cancers, such as small cell tumours and neuroendocrine tumours ( Jemal 2008 ; Meta\u2010analysis Collaboration 2008 ). Cervical cancer is the second most common cancer among women aged up to 65 years and is the most frequent cause of death from gynaecological cancers worldwide. A woman's risk of developing cervical cancer by 65 years of age ranges from 0.69% in developed countries to 1.38% in developing countries ( GLOBOCAN 2008 ). In Europe, about 60% of women with cervical cancer are alive five years after diagnosis ( EUROCARE\u20103 ). The stage of disease at diagnosis determines survival rates; lesions less than 4 cm in diameter and confined to the cervix (FIGO (International Federation of Gynecology and Obstetrics) Stage Ib1) at presentation have a five\u2010year survival rate of 92%, while women with cancer spread beyond the true pelvis to adjacent organs (FIGO Stage IVa) have a five\u2010year survival rate of only 17% ( Jemal 2008 ; Meta\u2010analysis Collaboration 2008 ). The overwhelming risk factor for development of cervical cancer is the presence of human papilloma virus (HPV), particularly subtypes 16 and 18. Vaccines against HPV have become available and introduced in some countries. In the future it is hoped this will significantly reduce the incidence of cervical cancer. HPV causes the cervical epithelium to become increasingly abnormal (graded as cervical intra\u2010epithelial neoplasia (CIN) grades I to III), this then becomes invasive in 30% to 70% of women over 10 to 12 years, although this process can be much faster in a small minority (< one year). Invasive cancer then spreads directly by invading adjacent structures and metastasising via regional lymph nodes and, less commonly, via the bloodstream to distant sites such as the lungs. Description of the intervention Early\u2010stage cancer confined to the cervix, or with extension into upper vagina (Stage I to IIa), can be successfully treated by radical surgery (with or without neoadjuvant chemotherapy) or concomitant chemoradiation, giving five\u2010year survival rates of 80% to 90% ( Eifel 2001 ). A European Organisation for Research and Treatment of Cancer (EORTC) trial (EORTC 55994) of chemoradiotherapy versus neoadjuvant chemotherapy plus surgery is on\u2010going. For more advanced cancer, Stages IIb to IVa the treatment of choice is chemoradiotherapy; as a Cochrane meta\u2010analysis has shown that the addition of platinum\u2010based chemotherapy significantly improves survival compared to radiotherapy alone ( Green 2005 ). Combined chemoradiotherapy will cure 50% to 70% of patients with locally advanced carcinoma of the cervix ( Meta\u2010analysis Collaboration 2008 ; Willmott 2009 ) and can be used for local control of metastatic disease. Palliative chemotherapy is used for the management of Stage IVb patients whose disease has spread to distant sites, or for patients with inoperable recurrent or persistent disease ( de la Motte Rouge 2006 ; Pectasides 2008 ). A number of chemotherapeutic options have been used either as single agents or in combination including cisplatin, adriamycin, ifosfamide, paclitaxel, irinotecan, topotecan, vinorelbine and gemcitabine with responses in the range of 15% to 46% ( Moore 2006 ; Tewari 2005 ). How the intervention might work The aim of chemotherapy is to slow cancer growth, improve survival with minimal toxicity so there is an improvement in quality of life (QoL). Why it is important to do this review There are no standard treatments for patients with persistent, recurrent or metastatic cervical cancer. In this group of patients, who are often young and otherwise fit and well, it is important to establish chemotherapy regimens that have the greatest chance of response with tolerable side effects and an improvement in QoL. In the era of chemoradiation as primary management for locally advanced cervical cancer the role of chemotherapy in this setting becomes even more important to establish.",
        "summary": "Data regarding different types and combinations of chemotherapy for women with metastatic and recurrent cervical cancer are sparse, as only old trials with small sample sizes are available. Results for most comparisons are uncertain but cisplatin in combination with other agents seems more effective than cisplatin alone at improving response rate (5 trials; 1100 participants) although it causes more toxicity (neutropenia, thrombocytopenia and infection). Cisplatin plus paclitaxel also seems to increase response rate when compared with cisplatin plus non\u2010taxane (2 trials; 600 participants)."
    },
    "CD011802": {
        "query": "What are the effects of stepping down the dose of inhaled corticosteroids for adults with asthma?",
        "document": "Background Description of the condition Asthma is a condition of the airways affecting adults and children. The number of diagnoses worldwide is estimated at more than 300 million ( Global Asthma Network 2014 ; Partridge 2006 ). During an asthma attack (exacerbation), narrowing of the airways and excess mucus production occurs, causing symptoms of chest tightness, wheezing and breathlessness. Lung function tests typically show airflow obstruction with a low peak expiratory flow rate (PEFR), low forced expiratory volume in one second (FEV 1 ) and a low FEV 1 /forced vital capacity (FVC) ratio ( SIGN/BTS 2016 ). Lung function abnormalities improve and function may return to normal with treatment. Variability in measures of airflow is the hallmark of asthma. Exacerbations of asthma can be triggered by environmental stimuli. In immunoglobulin E (IgE)\u2010mediated asthma (which may account for half of asthma cases) ( Pearce 1999 ), indoor inhaled allergens such as house dust mite, cat and dog are often implicated ( Custovic 2012 ). Other recognised environmental stimuli include air pollutants such as ozone and fine particulates, active and passive exposure to tobacco smoke ( Xepapadaki 2009 ), industrial chemicals such as phthalates ( Jaakkola 2008 ), isocyanates ( Fisseler\u2010Eckhoff 2011 ), viral infections and cold air. Description of the intervention Acute episodes of asthma are treated with reliever therapy, usually a short\u2010acting beta 2 \u2010agonist (SABA). Inhaled corticosteroids (ICS) are used widely as first\u2010line therapy for patients with asthma that is uncontrolled on reliever therapy alone ( SIGN/BTS 2016 ). Inhaled corticosteroids, which effectively relieve symptoms and prevent asthma exacerbations ( Adams 2005 ; Adams 2008 ), are preferable to treatment by the oral route, as they lead to lower systemic absorption and fewer side effects. However, economic and social factors may contribute to non\u2010compliance with inhaler\u2010based therapies in some low\u2010 and middle\u2010income countries ( GINA 2016 ). A variety of devices are available for delivery of ICS at differing doses and particle sizes. Generally, ICS are taken twice daily, although some newer preparations are taken once daily. For patients with persistent asthma, ICS are often taken alongside a long\u2010acting beta 2 agonist (LABA), sometimes via a combination inhaler. ICS should be commenced at a dose appropriate to disease severity and control. National and international guidelines recommend titrating up the dose of ICS to gain symptom control at the lowest possible dose. Long\u2010term use of higher doses of ICS carries risk of systemic adverse events (i.e. side effects caused by the action of the steroid at sites other than the intended target \u2010 the airways) ( Lipworth 1999 ); however, lower doses of up to 800 mcg per day of beclomethasone dipropionate are considered tolerable ( SIGN/BTS 2016 ). For patients whose asthma symptoms are controlled on moderate or higher doses of ICS, it may be possible to reduce the dose of ICS without compromising symptom control ( Hawkins 2003 ). How the intervention might work ICS offer effective treatment for asthma owing to their anti\u2010inflammatory and decongestive effects on bronchial airways ( Tse 1984 ). LABA function by decreasing bronchial hyperreactivity to physical and chemical stimuli and by relaxing bronchial smooth muscle ( Lipworth 1992 ). Guidelines for asthma treatment focus on achieving, then maintaining, control while balancing the risks associated with long\u2010term medication ( Bateman 2008 ). Once asthma control is achieved (e.g. as per GINA 2016 criteria), guidelines recommend 'stepping down' treatment to the lowest possible dose of ICS ( SIGN/BTS 2016 ). These recommendations are based on known risks of systemic adverse effects (e.g. loss of bone density in adults, growth retardation in children) associated with long\u2010term use of high\u2010dose ICS ( Colice 2006 ; Lipworth 1999 ; SIGN/BTS 2016 ). Why it is important to do this review Patients with persistent asthma are generally treated with a high dose of ICS or with a combination of ICS and LABA ( Ducharme 2010 ). Two separate Cochrane reviews ( Ahmad 2015 ; Kew 2015 ) have synthesised the evidence for removing the LABA from the ICS/LABA combination when treating children and adults with asthma. Stepping down the dose of ICS may reduce the likelihood of unwanted side effects, particularly the systemic side effects of steroid use ( Colice 2006 ; SIGN/BTS 2016 ). Indeed, current British Thoracic Society (BTS)/Scottish Intercollegiate Guidelines Network (SIGN) guidelines recommend that ICS should be titrated to the lowest possible dose at which effective asthma control is maintained ( SIGN/BTS 2016 ). However, debate continues regarding the best protocol for stepping down ICS treatment, particularly with respect to the lowest acceptable dose of ICS and the rate of down\u2010titration ( Rogers 2012 ). Therefore, synthesis of the evidence for 'stepping down ICS therapy' is important. Finally, ICS are among the most widely prescribed repeat medications and thus account for a substantial proportion of drug spending in the United Kingdom and in other countries ( NHS 2013 ). Any strategy to reduce the use of ICS may thus represent an important cost\u2010saving measure.",
        "summary": "Evidence of very low to low quality failed to detect clear differences in asthma control, lung function, exacerbations requiring oral steroids, or all\u2010cause serious adverse events when inhaled corticosteroids (ICSs) were reduced (50% to 60%) for people with asthma who were also treated with or without concomitant long\u2010acting beta\u2010agonists (LABAs). However, rates of exacerbation and serious adverse events were low, and the numbers of participants for all analyses too small to presume equivalence of these regimens for these outcomes. Quality of life appeared to be worse with ICS dose reduction when no LABAs were used, but similar in both groups when LABAs were used. However, reviewers used different scales to measure quality of life, and the quality of evidence is low to very low. No firm conclusions can be drawn."
    },
    "CD001390": {
        "query": "What are the effects of injectable vaccines for preventing pneumococcal infection in people with chronic obstructive pulmonary disease?",
        "document": "Background Description of the condition Chronic obstructive pulmonary disease (COPD) is characterised by airflow obstruction that is not fully reversible. Data from 12 countries in the Burden of Lung Disease (BOLD) initiative show that more than 10% of adults have COPD at Stage II or higher, as defined by GOLD 2016 . Prevalence and staging vary across countries between men and women ( Buist 2007 ) and increase with age. Worldwide, COPD was the fifth\u2010 leading cause of death in 2011, and it was the seventh\u2010leading cause of lost disability\u2010adjusted life years (DALYs) ( WHO 2013 ). Exacerbations and comorbidities contribute to the variable natural history of COPD in individual patients ( GOLD 2016 ) . Exacerbations contribute to long\u2010term decline in lung function ( Donaldson 2002 ) and reduced physical activity ( Donaldson 2005 ). They have a profound and long\u2010lasting effect on quality of life ( Groenewegen 2001 ; Seemungal 1998 ) and contribute to increased risk of death ( Soler\u2010Cataluna 2005 ). Exacerbations are a major contributor to healthcare costs, especially for hospital admission ( Wedzicha 2003 ). The clinical onset of an acute exacerbation is defined according to symptoms, although definitions vary ( Rodriguez\u2010Roisin 2000 ). Anthonisen defined type 1 exacerbations on the basis of three major symptoms: increased dyspnoea, sputum volume and sputum purulence. Type 2 exacerbations required two major symptoms, and type 3 exacerbations required one major symptom plus cough, wheeze or symptoms of an upper respiratory tract infection ( Anthonisen 1987 ). A later definition required an increase in two 'major symptoms' of dyspnoea \u2010 sputum volume and sputum purulence \u2010 or an increase in one major symptom and in one 'minor symptom' for two days (wheeze, sore throat, cough or common cold symptoms) ( Seemungal 2000 ). Researchers recently developed a standardised measure for assessing the frequency, severity and duration of exacerbations of COPD using patient\u2010reported outcomes as described in clinical studies ( Leidy 2010 ). Patients with COPD with persistent lower airway bacterial colonisation when stable are at increased risk of exacerbations ( Bogaert 2004 ; Patel 2002 ). Infection is frequently detected during exacerbations; one study found that 48.4% of participants had viral causes and 54.7% had bacterial causes of infection ( Papi 2006 ). Infection\u2010associated exacerbations required longer hospitalisation and resulted in greater impairment of lung function than exacerbations in which no infection was present ( Papi 2006 ). Investigators in one study ( Patel 2002 ) recovered Streptococcus pneumoniae ( S pneumoniae ) from the sputum of 33% of participants. Risk of exacerbations of COPD is increased among patients with pneumococcal colonisation ( Bogaert 2004 ). Researchers have discovered an association between detection of S pneumoniae as a new organism in the sputum of patients with COPD and significantly increased risk of an exacerbation ( Sethi 2002 ). Pneumonia is usually a serious illness, and diagnosis is based on the presence of radiological infiltrates, symptoms (cough, expectoration, fever, dyspnoea, pleuritic pain, altered mental status), signs of pulmonary consolidation on auscultation and leukocytosis ( Ochoa\u2010Gondar 2008 ). Community\u2010acquired pneumonia (CAP) is a major health problem among adults over 65 years of age ( Welte 2009 ), and prevalence of 14 cases per 1000 person\u2010years (95% confidence interval (CI) 12.7 to 15.3) has been reported. Hospitalisation rate is high (75%), and in\u2010patient stays are often lengthy (mean 10.4 days) ( Ochoa\u2010Gondar 2008 ). Overall mortality estimates are high: 6% in Canada, 20% in the USA and Spain, 13% in the UK and 8% in Sweden ( File 2003 ; Mandell 2007 ). Patients with COPD who develop CAP have more severe pneumonia, are admitted to the intensive care unit more frequently and have significantly higher 30\u2010 and 90\u2010day mortality than non\u2010COPD patients ( Molinos 2009 ; Restrepo 2006 ). S pneumoniae is the predominant pathogen among all patients with CAP ( Mandell 2007 ) and among patients with COPD and CAP, for whom a 43% pneumococcal aetiology has been found ( Lieberman 2002 ; Torres 1996 ). Progression from COPD to CAP has been shown to be strongly associated with the presence of S pneumoniae (57.3%), and other pathogens were predominant among exacerbations that did not progress to CAP (61.7%) ( File 2009 ). Description of the intervention On the basis of differences in polysaccharide capsules, investigators have identified 91 different serotypes of S pneumoniae. Capsule polysaccharides have antiphagocytic activity, which affects the pathogenesis of invasive pneumococcal disease (IPD), including CAP ( Postma 2012 ), and the incidence of IPD differs between serotypes. In the late 1970s, a 14\u2010valent pneumococcal polysaccharide vaccine (PPV\u201014) was registered in the United States; this was replaced in the 1980s by a 23\u2010valent pneumococcal polysaccharide vaccine (PPV\u201023) (Pneumovax/Pneumo 23) in the USA and Europe. This vaccine contains purified capsular antigens from 23 serotypes that cover 85% to 90% of cases of invasive pneumococcal disease among adults ( ERS 2014 ). The vaccine induces T\u2010cell\u2010independent short\u2010lived B\u2010cell immune responses by causing B cells to differentiate into plasma cells, producing antibodies without producing memory B cells. The immunological antibody response is age\u2010 and serotype\u2010dependent and generally is lower among elderly people than in younger adults. A booster vaccination produces no memory response. To enhance the immunogenicity of pneumococcal vaccines, researchers have developed conjugate vaccines. Polysaccharide antigens are chemically joined to a highly immunogenic protein carrier (such as tetanus or diphtheria toxoid). This process leads to the induction of B cell\u2010dependent and T cell\u2010dependent responses as well as a memory response to a booster dose of the vaccine. Healthcare providers have administered pneumococcal conjugate vaccine containing capsular polysaccharides from seven pneumococcal serotypes (PCV\u20107) to young children since the 2000s, with a resulting striking decrease in invasive pneumococcal disease caused by vaccine serotypes. As children are the main reservoir of S pneumoniae (60% are carriers), a reduction in the carrier rate has had beneficial effects among children and a protective herd effect in adults ( Moseley 2013 ). Investigators are evaluating new conjugate vaccines, including 7\u2010valent (PCV\u20107), 10\u2010valent (PCV\u201010) and 13\u2010valent (PCV\u201013) vaccines, for use in children and adults, although respiratory guidelines in Europe ( ERS 2014 ) and Australia ( COPDX 2016 ) recommend immunisation with the PPV\u201023 polysaccharide pneumococcal vaccine for adults at risk of pneumococcal disease, including those with COPD. The PCV\u201013 and the PCV\u201010 are not recommended for patients with COPD in Australia ( NHMRC 2013 ). Recommendations for age at immunisation and at revaccination vary depending on the guideline, with some recommending vaccination only for patients who are over 64 years of age, or for younger patients with severe COPD or comorbid conditions ( GOLD 2016 ), and others recommending vaccination for all patients 50 years of age and older, along with revaccination five years later ( NHMRC 2013 ). How the intervention might work Patients with COPD are able to mount a significant immune response to pneumococcal infection ( Bogaert 2004 ); thus immunisation against pneumococcal infection may be effective in preventing bacterial growth in the airways of patients with COPD, in turn decreasing the occurrence of exacerbations and pneumonia. Why it is important to do this review Major COPD guidelines ( COPDX 2016 ; ERS 2014 ; GOLD 2016 ; NICE 2010 ) have recommended pneumococcal vaccination, largely on the basis of results showing the efficacy of pneumococcal vaccination as reported by observational studies in general populations and by randomised controlled trials (RCTs) in people without COPD. Both a large indirect cohort study ( Butler 1993 ) and a meta\u2010analysis ( Fine 1994 ) of pneumococcal vaccination have confirmed protection against invasive bacteraemic disease, but efficacy remains to be assessed in the population with COPD, for which risks of CAP and of deterioration may be higher owing to exacerbations of the disease.",
        "summary": "In people with chronic obstructive pulmonary disease (COPD), injectable pneumococcal vaccine seems to protect against community\u2010acquired pneumonia and COPD exacerbation, but not against pneumococcal pneumonia. The vaccine does not reduce risk of hospital admission or death but seems to result in fewer people attending the emergency department. Moderate\u2010quality evidence shows that, compared with people given placebo or no vaccination, fewer people who had received the pneumococcal vaccine developed community\u2010acquired pneumonia or had at least one COPD exacerbation (on average, 94 vs 143 per 1000 people and 482 vs 608 per 1000 people, respectively). One trial with 142 participants indicated that fewer people who had received the vaccine attended the emergency department for any cause compared with people who had received placebo or no vaccine. However, low\u2010 to moderate\u2010quality evidence showed no reduction in the incidence of pneumococcal pneumonia, hospital admission or death for any cause, or from cardiorespiratory causes, with the pneumococcal vaccine. Trials poorly reported adverse effects."
    },
    "CD011608": {
        "query": "How does peripheral nerve block compare with no peripheral nerve block for pain control in adults undergoing elective hip replacement (arthroplasty) surgery?",
        "document": "Background Description of the condition Total hip replacement (arthroplasty) is one of the most successful orthopaedic operations performed for intractable hip pain due to primary and secondary osteoarthritis, osteonecrosis, and rheumatoid arthritis. It is estimated that over 300,000 total hip replacements are performed each year in the USA alone. For European countries, the number of hip replacement procedures performed in 2007 varied from fewer than 50 to over 250 per 100,000 people ( WHO 2011 ). Controlling pain after a hip replacement improves patient comfort and satisfaction and enables patients to participate in rehabilitation more fully, leading to an earlier return home and reduced demand on resources. Current methods of pain control following hip replacement surgery include: systemic opioids or other analgesics; neuraxial blocks (epidural analgesia); peri\u2010articular/intra\u2010articular analgesia; and nerve blocks (psoas compartment block, femoral block, fascia iliaca compartment block, combined nerve blocks). systemic opioids or other analgesics; neuraxial blocks (epidural analgesia); peri\u2010articular/intra\u2010articular analgesia; and nerve blocks (psoas compartment block, femoral block, fascia iliaca compartment block, combined nerve blocks). Pain after hip replacement has traditionally been managed using systemic pain medications including acetaminophen (paracetamol), non\u2010steroidal anti\u2010inflammatory drugs (NSAIDs) and opioid analgesics. The use of parenteral or oral opioids is associated with significant adverse effects including nausea, vomiting, constipation, drowsiness, and confusion; all may affect recovery and satisfaction and prolong hospital stays. Central neuraxial blocks provide effective pain control, but there is a higher risk of neurologic complications (transient paraesthesia, motor blockade, seizures) ( Auroy 1997 ; T\u00fcrker 2003 ), cardiovascular complications necessitating haemodynamic monitoring (intra\u2010operative hypotension, postural hypotension, cardiac arrest) ( Auroy 1997 ), pruritus (itch), nausea and vomiting ( Horlocker 1998 ), and urinary retention with central neuraxial blocks compared to peripheral nerve blocks ( Horlocker 1998 ; Fischer 2005 ). Epidural anaesthesia for non\u2010obstetric indications has sometimes been associated with a risk of epidural haematoma ( Volk 2012 ). The risk factors predisposing to epidural haematoma, such as higher age, renal insufficiency, and use of anticoagulants, are very common in people undergoing hip replacement. Central neuraxial blocks in people who are anticoagulated increase the risk of epidural/spinal haematoma that may result in permanent neurological damage ( Dahlgren 1995 ; Sternlo 1995 ). Elderly people undergoing hip replacements may have spinal stenosis, which may increase the risk of neurological complications with central neuraxial blocks ( Hebl 2010 ). A prior Cochrane review studied epidural analgesia for pain relief following hip or knee replacement ( Choi 2003 ). Peri\u2010articular analgesia (injecting various combinations of local anaesthetics and analgesics into the hip joint or periarticular tissues) has gained popularity recently. A Cochrane Review protocol has been published on this topic ( Hadi 2014 ). Because peripheral nerve blocks confine anaesthesia to the surgical region, many disadvantages of neuraxial blocks can be avoided. Peripheral nerve damage can occur after peripheral nerve block, but consequences may be more limited than with central neuraxial blocks. Given the significant differences in safety profiles of peripheral nerve blocks and central neuraxial blocks, we focused specifically on peripheral nerve blocks. Description of the intervention Nerve blocks (injection of a local anaesthetic around a nerve) relieve pain by interrupting transmission of pain signals from the peripheral nerves. Nerve blocks for orthopaedic procedures have been shown to facilitate the execution of surgery in ambulatory surgery (day surgery), improve pain control and sleep after surgery, and decrease time to discharge home ( Ilfeld 2006a ; Ilfeld 2006b ). Nerve blocks may also reduce the need for systemic pain medications limiting associated adverse effects. How the intervention might work The hip area is innervated by branches of the lumbar plexus. The hip joint is supplied with femoral and obturator nerves, nerve to quadratus femoris, superior gluteal and sciatic nerves. The dermatomal supply of the hip joint is typically from spinal nerve roots lumbar\u20104 to as low as sacral\u20102. The bony structures of the hip joint are supplied from spinal nerve roots lumbar\u20103 to sacral\u20101. It is difficult to achieve complete pain relief of the hip with peripheral nerve blocks ( de Visme 2000 ), and some techniques (psoas compartment block) are considered to be expert\u2010level (practiced only by some anaesthesiologists; Hargett 2005 ). There are many types and techniques for blocking the lumbar plexus nerves following hip replacement. Lumbar plexus, or psoas compartment block: peripheral regional anaesthetic technique to block the major nerves of the lumbar plexus (femoral, lateral femoral cutaneous and obturator nerves) in the psoas major muscle ( Chayen 1976 ; Capdevila 2002 ; Karmakar 2015 ). Femoral nerve block is a safe and widely practiced local anaesthetic technique used to supplement anaesthesia and provide postoperative analgesia after hip surgery ( Szucs 2010 ; Winnie 1973 ). Local anaesthetic is infiltrated around the femoral nerve, which provides anaesthesia to the anterior thigh (femoral nerve) and the medial lower leg (through the saphenous nerve). However, the cephalad spread of the local anaesthetic may not be sufficient to block the obturator nerve (medial thigh) and the lateral cutaneous nerve of thigh ( Marhofer 2000 ). Fascia iliaca compartment block (FICB) is an anterior\u2010thigh regional anaesthetic block targeting the lumbar plexus ( Dalens 1989 ; Murgatroyd 2013 ). This block was initially described by Dalens 1989 for children where sensory blockade of the obturator nerve was believed to be observed. It was believed the local anaesthetic spread underneath the fascia iliaca proximally towards the lumbosacral plexus ( Dalens 1989 ). However, it has since been discovered that nearly half of patients do not have a skin component of the obturator nerve and that assessing adductor strength is the only effective way to measure obturator nerve function ( Bouaziz 2002 ). Kaloul 2004 found that motor obturator nerve blockade is achieved in fewer than 50% of patients undergoing femoral nerve blocks. The effect of the FICB is similar to the femoral nerve block, but may provide a more reliable method of reaching the femoral lateral cutaneous nerve. It is possible to individually block nerves supplying the hip, but this is time consuming. Lumbar plexus, or psoas compartment block: peripheral regional anaesthetic technique to block the major nerves of the lumbar plexus (femoral, lateral femoral cutaneous and obturator nerves) in the psoas major muscle ( Chayen 1976 ; Capdevila 2002 ; Karmakar 2015 ). Femoral nerve block is a safe and widely practiced local anaesthetic technique used to supplement anaesthesia and provide postoperative analgesia after hip surgery ( Szucs 2010 ; Winnie 1973 ). Local anaesthetic is infiltrated around the femoral nerve, which provides anaesthesia to the anterior thigh (femoral nerve) and the medial lower leg (through the saphenous nerve). However, the cephalad spread of the local anaesthetic may not be sufficient to block the obturator nerve (medial thigh) and the lateral cutaneous nerve of thigh ( Marhofer 2000 ). Fascia iliaca compartment block (FICB) is an anterior\u2010thigh regional anaesthetic block targeting the lumbar plexus ( Dalens 1989 ; Murgatroyd 2013 ). This block was initially described by Dalens 1989 for children where sensory blockade of the obturator nerve was believed to be observed. It was believed the local anaesthetic spread underneath the fascia iliaca proximally towards the lumbosacral plexus ( Dalens 1989 ). However, it has since been discovered that nearly half of patients do not have a skin component of the obturator nerve and that assessing adductor strength is the only effective way to measure obturator nerve function ( Bouaziz 2002 ). Kaloul 2004 found that motor obturator nerve blockade is achieved in fewer than 50% of patients undergoing femoral nerve blocks. The effect of the FICB is similar to the femoral nerve block, but may provide a more reliable method of reaching the femoral lateral cutaneous nerve. It is possible to individually block nerves supplying the hip, but this is time consuming. Why it is important to do this review Nerve blocks have been used successfully to reduce opioid requirements following other surgical interventions. A Cochrane Review has been published that investigated adding peripheral nerve blocks for hip fracture surgery ( Guay 2017 ). Several RCTs have reported on the use of nerve blocks for pain control after hip replacement. A Cochrane Review focusing on functional improvement with regional analgesia at 3, 6, or 12 months after hip, knee, or shoulder replacement is available ( Atchabahian 2015 ). This review aimed to provide evidence to assist people undergoing hip replacement to decide which pain management protocol to choose and whether the risks associated with having nerve blocks exceed the benefits. The rapidly rising volume of hip replacement surgeries being performed annually worldwide will considerably increase the burden on healthcare resources. A systematic review evaluating the current evidence of the short\u2010 and long\u2010term safety and efficacy of nerve blocks after hip replacement surgery was necessary.",
        "summary": "Evidence from randomized controlled trials (RCTs) suggests that peripheral nerve block (PNB) reduces postoperative pain at rest and with movement up to 48 hours after hip replacement surgery; this reduction is moderate to large for all time points. Reviewers assessed the quality of evidence only for pain at rest immediately post surgery (moderate\u2010quality evidence); lack of blinding of participants and outcome assessors could impact the reliability of this subjective outcome. Trialists report a moderate reduction in the requirement for opioids within the first 24 hours after surgery, and low\u2010quality evidence suggests that patient satisfaction is moderately greater with PNB than with no PNB. When evidence shows differences between PNB and no PNB for adverse events/complications (acute confusional status and pruritus), these differences favor PNB, but the quality of this evidence is very low. It is interesting to note that individuals in both groups were able to walk on postoperative day 1; however, the quality of this evidence is also very low."
    },
    "CD008709": {
        "query": "How do single\u2010incision slings compare with retropubic or trans\u2010obturator minimally invasive slings in women with urinary incontinence?",
        "document": "Background Urinary incontinence (UI) is an extremely common yet under\u2010reported, under\u2010diagnosed, under\u2010treated and potentially manageable condition that is prevalent throughout the world. It can cause a great deal of distress and embarrassment to individuals, as well as significant financial costs to those individuals and to societies. Estimates of prevalence vary from 10% to 40% depending on the definition and type of incontinence studied, with annual incidence ranging from 2% to 11% ( Hunskaar 2002 ; Milsom 2009 ). Studies in the USA have shown that up to 80% of women with incontinence have an element of stress urinary incontinence ( Hampel 1997 ). At the turn of the century, Turner estimated that the total annual cost to the United Kingdom National Health Service of treating clinically significant urinary incontinence was GBP 233 million (1999/2000 GDP), with the cost to individuals estimated at an additional GBP 178 million ( Turner 2004 ). In the USA the annual direct costs of urinary incontinence in both men and women is over USD 16 billion (1995 USD) ( Chong 2011 ), with a societal cost of USD 26.2 billion (1995 USD) ( Wagner 1998 ). Approximately USD 13.12 billion (1995 USD) of the total direct costs of urinary incontinence are spent on SUI ( Chong 2011 ; Kunkle 2015 ). About 70% of this USD 13.12 billion is borne by the patients, mainly through routine care (purchasing pads and disposable underwear (diapers), laundry and dry cleaning). This constitute a significant individual financial burden. Of the remaining 30%, 14% is spent on nursing home admissions, 9% on treatment, 6% on addressing complications and 1% on diagnosis ( Chong 2011 ). Subak 2008 reported that about 1% of the median annual household income (USD 50,000 to USD 59,999) was spent by women on incontinence management. This study estimated that women spent an annual mean cost of USD 751 to USD 1277(2006 USD) on incontinence. This cost increases based on the severity of the symptoms ( Subak 2008 ).The indirect cost associated exerts social and psychological burdens which are unquantifiable. ( Chong 2011 ; Kilonzo 2004 ). Nevertheless, Birnbaum 2004 estimated that the annual average direct medical costs of SUI for one year (1998 USD) was USD 5642, and USD 4208 for indirect workplace costs.The cost of management and treatment of SUI appears to have increased over time, due to increasing prevalence and increased desire for improved quality of life. This in turn has resulted from improved recognition of the condition, as well as increased use of surgical and non\u2010surgical managements. The surgical approach to stress urinary incontinence has progressed rapidly over the past one and a half decades. In the mid\u20101990s, a prospective randomised study confirmed the superiority of the colposuspension over the Kelly plication and modified Pereyra needle suspension techniques, with five\u2010year cure rates in excess of 80% ( Bergman 1995 ). This established the colposuspension as the standard approach to stress incontinence surgery. A colposuspension, however, entails major surgery with substantial operating time and lengthy hospital stay, as well as significant potential for morbidity ( Lapitan 2012 ). The pubovaginal sling, which employs a fascial strip for support, is an effective alternative to the colposuspension, with similar efficacy ( Rehman 2011 ). The incidence of severe adverse events following these procedures is high, for example, 10% after colposuspension ( Lapitan 2012 ) and 13% after pubovaginal slings ( Bezerra 2005 ). Description of the condition Classically, UI is subdivided into three main types. Stress urinary incontinence (SUI) is characterised by leakage that occurs mainly during 'stress,' which can be brought about by coughing, sneezing, exercise or any manoeuvre that increases intra\u2010abdominal pressure. SUI is generally due to an anatomical/mechanical abnormality or weakness in the urethra/sphincter/pelvic floor support, and it is commonly treated with an anatomical/mechanical solution (i.e. surgery). Urgency urinary incontinence (UUI) is characterised by leakage associated with a sense of urgency (defined as a sudden compelling desire to pass urine that cannot be postponed for fear of leakage). UUI is thought to be caused by involuntary detrusor contractions, which may be neurogenic or idiopathic, and it is treated with medication (most commonly anti\u2010muscarinic drugs), intra\u2010vesical botulinum toxin injections or, in extreme cases, surgery. Mixed urinary incontinence (MUI) is a combination of stress and urgency incontinence. Stress urinary incontinence (SUI) is characterised by leakage that occurs mainly during 'stress,' which can be brought about by coughing, sneezing, exercise or any manoeuvre that increases intra\u2010abdominal pressure. SUI is generally due to an anatomical/mechanical abnormality or weakness in the urethra/sphincter/pelvic floor support, and it is commonly treated with an anatomical/mechanical solution (i.e. surgery). Urgency urinary incontinence (UUI) is characterised by leakage associated with a sense of urgency (defined as a sudden compelling desire to pass urine that cannot be postponed for fear of leakage). UUI is thought to be caused by involuntary detrusor contractions, which may be neurogenic or idiopathic, and it is treated with medication (most commonly anti\u2010muscarinic drugs), intra\u2010vesical botulinum toxin injections or, in extreme cases, surgery. Mixed urinary incontinence (MUI) is a combination of stress and urgency incontinence. Surgical management of SUI or stress\u2010predominant MUI is most commonly achieved these days by using a mid\u2010urethral support in the form of a tape or mesh. A great deal of research continues to be conducted to find the best balance of efficacy and minimal adverse events in choosing the right kind of tape. Women with SUI or stress\u2010predominant MUI, diagnosed clinically or on urodynamics, have been included in this review. Description of the intervention In 1993 Ulmsten and Petros proposed the integral theory, a new concept in the maintenance of female urinary continence ( Petros 1993 ). This is considered to be one of the drivers for the development of \u201ctension\u2010free vaginal tape\u201d (TVT), which was the first effective minimally invasive procedure for stress incontinence in women ( Ulmsten 1998 ). The five\u2010year efficacy of TVT has been shown to be comparable with that of the Burch colposuspension, with the added benefits of shorter operating time and decreased hospital stay ( Ward 2008 ). The major disadvantage of the TVT procedure is that it involves the \u201cblind\u201d passage of a retropubic needle, which poses a significant risk for bladder, bowel and major vessel damage. The incidence of bladder injury is approximately 6% ( Ogah 2009 ). This led to the development of the next generation of sub\u2010urethral sling procedures with the launch of transobturator tape (TOT) ( Delorme 2001 ). Objective and subjective cure rates for both types of mid\u2010urethral tape have been shown to be equivalent ( Nambiar 2012 ), but the transobturator passage resulted in fewer injuries to the bladder and other organs. A recent Cochrane review ( Ogah 2009 ) describes lower complication rates with TOT, including less bladder perforation and shorter operating time. The transobturator approach is not without complications, and it has been shown to be associated with significant risk of groin and hip pain following surgery. A meta\u2010analysis ( Latthe 2007 ) reported an incidence of 12% for groin and hip pain following an obturator\u2010type sling compared with only 1% for the retropubic approach. The significant risk of visceral injury associated with the retropubic tape and the high incidence of groin pain following the transobturator route have led to the development of a new generation of stress incontinence devices. Popularly known as the \u201cmini\u2010slings\u201d ( Moore 2009 ), these third\u2010generation devices differ from previous sling procedures in that a single incision is made within the vagina with no tape exit incisions. They have also been called single\u2010incision slings ( Molden 2008 ). The tape used in these devices is significantly shorter (eight to 14 cm) in length than first\u2010 and second\u2010generation slings. The insertion pass stops short of the obturator membrane or pelvic floor. This less invasive approach is thought to reduce complications, including bladder/bowel and vascular injury and groin and thigh pain, with a shorter hospital stay and less postoperative pain. Interest is gradually increasing regarding the efficacy and safety of the mini\u2010slings, but at present, clinical data on these procedures are lacking. How the intervention might work Single\u2010incision slings have been developed that are based on the same mechanistic principles as minimally invasive slings, that is, to restore or enhance the woman's urethral support during a sudden rise in intra\u2010abdominal pressure, such as during a cough or sneeze, thus preventing involuntary loss of urine. At the same time, they aim to minimise the risk of major side effects associated with minimally invasive slings, such as bladder/vaginal/urethral/vascular perforations or erosions and chronic pain. To try to achieve this, these slings have shorter tape lengths and different fixation systems compared with minimally invasive slings. The main difference in these fixation systems is that they do not penetrate the obturator fossa (hence potentially minimising the risk of groin pain) or the retropubic space (minimising the risk of major vessel or visceral injury). Currently six minimally invasive sling devices are available, including TVT Secur, MiniArc, Ajust, Needleless, Tissue Fixation System and Ophira. Differences between the various devices include the following. The TVT\u2010Secur is inserted with a metal introducer that anchors the device in the obturator membrane. It is placed snugly against the urethra. The MiniArc has a curved introducer that clips into two plastic anchoring hooks on the ends of the sling; this is used to insert the sling and secure it into the obturator membrane. The Ajust also has a curved introducer with plastic anchoring hooks, but it differs from the other devices in that it has a pulley\u2010like system that allows adjustment following insertion. The Needleless device is 60% longer than the other mini\u2010slings. It has a pocket\u2010like fold on each end, and an artery forceps is placed onto the end of the sling in this pouch. The sling is pushed laterally and through the obturator membrane at insertion. The Ophira mini\u2010sling is a type 1 polypropylene monofilament mesh with two fixation arms that penetrate the obturator internus muscle on either side with the help of a retractile insertion guide. The TFS consists of non\u2010stretch multi\u2010filament polypropylene tape with two polypropylene soft tissue anchors at either end. The tape is passed in the same direction as standard TVT, but the anchors are embedded into the pubourethral ligament inferior to the pubic symphysis. CureMesh is a 14\u2010cm polypropylene mesh similar to the MiniArc sling but manufactured domestically in South Korea. The TVT\u2010Secur is inserted with a metal introducer that anchors the device in the obturator membrane. It is placed snugly against the urethra. The MiniArc has a curved introducer that clips into two plastic anchoring hooks on the ends of the sling; this is used to insert the sling and secure it into the obturator membrane. The Ajust also has a curved introducer with plastic anchoring hooks, but it differs from the other devices in that it has a pulley\u2010like system that allows adjustment following insertion. The Needleless device is 60% longer than the other mini\u2010slings. It has a pocket\u2010like fold on each end, and an artery forceps is placed onto the end of the sling in this pouch. The sling is pushed laterally and through the obturator membrane at insertion. The Ophira mini\u2010sling is a type 1 polypropylene monofilament mesh with two fixation arms that penetrate the obturator internus muscle on either side with the help of a retractile insertion guide. The TFS consists of non\u2010stretch multi\u2010filament polypropylene tape with two polypropylene soft tissue anchors at either end. The tape is passed in the same direction as standard TVT, but the anchors are embedded into the pubourethral ligament inferior to the pubic symphysis. CureMesh is a 14\u2010cm polypropylene mesh similar to the MiniArc sling but manufactured domestically in South Korea. Why it is important to do this review Various observational trials have reported cure rates of 77% ( Debodinance 2008 ) and 81% ( Meschia 2009 ) for the TVT\u2010Secur and 77% for the MiniArc ( Gauruder\u2010Burmester 2009 ). Preliminary data also suggest lower rates of bladder injury and groin or hip pain following insertion of these devices. With the introduction of new devices, clinicians have to decide whether they are going to adopt the new technique. Studies of surgical devices can be notoriously difficult to conduct and to report and interpret. It is therefore imperative that a high\u2010quality review is conducted to pool relevant data from randomised controlled trials to try to answer the question of whether these new single\u2010incision slings are capable of providing adequate treatment for stress incontinence with a lower rate of side effects compared with currently available standard methods of treatment. This is even more important in the current clinical climate in 2014, when implantable meshes and tapes are under intense scrutiny, both in the media and in clinical circles.",
        "summary": "Compared with minimally invasive slings (retropubic or trans\u2010obturator), single\u2010incision slings seem to result in poorer incontinence outcomes among women with stress urinary incontinence. However, included randomized controlled trials (RCTs) had methodological flaws, making it difficult to draw conclusions with any confidence. Compared with retropubic minimally invasive slings in women with stress urinary incontinence, use of single\u2010incision slings led to more women with continued stress urinary incontinence (on average, 485 vs 233 per 1000 women), worse Incontinence Severity Index scores (on average, by 0.70 points on an 8\u2010point scale), and more women with de novo urgency (on average, 165 vs 69 per 1000 women). Women were more likely to have urinary incontinence with single\u2010incision slings than with trans\u2010obturator minimally invasive slings (on average, 200 vs 105 per 1000 women). They also were more likely to report no improvement in incontinence (on average, 117 vs 51 per 1000 women), to undergo repeat stress incontinence surgery (55 vs 18 per 1000 women), to exhibit mesh extrusion into the bladder or urethra, and have vaginal mesh exposure (68 vs 26 per 1000 women). Women with single\u2010incision slings also were more likely to need additional or new surgery to treat complications (60 vs 29 per 1000 women). Some evidence indicates that women were less likely to be in pain postoperatively and over the longer term after use of a single\u2010incision sling. It is worth noting that although researchers did not assess the quality of the evidence, several RCTs had methodological flaws that would have affected the reliability of their results, and some did not report important aspects of their methods, meaning that these results should be interpreted with caution."
    },
    "CD006178": {
        "query": "Do antenatal screening and treatment programs for lower genital tract infection help to prevent preterm delivery?",
        "document": "Background Description of the condition Preterm birth, defined as birth occurring prior to 37 weeks' gestation, occurs in 5% to 10% of all pregnancies and is the most common cause of perinatal morbidity and mortality in the world. Preterm birth is implicated in at least two\u2010thirds of early infant deaths ( Cunningham 1997 ) and 60% of perinatal mortality including long\u2010term neurologic disability such as cerebral palsy. It is associated with admission to neonatal intensive care, severe morbidity in the first weeks of life, prolonged hospital stay after birth, and readmission to hospital in the first year of life ( Cunningham 2001 ; Goldenberg 1998 ; Roberts 2000 ; Wood 2000 ). Surviving infants, especially those born before 32 weeks, have a substantially increased risk of chronic lung disease, and major and minor impairments ( Doyle 1996 ; Saigal 2000 ). Whatever the result, the emotional impact on the family can be enormous. A wide spectrum of causes and demographic factors have been implicated in preterm birth. These can be categorized into four groups. Medical and obstetric complications: there are associations with placental hemorrhage and hypertensive disorders in about one\u2010third of cases ( Meis 1995 ). Lifestyle factors: there is an association with alcohol abuse, low maternal age, and occupational factors ( Henriksen 1995 ; Holzman 1995 ; Satin 1994 ). Amniotic fluid infection caused by a variety of micro\u2010organisms located in the genital tract: approximately one\u2010third of preterm births are associated with chorioamniotic infection ( Lettieri 1993 ). Asymptomatic cervical dilatation ( Papiernik 1986 ). Medical and obstetric complications: there are associations with placental hemorrhage and hypertensive disorders in about one\u2010third of cases ( Meis 1995 ). Lifestyle factors: there is an association with alcohol abuse, low maternal age, and occupational factors ( Henriksen 1995 ; Holzman 1995 ; Satin 1994 ). Amniotic fluid infection caused by a variety of micro\u2010organisms located in the genital tract: approximately one\u2010third of preterm births are associated with chorioamniotic infection ( Lettieri 1993 ). Asymptomatic cervical dilatation ( Papiernik 1986 ). Many micro\u2010organisms cause both symptomatic and asymptomatic infection and may result in preterm prelabour rupture of membranes, preterm labour, or both. For example, bacterial vaginosis (including Gardnerella vaginalis , Bacteroides species, Mobiluncus species, Ureaplasma urealyticum , and Mycoplasma hominis ) ( Hillier 1995 ; McDonald 1994 ; McGregor 1990 ; Meis 1995 ), Chlamydia trachomatis ( Gravett 1986 ), Trichomonas vaginalis ( Cotch 1997 ), Neisseria gonorrhoeae ( Elliott 1990 ), Group B S treptococci (GBS; Regan 1981 ), Staphylococcus aureus ( McGregor 1990 ), syphilis ( McFarlin 1995 ), HIV ( Temmerman 1994 ), enteropharyngeal bacteria and Peptostreptococcus species ( McDonald 1994 ) have been associated with an increased risk of preterm birth. Candida species, however, have been associated with an unclear risk of preterm birth ( Roberts 2011 ). A possible mechanism for the link between infection and preterm birth is the bacterial stimulation of the biosynthesis of prostaglandins. This may occur either directly via phospholipase A 2 and C ( Bejar 1981 ) or as a result of bacterial endotoxin introduced into the amniotic fluid which stimulates decidual cells to produce cytokines and prostaglandins that initiate labour ( Cox 1989 ). Indirect links via substances such as interleukin\u20101, tumour necrosis factor and platelet activating factor, all of which may be found in infected amniotic fluid, have also been identified ( Romero 1992 ; Yoon 2000 ). Description of the intervention By identifying and treating vaginal infections, screening programs may be able to reduce the rate of preterm birth. Different screening methods are used for different types of organisms, however there is scant evidence to inform the optimal screening regimen for detecting these organisms during pregnancy. Therefore, it is unclear whether all women should be routinely screened, how often the screening should occur, and which tests should be used. How the intervention might work Chlamydia trachomatis has been identified by multiple tests from different specimen sources. The samples may be analysed by three types of DNA\u2010based test: ligase chain reaction, polymerase chain reaction (PCR) and enzyme immuno\u2010assay ( Watson 2002 ). DNA amplification techniques allow for highly sensitive and specific tests ( Black 1997 ) that are more sensitive than cell culture ( Jespersen 2005 ). These screening tests can detect Chlamydia in genital secretions, urine specimens, and endocervical, vaginal or urethral samples ( Domeika 1999 ; Shrier 2004 ). Trichomoniasis may be asymptomatic in up to 50% of infected women ( Wolner\u2010Hanssen 1989 ). The diagnosis is usually made on clinical findings and laboratory procedures ( Petrin 1998 ). Most frequently, the saline wet\u2010mount preparation is used for observation of motile organisms under the light microscope. Wet\u2010mount smear is a cheap and quick method but more sensitive techniques are culture, immunofluorescence and enzyme immunoassay ( Lossick 1991 ; Borchardt 1991 ). Different staining techniques include Gram stain, Giemsa stain, Papanicolaou smear, acridine orange ( Borchardt 1991 ; Rein 1990 ); diverse molecularly\u2010based diagnostic methods, such as hybridization assay and PCR, may also be used. These tests vary widely in sensitivities and specificities for screening trichomoniasis ( DeMeo 1996 ; Madico 1998 ; Mayta 2000 ; Muresu 1994 ). Bacterial vaginosis is a clinical syndrome; the microbiology of bacterial vaginosis is complex and is composed of Gardnerella vaginalis , Mycoplasma hominis and anaerobic bacteria ( Amsel 1983 ). The diagnosis is usually made on clinical Amsel criteria findings ( Amsel 1983 ) and laboratory tests. Vaginal pH testing may be a valuable screening tool as it is a quick and inexpensive test ( Gjerdingen 2000 ). Vaginal swab Gram stain with quantification of the microbial flora has high sensitivity and specificity and is accepted as an alternative method ( Nugent 1991 ). Multiple screening tests exist for other organisms including syphilis. Screening tests such as Treponema pallidum hemagglutination assay, Treponema pallidum particle agglutination assay, and enzyme\u2010linked immunosorbent assays (ELISAs) are more reliable than Venereal Disease Research Laboratory testing, the fluorescent treponemal antibody absorption test, and immunoblot assays ( Muller 2006 ). The screening test for Neisseria gonorrhoeae , usually from a culture, remains accurate when transport conditions are suitable; this tests can be used with cervical, urine and vaginal swabs. Diagnosis of HIV infection can be obtained from enzyme\u2010linked immunosorbent assay (ELISA), Western blot, and RNA PCR testing ( Kleinman 1998 ). The HIV\u2010p24 Ag is effective for early diagnosis of an acute HIV infection ( Thies 1994 ). Strategies for the diagnosis of GBS include obtaining vaginal or both vaginal and anorectal GBS cultures ( Quinlan 2000 ) and a rapid enrichment cum antigen detection test ( Das 2003 ). Why it is important to do this review Other Cochrane reviews have addressed a number of issues regarding treatment of infection in pregnancy. Antibiotic treatment of chlamydia, trichomoniasis, bacterial vaginosis and gonorrhoeal infection in pregnancy appear to be effective to clear organisms ( Brocklehurst 1998 ; Brocklehurst 2002 ; G\u00fclmezoglu 2002 ; Brocklehurst 2013 ) but it is not known whether treatment of trichomonas will have any effect on pregnancy outcomes ( G\u00fclmezoglu 2002 ). There is also little evidence to show that screening and treatment in all asymptomatic pregnant women for bacterial vaginosis can prevent preterm birth ( Brocklehurst 2013 ), although antibiotic prophylaxis in pregnancies with a previous preterm birth associated with bacterial vaginosis can reduce preterm delivery ( Thinkhamrop 2002 ). There is insufficient evidence regarding the treatment of ureaplasmas to reduce preterm birth ( Raynes\u2010Greenow 2004 ), and there is no evidence that antiretrovirals and the treatment of syphilis influence the incidence of preterm birth ( Volmink 2007 ; Walker 2001 ). None of the aforementioned reviews are concerned primarily with screening programs for antenatal lower genital tract infection, thus a review of the effects of screening programs for lower genital tract infection to prevent preterm birth is required.",
        "summary": "In women being screened antenatally for lower genital tract infection (LGTI) for prevention of preterm birth, a single large randomized controlled trial (4000 women) found that conducting vaginal smears and treating LGTI in pregnant women with no history of LGTI reduced preterm birth and low birth weight. There is insufficient randomized controlled trial evidence to determine the effects of treating recurrent/persistent LGTI on preterm labor and pregnancy outcomes. On trial including 4155 women with singleton pregnancies and no history of LGTI showed that simple screening plus treatment for LGTI reduced preterm birth less than 37 weeks (29 per 1000 women (95% CI 22 to 40) with infection screening and treatment compared with 53 per 1000 women with no screening). Simple LGTI screening also reduced the incidence of low birth less than 2500 grams (24 per 1000 women (95% CI 17 to 34) with infection screening and treatment compared with 51 per 1000 women with no screening) and the incidence of very low birth weight less than 1500 grams (4 per 1000 women (95% CI 2 to 9) with infection screening and treatment compared with 11 per 1000 women with no screening). None of the trials assessed neonatal morbidity, maternal or peri/neo\u2010natal mortality, duration of admission to the neonatal intensive care unit or women\u2019s satisfaction."
    },
    "CD005187": {
        "query": "What are the effects on their patients of giving influenza vaccination to healthcare workers who care for people aged 60 or older living in long\u2010term care institutions?",
        "document": "Background Description of the condition Healthcare workers, such as doctors, nurses, other health professionals, cleaners and porters (and also family visitors), may have substantial rates of clinical and sub\u2010clinical influenza during influenza seasons ( Elder 1996 ; Jefferson 2009 ; Ruel 2002 ). Laboratory\u2010proven influenza in the general population on average accounts for a small proportion of 'influenza\u2010like illnesses'. A systematic review of 29 observational studies with 58,245 participants during 97 influenza seasons found that 3% (95% confidence interval (CI) 1.79% to 5.15%) of vaccinated working adults had a symptomatic influenza infection (tested by serology) per influenza season. Among vaccinated healthcare workers 4.8% (95% CI 3.23% to 7.16%) had an influenza infection per influenza season. Of unvaccinated working adults 5.12% (95% CI 3.08% to 8.52%) had an influenza infection per season; in unvaccinated HCWs this was 7.54% (95% CI 4.86% to 11.70%) ( Kuster 2011 ). Healthcare workers often continue to work when infected with influenza, increasing the likelihood of transmitting influenza to those in their care ( Coles 1992 ; Weingarten 1989 ; Yassi 1993 ). However, a review of infection transmission in hospitals was unable to provide numerical data for influenza transmission by HCWs ( Sydnor 2014 ). Those aged 60 or older in institutions such as long\u2010stay hospital wards and nursing homes are at risk of influenza and its complications, especially if affected with multiple pathologies ( Fune 1999 ; Jackson 1992 ; Muder 1998 ; Nicolle 1984 ). Description of the intervention One way to prevent the spread of influenza to those aged 60 years or older resident in long\u2010term care institutions (LTCIs) may be to vaccinate healthcare workers. The Centers for Disease Control (CDC) Advisory Committee on Immunisation Practices (ACIP) recommends vaccination of all healthcare workers ( Harper 2004 ). However, only 36% of healthcare workers in the US were vaccinated in 2003 ( CDC 2003 ), 35% of staff in LTCIs in Canada were vaccinated in 1999 ( Stevenson 2001 ), and 34% to 44% after a RCT in 43 geriatric healthcare settings in France to increase vaccination rates ( Rothan\u2010Tondeur 2010 ). Nurses and (in some institutions) physicians, tend to have lower influenza vaccination rates than other healthcare workers. This relatively low uptake may partly be a reflection of doubts as to the vaccine's ability to prevent influenza ( Ballada 1994 ; Campos 2002\u20103 ; Ludwig\u2010Beymer 2002 ; Martinello 2003 ; Quereshi 2004 ). The design and execution of campaigns to increase vaccination rates are also important ( Doebbeling 1997 ; NFID 2004 ; Russell 2003a ; Russell 2003b ), in order to provide an intervention at minimal risk of bias from inadequate randomisation, concealment of allocation, blinding, attrition, incomplete reporting and inappropriate statistical analysis. How the intervention might work Healthcare workers are the key group who enter nursing and LTCIs on a daily basis. The immune systems of the elderly are less responsive to vaccination and vaccinating healthcare workers could reduce the exposure of those aged 60 years or older to influenza. Why it is important to do this review Previous systematic reviews of the effects of influenza vaccines in those aged 60 years or older are now out of date or do not include all relevant studies. The Gross 1995 review is 17 years old and its conclusions are affected by the exclusion of recent evidence. The Vu 2002 review has methodological weaknesses (excluding studies with denominators smaller than 30 and quantitative pooling of studies with different designs), which are likely to undermine the conclusions. A systematic review by Jordan 2004 of the effects of vaccinating healthcare workers against influenza on high\u2010risk individual elderly reports significantly lower mortality in the elderly (13.6% versus 22.4%, odds ratio (OR) 0.58, 95% CI 0.4 to 0.84) but does not include the latest studies. The Burls 2006 systematic review of the effects on elderly people only identified the RCTs by Potter 1997 and Carman 2000 . Anikeeva 2009 does not include the study by Lemaitre 2009 . It is important to provide accurate information for policy makers and to highlight the need for high quality trials to test combinations of interventions, including healthcare worker vaccination. There are Cochrane systematic reviews assessing the effects of influenza vaccines in children ( Jefferson 2012 ), the elderly ( Jefferson 2010 ), healthy adults ( Demicheli 2014 ), people affected with chronic obstructive pulmonary disease ( Poole 2010 ), and cystic fibrosis ( Dharmaraj 2009 ).",
        "summary": "Based on randomized controlled trial evidence, routine influenza vaccination of healthcare workers does not seem to be effective for reducing influenza morbidity and mortality in the older patients that they care for who are hospitalized or in care\u2010homes. In theory, influenza vaccination of health care staff may reduce illness in staff and also reduce transmission to patients. However, randomized controlled trials including around 4,500 carers found no evidence of a beneficial effect on patients. The review assessed the effects of healthcare worker vaccination on older adults in long\u2010term care institutions. The interventions included mandatory vaccination or promotional campaigns to improve uptake of influenza vaccination amongst staff. Primarily low\u2010quality evidence suggested that rates of influenza, lower respiratory tract infection, hospitalizations with respiratory illness and death from influenza were not modified by influenza vaccination. The reviewers did not consider the outcome of all\u2010cause mortality."
    },
    "CD004184": {
        "query": "What are the benefits and harms of evening compared with morning antihypertensive regimens in people with hypertension?",
        "document": "Background Description of the condition Elevated blood pressure or hypertension (defined as resting blood pressure levels of 140/90 mm Hg or more) is estimated to affect 20% of the adult population in both developed and developing countries. It is associated with an increased risk of death, and cardiovascular disease (CVD). Six main classes of antihypertensive drugs are used worldwide: diuretics, angiotensin converting enzyme inhibitors (ACEIs), calcium channel blockers (CCBs), beta\u2010adrenergic receptor blockers (BBs), angiotensin II receptor blockers (ARBs) and alpha\u2010adrenergic antagonists. It is a well known fact that variation in blood pressure levels display circadian rhythms. The morning surge in blood pressure is known to increase the risk of myocardial events in the first several hours post awakening. WHO recommends using once daily long acting antihypertensive drugs, since they provide a more consistent 24\u2010hour BP control, reduce BP variability, and improve adherence to therapy ( Guidelines Subcommittee 1999 ). Antihypertensive drugs are traditionally administered either as monotherapy or in combinations in the morning upon arising from bed. This is mainly because this approach has been applied in the vast majority of outcome trials that showed benefits of treatment in reducing the risk of CVD. Another important reason is that a once\u2010daily morning regimen improves patients' adherence to the long\u2010term treatment ( Chobanian 2003 , Waeber 1999 ). However, the administration\u2010time\u2010effects of evening versus morning dosing regimen of antihypertensive drugs on clinically relevant outcomes such as death and cardiovascular outcomes in the management of patients with primary hypertension has not been studied in a systematic review. Description of the intervention In this review, the conventional or routine administration of antihypertensive drug therapy for essential hypertension means dosing in the morning upon arising from bed. The traditional antihypertensive agents include long acting medications or the conventional (so\u2010called homeostatically formulated) drugs administered without regard to BP circadian rhythm. They differ from chronotherapeutic formulations which are specially designed to provide peak plasma concentrations during the early morning hours when BP rises to peak and provide lower concentrations at night ( Smolensky 1999 ). Chronotherapeutics is defined as the purposeful timing of medications, whether or not they utilize special drug release technology, to proportion serum and tissue concentrations in synchrony with known circadian rhythms in disease processes and symptoms as a means of enhancing beneficial outcomes and/or attenuating or averting adverse effects ( Smolensky 1996 ). The chronotherapy of hypertension specifically entails significant attenuation of the accelerated morning rise of SBP and DBP and this may be achieved through the use of special drug\u2010delivery technology ( Smolensky 2005 ) or by changing the dosing timing of conventional BP\u2010lowering medications ( Hermida 2004a , Hermida 2005e ). Ambulatory blood pressure monitoring (ABPM) is a valuable technique to determine antihypertensive efficacy both in clinical practice and in research settings ( O'Brien 1991 ). The use of such monitoring makes it feasible to follow the time course of BP variation around the clock in large groups of subjects. Compared with traditional resting BP measurement, it allows the assessment of duration of action of antihypertensive agents and compensates for most of the limitations of office determinations ( Hermida 1999 ). It also makes it possible to exclude pharmacotherapy in patients who have white coat hypertension, and allows the evaluation of the consistency of the antihypertensive effect of new drug\u2010chronotherapeutic agents ( Canter 1994 ). How the intervention might work Blood pressure (BP) varies throughout the day, has a distinct and reproducible 24\u2010hour circadian rhythm in both normotensive and uncomplicated hypertensive patients ( Hermida 2002 , O'Brien 2003 , White 1997a , White 1999 ). In patients who are awake during the daytime and asleep during the nighttime, their BP and HR have showed a typical circadian variation, with lower BP levels during nighttime sleep and an abrupt rise upon arising in the morning ( Pickering 1993 , White 1989 , White 1997a , White 1999 ). This pattern is rapidly reversed when individuals work night shifts and sleep during the day ( Sunderg 1988 ). It was previously reported that the morning BP surge upon arising from bed appeared to parallel the morning surge in the incidence of cardiovascular events and was significantly associated with a greater target organ damage and higher cardiovascular events risk ( Kario 2003 , Kuwajima 1995 , Muller 1989 , White 2001 ). Based on this rationale, it is hypothesized that antihypertensive medication targeted for early morning BP control in addition to providing 24\u2010hour BP control would result in a significant reduction of cardiovascular events in hypertensive patients. In other words, the medication is considered to lower BP consistently as well as reduce excessive peaks in pressure that may pose an additional cardiovascular risk. Why it is important to do this review Based on the above mentioned relationship, researchers began to apply the science of chronotherapeutics, or timing of drug effect to the treatment of essential hypertension to improve cardiovascular outcomes. A number of studies investigated the administration\u2010time\u2010dependent antihypertensive efficacy, e.g. ACEIs such as ramipril ( Hermida 2009a , Myburgh 1995 ), trandolapril ( Kuroda 2004 ), perindopril ( Morgan 1997 ), and quinapril ( Palatini 1992 ); CCBs such as diltiazem ( Glasser 2003 ), nifedipine ( Hermida 2007 , Hermida 2008 , Hermida 2009b ), cilnidipine ( Kitahara 2004 ), nisoldipine ( White 1999a ) and amlodipine ( Nold 1998 , Qiu 2003 ); diuretics such as torasemide ( Calvo 2006a , Hermida 2008a ); ARBs such as valsartan ( Hermida 2003 , Hermida 2005a , Hermida 2005b ), telmisartan ( Hermida 2007a ), and olmesartan ( Hermida 2009 ); \u03b1\u2010blockers such as doxazosin ( Hermida 2004 ). A few clinical trials had found that nighttime dosing was more effective than morning administration to optimize morning BP control while maintaining 24\u2010hour efficacy ( Glasser 2003 , White 2004 ), but another trial found no difference in morning SBP between the two groups ( Wright 2004 ). One large trial compared verapamil versus atenolol or HCTZ on reduction of BP and cardiovascular risk ( Black 2003 ). A study in hypertensive rats showed that dosing an ACE inhibitor, trandolapril, at night, had a better organ protective effect than dosing in the morning ( Sugimoto 2001 ). Clinical trials have been performed in hypertensive patients by changing the time of dosing from morning to evening to enhance their effectiveness on cardiovascular outcomes ( Fujimura 1999 ). There are also a few non\u2010systematic or traditional reviews focusing on this issue ( Ezeugo 2009 , Hermida 2007C , Hermida 2007d , Ohmori 2005 , Stergiou 2007 ), some of which reported that nighttime administration of antihypertensive drugs had a larger blood pressure lowering effect during nighttime and the early morning hours. There is considerable evidence that the morning administration gives its full effect during daytime activities and a lesser effect during nighttime and the early morning hours, whereas bedtime administration has a larger effect during nighttime and the early morning hours. However, no systematic review and meta\u2010analysis has been conducted to confirm these findings. It might be argued that bedtime administration should be considered as an alternative strategy that has the potential benefits to provide more effective cardiovascular protection.",
        "summary": "Patients with primary hypertension and a blood pressure > 140/90 mm Hg may experience a greater short\u2010term reduction in 24hr mean blood pressure with an evening dose of antihypertensive medication than with a morning dose. However, none of the trials addressed outcomes of all\u2010cause mortality, cardiovascular mortality, or cardiovascular morbidities, so the clinical importance of the reductions in blood pressure is unclear. In twenty\u2010one randomized controlled trials involving 2152 participants there were modestly greater blood pressure reductions over a period of 4 to 12 weeks with an evening rather than a morning dose of antihypertensive medication (systolic mean difference: \u20101.71 mmHg, 95%CI \u20102.78 to \u20100.65 mmHg; diastolic mean difference: \u20101.38 mmHg, 95%CI \u20102.13 to \u20100.62 mmHg). There was no evidence of a difference between groups in overall adverse events (5 RCTs, 702 participants) or withdrawals due to adverse events (6 RCTs, 1042 participants), however, event rates were low and these analyses were likely to have been too underpowered to detect clinically meaningful differences if present. Subgroup analyses by antihypertensive therapeutic class further reduced the sample sizes, and consequently the power of the analyses, making it impossible to draw conclusions."
    },
    "CD001534": {
        "query": "How do different antibiotics compare for preventing recurrence of symptomatic urinary tract infection (UTI) in children?",
        "document": "Background Description of the condition Acute urinary tract infection (UTI) is common in children. By the age of seven years, 8.4% of girls and 1.7% of boys will have suffered at least one episode ( Hellstrom 1991 ). Death is now a rare complication but hospitalisation is frequently required (40%), particularly in infancy ( Craig 1998 ). Transient damage to the kidneys occurs in about 40% of children ( Craig 1998 ), and permanent damage occurs in about 5% ( Coulthard 1997 ), sometimes even following a single infection. Symptoms are systemic rather than localised in early childhood and consist of fever, lethargy, anorexia, and vomiting. UTI is caused by Escherichia coli in over 80% of cases ( Rushton 1997 ) and treatment consists of a course of antibiotics. Children who have had one infection are at risk of further infections. Recurrent UTI occurs in up to 30% ( Winberg 1975 ). The risk factors for recurrent infection are vesicoureteric reflux (VUR), bladder instability and previous infections ( Hellerstein 1982 ; Rushton 1997 ). Recurrence of UTI occurs more commonly in girls than boys ( Bergstrom 1972 ; Winberg 1975 ). Due to the unpleasant acute illness caused by UTI and the risk of pyelonephritis\u2010induced permanent kidney damage, many children are given long\u2010term antibiotics aimed at preventing recurrence. Cotrimoxazole, nitrofurantoin and trimethoprim are commonly used for this purpose. These medications may cause side effects and promote the development of resistant bacteria. Description of the intervention Various low dose antibiotics have been used as prophylactic treatment in children, common options include; trimethoprim/sulphamethoxazole (2mg/kg/day /10 mg/kg/d) and nitrofurantoin (1 to 2 mg/kg/d). Other less frequently used antibiotics include cefadroxil (12.5 to 15 mg/kg/d), nalidixic acid (30 mg/kg/d), pivmecillinam (100 to 200 mg/d), cefixime (2 mg/kg), and co\u2010amoxiclav (15 mg/kg/d) and probably others. Durations of treatment range from one month to several years. How the intervention might work Theoretically, maintaining a small amount of antibiotic in the body could prevent bacteria growing out of control and causing illness. Why it is important to do this review Low dose antibiotic prophylaxis has been used to prevent recurring UTIs in children for many years. Anecdotal evidence and a cohort study ( Craig 1998 ) has suggested some children on prophylactic antibiotics experience recurrence despite the treatment and theoretical concerns over bacterial resistance to such long term use of antibiotics were also raised.",
        "summary": "RCT evidence is insufficient to allow definitive conclusions about the most effective antibiotic for preventing recurrence of UTI in children without underlying urinary pathology. Nitrofurantoin has shown some benefit over trimethoprim\u2010based regimens . Nitrofurantoin appears superior to trimethoprim\u2010sulfamethoxazole (co\u2010trimoxazole; TMP/SMX) in preventing recurrence of UTI (on average, 258 per 1000 children taking nitrofurantoin vs 455 per 1000 taking co\u2010trimoxazole; moderate\u2010certainty evidence). Nitrofurantoin may reduce the rate of repeat positive urine cultures compared with trimethoprim (based on one RCT at high risk of bias). The effect of nitrofurantoin compared with cefixime on repeat positive urine cultures is unclear. Nitrofurantoin likely results in a decrease in development of antimicrobial resistance when compared with TMP\u2010SMX (on average, 406 vs 757 children per 1000; moderate\u2010certainty evidence). Nitrofurantoin may lead to more adverse events than trimethoprim (low\u2010certainty evidence from a single RCT with 120 participants) but fewer adverse events than cefixime (single RCT with 60 participants at high risk of bias; evidence of unclear certainty). The effect of TMP/SMX compared with cefadroxil or cefprozil on recurrence of UTI and on adverse events remains unclear. This review identified no trials assessing the impact of antibiotic choice on hospitalization for UTI. No trials compared different antibiotics among children with underlying urinary pathology."
    },
    "CD009631": {
        "query": "What are the effects of heparin alternatives for the prevention of central venous hemodialysis catheter malfunction in patients with end\u2010stage kidney disease?",
        "document": "Background Central venous haemodialysis catheters are a necessary but problematic component of dialysis practice. Central venous catheters (CVC) are used for approximately 57% of incident dialysis patients in Australia among whom they are associated with significantly increased dialysis\u2010related mortality ( Hariharan 2006 ; Polkinghorne 2013 ; Schwab 1999 ; USRDS 2009 ). CVC are associated with the risk of catheter\u2010related thrombosis which can result in catheter malfunction ( Suhocki 1996 ). Description of the condition Catheter\u2010related thrombosis can be classified as either extrinsic or intrinsic ( Beathard 2001 ) based on the site at which the thrombosis forms. The main consequences from catheter\u2010related thrombotic events are deep venous thrombosis ( Vanherweghem 1994 ), shortened access life and requirement for extra procedures ( Linenberger 2006 ), inadequate dialysis ( Little 2001 ) and increased risk of sepsis ( Timsit 1998 ). The incidence of catheter\u2010related thrombosis varies by catheter location ( Trerotola 2000 ), sex, systemic prothrombotic states, site of insertion (subclavian compared with internal jugular) ( Trerotola 2000 ), previous catheter\u2010related thrombosis and catheter malposition ( Liangos 2006 ; Trerotola 2000 ). Description of the intervention Current guidelines recommend antithrombotic locking solutions to prevent catheter\u2010related malfunction in dialysis patients but do not refer to specific agents or concentrations in recognition of the lack of definitive evidence for individual regimens ( UK Renal Association 2011 ; KDOQI 2006 ). Newer approaches including alternative anticoagulant containing locking solutions, antibiotic containing lock solutions, systematic anticoagulants, antiplatelet agents ( Abdul\u2010Rahman 2007 ), catheter flushing regimes ( Pepper 2007 ) and recombinant tissue\u2010type plasminogen activator (rt\u2010PA) ( Schenk 2000 ) have been investigated to seek improvements in catheter patency or treatment\u2010associated harm rates. How the intervention might work Standard care in the prevention of catheter malfunction is the use of heparin solutions as a post dialysis \u2018lock\u2019 in the catheter ports. Heparin is a mucopolysaccharide with in vitro and in vivo anticoagulant properties. It exerts anticoagulant effect by deactivating activated factor X and inhibiting conversion of prothrombin to thrombin. Heparin locking solutions have been recommended for maintaining catheter patency, though the nominated wide concentration range of 1000 to 10,000 U/mL reflects the lack of evidence on optimal dosing ( Besarab 2011 ). The potential impact on bleeding risk is an acknowledged concern ( Moritz 2003 ; Yevzlin 2007 ). Other adverse events associated with heparin use include major bleeding, heparin\u2010induced thrombocytopenia and thrombosis, and osteoporosis ( Thomas 2007 ). Alternative approaches, therefore, have been tested to improve catheter function and reduce adverse events. These potentially include alternative anticoagulant based locking solutions (e.g. citrate locking solutions), systemic anticoagulation, and impregnated catheters. These may improve efficacy by their anticoagulant properties. For example, solutions containing 4% to 5% of trisodium citrates express anticoagulant activity ( von Brecht 1986 ) by binding Ca\u00b2 + to prevent progression of the coagulation cascade ( Pinnick 1983 ). Alternative approaches to reducing adverse events include reduced heparin or no heparin saline flushes. Why it is important to do this review Catheter thrombosis is associated with negative outcomes including reduced dialysis adequacy, requirement for repeated invasive interventions, increased risk of catheter\u2010related bacteraemia and higher rates of hospitalisation and mortality.",
        "summary": "There appears to be limited benefit of anticoagulant antilocking solutions over unfractionated heparin, placebo or no treatment. However, given the failure of trials to report key methodological aspects, no firm conclusions can be drawn. When anticoagulant antilocking solutions other than unfractionated heparin (citrates, recombinant tissue plasminogen activators (rt\u2010PA), antibiotics, low molecular weight heparin, ethanol), systemic agents (aspirin, warfarin) or low dose/no heparin were used to maintain patency of central venous catheters (CVCs) in people with end\u2010stage kidney disease, the rate of CVC loss, bleeding (total, major or minor) and mortality appeared to be similar when compared with unfractionated heparin, placebo or no treatment. However, there may be a benefit of alternative antilocking solutions in terms of avoiding CVC\u2010bacteremia. Although overall no benefit of antilocking solutions was observed for avoiding the use of rescue thrombolytic agents, subgroup analysis suggest that rt\u2010PA may be beneficial whilst citrates may be detrimental. Non\u2010bleeding adverse events may be more commonly associated with citrate locking solutions (including thrombocytopenia, intermittent nonspecific dizziness, metallic taste, facial and/or digital paresthesia), but these were reported in single studies and data were not provided. The overall quality of the evidence was not assessed; however, the trials failed to report key aspects of trial methodology, making the risk of bias and reliability of the trials uncertain, calling for caution when interpreting the results."
    },
    "CD001480": {
        "query": "Can the PHiD\u2010PC10 pneumococcal conjugate vaccine in early infancy help prevent otitis media in children?",
        "document": "Background Description of the condition Acute otitis media (AOM), defined as the presence of middle ear fluid together with one or more signs or symptoms of acute middle ear inflammation such as otalgia, otorrhoea, fever, or irritability, is one of the most common diseases in childhood and imposes a large burden on public health ( Lieberthal 2013 ). Global AOM incidence rates are highest in children one to four years of age, with a peak incidence in six\u2010 to 11\u2010month\u2010old infants ( Monasta 2012 ). By the age of two years, up to 5% of all children have experienced recurrent AOM, defined as three or more AOM episodes in six months or four or more in one year ( Kvaerner 1997 ; Lieberthal 2013 ). The three main bacterial pathogens isolated from the middle ear fluid of children with AOM collected before the widespread use of pneumococcal conjugate vaccines (PCVs) were Streptococcus pneumoniae (25% to 39%), (non\u2010typeable) Haemophilus influenzae (12% to 23%), and Moraxella catarrhalis (4% to 15%) ( Bluestone 1992 ; Heikkinen 1999 ; Jacobs 1998 ; Luotonen 1981 ). Recent studies have shown that nationwide implementation of PCVs may have changed the frequency of the causative otopathogens involved in AOM towards pneumococcal serotypes not included in the vaccines and other bacteria including non\u2010typeable H influenzae ( Allemann 2017 ; Barenkamp 2017 ; Casey 2013 ; Coker 2010 ; Kaur 2017 ; Somech 2011 ; Tamir 2015 ; Wiertsema 2011 ). Description of the intervention The marginal benefits of antibiotics for AOM in low\u2010risk populations ( Rovers 2006 ; Venekamp 2015 ); the increasing problem of bacterial resistance against antibiotics ( Laxminarayan 2013 ); and the high estimated direct and indirect annual costs associated with AOM have prompted a search for effective vaccines to prevent this condition ( Ahmed 2014 ; Boonacker 2011 ). With S pneumoniae (pneumococcus) being a common causative pathogen in childhood AOM and pneumonia, and one of the most frequent causes of invasive bacterial disease such as bacteraemia and meningitis, research has focused on the prevention of pneumococcal infections by pneumococcal vaccines. Pneumococcal polysaccharide vaccines (PPVs) have been available for decades, but have been shown to be poorly immunogenic in children aged up to two years, who are most prone to pneumococcal infections. In the most recent versions of this review, no further attention has been paid to the effect of PPVs, which were described in prior versions of this review ( Straetemans 2003 ). The first pneumococcal conjugate vaccines (PCVs), in which the pneumococcal capsular serotypes are covalently conjugated to carrier proteins, were developed in the 1990s and proved to be adequately immunogenic in infants and toddlers ( Dagan 1997 ; Eskola 1999 ; Shinefield 1999 ). Over the past decades, various PCVs have been developed for use in children including: licenced 7\u2010valent PCV containing the polysaccharides of seven serotypes (4, 6B, 9V, 14, 18C, 19F, and 23F) conjugated to the diphtheria\u2010derived carrier protein CRM197 ( CRM197\u2010PCV7 ); 7\u2010valent PCV with the outer membrane complex of Neisseria meningitidis serogroup B as carrier protein ( OMPC\u2010PCV7 ); 9\u2010valent PCV containing the capsular polysaccharides of serotypes 1 and 5 in addition to those included in PCV7, conjugated to CRM197 ( CRM197\u2010PCV9 ); licenced 10\u2010valent PCV containing the capsular polysaccharides of 10 serotypes (1, 4, 5, 6B, 7F, 9V, 14, 18C, 19F, and 23F) mostly conjugated to protein D, which is a surface lipoprotein of H influenzae ( PHiD\u2010CV10 ); 11\u2010valent containing the capsular polysaccharides of serotype 3 as well as those included in PHiD\u2010CV10 ( PHiD\u2010CV11 ); and licenced 13\u2010valent PCV containing the capsular polysaccharides of 13 serotypes (1, 3, 4, 5, 6A, 6B, 7F, 9V, 14, 18C, 19A, 19F, and 23F) conjugated to CRM197 ( CRM197\u2010PCV13 ). licenced 7\u2010valent PCV containing the polysaccharides of seven serotypes (4, 6B, 9V, 14, 18C, 19F, and 23F) conjugated to the diphtheria\u2010derived carrier protein CRM197 ( CRM197\u2010PCV7 ); 7\u2010valent PCV with the outer membrane complex of Neisseria meningitidis serogroup B as carrier protein ( OMPC\u2010PCV7 ); 9\u2010valent PCV containing the capsular polysaccharides of serotypes 1 and 5 in addition to those included in PCV7, conjugated to CRM197 ( CRM197\u2010PCV9 ); licenced 10\u2010valent PCV containing the capsular polysaccharides of 10 serotypes (1, 4, 5, 6B, 7F, 9V, 14, 18C, 19F, and 23F) mostly conjugated to protein D, which is a surface lipoprotein of H influenzae ( PHiD\u2010CV10 ); 11\u2010valent containing the capsular polysaccharides of serotype 3 as well as those included in PHiD\u2010CV10 ( PHiD\u2010CV11 ); and licenced 13\u2010valent PCV containing the capsular polysaccharides of 13 serotypes (1, 3, 4, 5, 6A, 6B, 7F, 9V, 14, 18C, 19A, 19F, and 23F) conjugated to CRM197 ( CRM197\u2010PCV13 ). How the intervention might work Early and dense colonisation of the nasopharynx with bacterial otopathogens, including S pneumoniae , increases the risk of AOM substantially ( Faden 1997 ; Leach 1994 ; Schilder 2016 ). As a consequence, reducing or eliminating nasopharyngeal colonisation of S pneumoniae by PCVs may lead to reductions in AOM incidence. In recent years, evidence has accumulated that PCVs might also disrupt the continuum of evolution from pneumococcal\u2010associated otitis media (OM) towards chronic/recurrent OM by prevention of early vaccine\u2010serotype AOM and thereby reducing subsequent and more complex disease caused by non\u2010vaccine serotypes and non\u2010typeable H influenzae ( Ben\u2010Shimol 2014 ; Dagan 2016 ). Why it is important to do this review With AOM amongst the most common diseases in early childhood, the need for a vaccine to effectively prevent AOM is high. Over the past decades various randomised controlled trials have been performed to assess the effects of pneumococcal vaccination to prevent AOM. From 2009 onwards, two multivalent PCVs (PHiD\u2010CV10 and CRM197\u2010PCV13) have been licenced and are being implemented in nationwide immunisation programmes worldwide ( WHO 2012 ). These new vaccines may have an increased benefit in preventing AOM ( Marom 2014 ; O'Brien 2009 ). As such, it was important to provide an up\u2010to\u2010date systematic review on the effects of PCVs on preventing AOM. This review is an update of a Cochrane Review first published in 2002 ( Straetemans 2002 ), and updated in 2004 ( Straetemans 2004 ), 2009 ( Jansen 2009 ), and 2014 ( Fortanier 2014 ).",
        "summary": "Compared with hepatitis A and/or B vaccine in healthy infants at low risk, use of the PHiD\u2010PC10 pneumococcal conjugate vaccine in early infancy probably has little to no impact on the absolute numbers of children developing acute otitis media; however, the overall numbers of children who developed otitis media were very low (\u2264 1 episode/person\u2010year; all values on average). Researchers observed a 53% reduction in pneumococcal acute otitis media with the PHiD\u2010PC10 vaccine, which would be a subset of all\u2010cause otitis media; therefore, the absolute numbers benefiting from the PHiD\u2010PC10 vaccine are likely to be even lower. Mortality rates were also low, at \u2264 0.05% across all groups. The incidence of severe adverse events was similar with the PHiD\u2010PC10 and hepatitis B vaccines (approx. 22%); among 28,916 infants, only seven events were considered causally related to vaccination (in four infants receiving the 3 + 1 PHiD\u2010CV10 vaccine and three infants receiving the hepatitis B vaccine). All evidence was of moderate to high certainty."
    },
    "CD011673-1": {
        "query": "What are the effects of adding bevacizumab to interferon\u2010\u03b1 (IFN\u2010\u03b1) for first\u2010line treatment of people with metastatic renal cell carcinoma?",
        "document": "Background Description of the condition Kidney cancer is classified into renal cell carcinoma (RCC) and urothelial carcinoma of the renal pelvis. Kidney cancer is the 14th most common malignancy worldwide with approximately 337,800 new cases diagnosed in 2012 ( Ferlay 2013 ). In 2012, 375,925 people had kidney and pelvis cancer in the US with an estimated 63,920 newly diagnosed cancer cases and 13,860 deaths in 2014 ( Howlader 2015 ; Siegel 2014 ). Incidence is highest in Europe, North America and Australia and lowest in India, Japan, Africa and China ( Ljungberg 2011 ). In the European Union, 85,215 new cases of kidney cancer occurred in 2012 with 35,134 patient deaths ( Ferlay 2013 ). RCC is the most common tumour of the kidney comprising 90% of cases. It is a heterogeneous cancer and is classified into three major histological RCC types; clear\u2010cell RCC (70% to 85% of cases), papillary RCC (7% to 15%) and chromophobe RCC (5% to 10%) ( Escudier 2014 ). In the Western world, RCC shows an age\u2010standardized incidence rate of 5.8 per 100,000 people and a mortality rate of 1.4 per 100,000 people. Despite advances in diagnosis, about 30% of people with RCC have already developed metastatic RCC (mRCC) at presentation ( Gupta 2008 ), and another 20% of people with clinically localized RCC eventually develop metastases during the course of the disease despite treatment ( Athar 2008 ; Motzer 1996 ; Zisman 2002 ). The annual incidence of mRCC was estimated at 8567 cases in the US and 3026 cases in Germany (2002 figures). These numbers correspond to incidence rates of 3.9 (US males), 2.1 (US females), 4.7 (German males) and 2.7 (German females) per 100,000 inhabitants in these countries ( Gupta 2008 ). Prognosis of patients is directly related to the dissemination stage of the tumour. Therefore, the five\u2010year survival for people with localized RCC in the US is 92.1% decreasing to 65.4% for people with regional disease and down to 11.8% for people with mRCC ( Howlader 2015 ). The estimated economic burden of mRCC has not been adequately studied and can only be estimated from incidences and costs for all types of RCC and kidney cancer. Annual healthcare costs and lost productivity accounted for between USD 107 million to USD 556 million spent in the US whereas the worldwide mRCC cost was estimated to lie between USD 1.1 billion and USD 1.6 billion (2006 figures) ( Gupta 2008 ). In the case of metastatic disease, the central aim of treatment is to optimize improvement in quality and quantity of life. Therefore, the development of new agents with more effective antitumour activity is urgently required for the enhancement of quality of life (QoL) in people with mRCC. Description of the intervention RCC has been reported to be a highly immunogenic tumour, an observation that explains the rationale behind the application of immunotherapy to promote an antitumour effect ( Michael 2003 ; Rayman 2004 ). Due to high levels of intratumoural immune cell infiltration and spontaneous remission rates, various immunotherapeutic approaches have been developed for the treatment of this disease. Most studies have focused on the implementation of non\u2010specific cytokines, such as interferon (IFN)\u2010\u03b1 and interleukin (IL)\u20102 and their combinations. Studies have so far produced inconsistent results and failed to define a globally recognized, standardized immunotherapy regimen for metastatic disease ( Johannsen 2007 ). Since the mid\u20102000s, the transformation of mRCC treatment following advances that led to improved understanding of RCC biology and the approval of targeted agents inhibiting the vascular endothelial growth factor (VEGF) and mammalian target of rapamycin (mTOR) signalling pathways. This has led to a switch from cytokine\u2010based therapies to targeted therapies in the treatment of mRCC, a change that is further highlighted by the omission of cytokine monotherapy from current evidence\u2010based guidelines of the European Association of Urology ( Ljungberg 2015 ). For most people with metastatic disease, cytoreductive nephrectomy alone is merely palliative and thus, additional systemic treatments are necessary. Current guidelines recommend systemic treatment with targeted therapies (sunitinib, bevacizumab plus IFN\u2010\u03b1, pazopanib, temsirolimus, sorafenib, everolimus and axitinib) according to histology, patient risk stratification and treatment line ( Ljungberg 2015 ). Despite remarkable improvements in progression\u2010free survival (PFS) and objective response rates (ORR) with targeted therapies, an increase in complete remission of mRCC has not been achieved perhaps due to intrinsic or acquired drug resistance of patients ( Abe 2013 ). The novel immune\u2010mediated therapeutic options block the immunosuppressive cancer mechanisms culminating in the stimulation of the host antitumour immune response leading to long\u2010term, persistent tumour destruction ( Draube 2011 ; Postow 2015 ). This review summarizes pivotal studies reported since the last version of this review that demonstrate the superiority of targeted agents over IFN\u2010\u03b1 as first\u2010line treatment ( Coppin 2007 ; Hudes 2007 ; Motzer 2007 ), including studies focusing on the possible synergy between therapeutic vaccines and antiangiogenic agents ( Amato 2010 ; Rini 2015 ), personalized immunotherapy ( Figlin 2014 ), or immune checkpoint inhibitors against current standard therapy options ( Motzer 2015a ). How the intervention might work The better understanding of the tumour microenvironment and of T\u2010cell responses has led to the development of specific immunotherapeutic strategies and as such, a new class of cancer immunotherapy agents, known as immune checkpoint inhibitors, is at the focus for clinical application ( Bedke 2014 ; Postow 2015 ). These agents mainly comprise of antibodies that target inhibitory molecules such as the cytotoxic T\u2010lymphocyte\u2010associated antigen 4 (CTLA\u20104), or the programmed death protein 1 pathway (PD\u20101 and its ligand PD\u2010L1). The expression of these inhibitory co\u2010receptors on T lymphocytes can cause complete suppression or weakening of the antitumour T\u2010cell responses. It is now recognized that people with mRCC characterized by PD\u20101\u2010positive cancer\u2010infiltrating lymphocytes often have larger and more aggressive tumours ( Thompson 2007 ). Furthermore, PD\u2010L1 expression by RCC cancer cells is often associated with a worse clinical outcome of these patients. Vaccines are another alternative in immunotherapy for the treatment of mRCC. The aim of vaccines is the activation of immune cells to recognize and destroy tumour cells. Vaccination of RCC patients with synthetic peptides representing epitopes derived from tumour\u2010associated antigens (TAA) and recognized by T\u2010cell receptors has been shown to induce a well\u2010defined T\u2010cell response ( Brookman\u2010May 2011 ; Dutcher 2013 ). Different vaccination strategies are under development including cell\u2010based vaccines that utilize either tumour cells or dendritic cells and cell\u2010free vaccines that are based on the application of TAA. Furthermore, studies have proposed that the addition of immune\u2010modulator drugs such as cyclophosphamide can enhance the infiltration of vaccine\u2010induced effector T cells into tumours ( Walter 2012 ). Why it is important to do this review This Cochrane Review serves as the latest update of the Cochrane Review first published in 2000 and previously updated in 2005 and 2007 ( Coppin 2000 ; Coppin 2005 ; Coppin 2007 ). In summary, the old review indicates that no cytokine\u2010based immunotherapy is significantly effective for advanced RCC. IFN\u2010\u03b1 and high\u2010dose interleukin\u20102 (HD\u2010IL\u20102) are of unknown survival benefit prior to current first\u2010line therapy of mRCC with targeted agents. HD\u2010IL\u20102 has been associated with durable complete responses in a small number of patients, but it is of limited use due to its severe toxicity. Furthermore, no clinical factors or biomarkers exist to accurately predict a durable response in patients treated with HD\u2010IL\u20102 ( McDermott 2015a ). The update will focus on the current role of non\u2010specific cytokines and implementation of new, specific immunotherapeutic approaches for treatment of people with mRCC. Comparisons were made against the current standard of care options ( Ljungberg 2015 ). Despite the availability of targeted therapies inhibiting angiogenesis or signal transduction pathways, progress with these agents has reached a plateau and therapy remains non\u2010curative. Stadler 2014 refers to the 'maturing' of RCC therapy suggesting little progress has been made beyond fine\u2010tuning the choice and order of the targeted agents. As a result of this as well as an enhanced understanding of the complex interaction between cancer and host cells (e.g. of the ability of cancer cells to evade immune surveillance or the efficacy of checkpoint inhibitors, such as ipilimumab and nivolumab), interest in immunotherapy has rekindled. Novel therapeutic options are focusing on the possible synergy between standard targeted therapy and immunotherapeutic agents or vaccine approaches ( Combe 2015 ). The key aims of this review were: 1. to determine the role of non\u2010specific and new immunotherapies in the development of standard of care guidelines and their place in the current management of mRCC and 2. determine which immunotherapeutic approach, either alone or in combination with standard targeted therapies, is the most efficient to maximize patient benefit. We focused on the entire body of evidence for our clinical question, as well as on patient\u2010important outcomes and used the GRADE approach to rate quality of evidence ( Guyatt 2011a ).",
        "summary": "For people undergoing first\u2010line therapy for metastatic renal cell carcinoma (most had a clear\u2010cell component, prior nephrectomy, and Eastern Cooperative Oncology Group [ECOG] performance status 0 or 1), adding bevacizumab to IFN\u2010\u03b1 reduced one\u2010year mortality (344 vs 402 per 1000 people; all values on average; low\u2010certainty evidence), slightly increased overall survival time (by approx. 1 to 2 months), and increased the tumor remission rate (351 vs 138 per 1000 people). Progression\u2010free survival (PFS) was also better with IFN\u2010\u03b1 plus bevacizumab than with IFN\u2010\u03b1, but researchers did not report absolute numbers and median PFS times. In contrast, IFN\u2010\u03b1 was associated with fewer adverse events grade 3 or above than IFN\u2010\u03b1 plus bevacizumab (618 vs 801 per 1000 people; moderate\u2010certainty evidence). Researchers did not assess quality of life. The comparison of IFN\u2010\u03b1 plus bevacizumab with standard targeted therapy included only 83 people and was therefore underpowered to be clinically meaningful."
    },
    "CD002896": {
        "query": "In people with drug\u2010resistant partial seizures, how does vagus nerve stimulation using high stimulation compare with low stimulation at improving outcomes?",
        "document": "Background This review is an update of a previously published review in The Cochrane Database of Systematic Reviews (The Cochrane Library, Issue 7, 2010). Description of the condition Epilepsy is a condition characterized by a tendency for recurrent seizures unprovoked by any known proximate insult. Epileptiform discharges involve either a localized area of the brain resulting in a partial seizure, or the entire brain resulting in a generalized seizure. The prevalence of epilepsy is estimated to be five to eight per 1000 population in developed countries, and in adults the most common type is partial epilepsy ( Hauser 1975 ; Forsgren 2005 ). The majority of people given a diagnosis of epilepsy have a good prognosis, and their seizures will be controlled by treatment with a single antiepileptic drug (AED). However, 20% (reported in population\u2010based studies) to 30% (reported in clinical (non\u2010population\u2010based) series) will develop drug\u2010resistant epilepsy ( Cockerell 1995 ; Kwan 2000 ), often requiring treatment with combinations of antiepileptic drugs (AEDs). These patients tend to have frequent, disabling seizures that limit their ability to work and participate in activities. Many of these patients also suffer from the chronic effects of long\u2010term, high\u2010dose AED polytherapy, while anxiety and depressive disorders are common in patients with epilepsy.The development of effective new therapies for the treatment of refractory seizures is therefore of considerable importance. Description of the intervention Vagus nerve stimulation (VNS) is a neuromodulatory treatment that is used as an adjunctive therapy for people with medically refractory epilepsy who are not eligible for epilepsy surgery or in whom surgery has failed. In this procedure, a pacemaker\u2010like device \u2010 the Neuro\u2010cybernetic Prosthesis (NCP) \u2010 is implanted under the skin of the chest. The stimulating electrodes of the NCP carry electrical signals from the generator to the left vagus nerve. By programming the device, the frequency, intensity, and duration of stimulation can be varied (the stimulation paradigm). In the initial trials, the vagus nerve was stimulated for 30 seconds, every five minutes ( Sackeim 2001 ). During each 30\u2010second stimulation, the device delivered 500 microsecond pulses at 30 Hz frequency. For each individual, the intensity of the current was set at the highest that was tolerable, or to low intensity stimulation, depending on the allocated treatment group. Also, in an attempt to further abort seizures, patients could activate the device by placing a magnet over it when a seizure had occurred, or was about to occur. Participants enrolled into the initial randomised controlled trials of VNS had drug\u2010resistant partial epilepsy, and experienced a 24% to 28% median reduction in seizure frequency over a three month treatment period ( Selway 1987 ). How the intervention might work Left VNS is a promising, relatively new treatment for epilepsy. In 1997, VNS was approved in the United States as an adjunctive treatment for medically refractory partial\u2010onset seizures in adults and adolescents. For some patients with partial\u2010onset seizures, the adverse effects of antiepileptic drugs (AEDs) are intolerable; for others, no single AED or combination of anticonvulsant agents is effective. Cerebral resective surgery is an alternative to pharmacotherapy in some cases, but many patients with partial\u2010onset seizures are not optimal candidates for intracranial surgery ( Schachter 1998 ). The mechanism of action of VNS is not fully understood, but can be reasonably assumed to involve brainstem nuclei. The nucleus of the solitary tract, the main terminus for vagal afferents, has direct or indirect projections to the locus coeruleus, raphe nuclei, reticular formation, and other brainstem nuclei. These nuclei have been shown to influence cerebral seizure susceptibility, hence vagal modulation of one or more of these nuclei could plausibly represent the mechanism for seizure suppression ( Krahl 2012 ). In this context, the immunomodulatory function of the vagus nerve is of particular interest. Afferent signals can activate the so\u2010called cholinergic anti\u2010inflammatory pathway upon inflammation. Through this pathway, efferent vagus nerve fibres inhibit the release of pro\u2010inflammatory cytokines and in this way reduce inflammation. In recent years, inflammation has been strongly implicated in the development of seizures and epilepsy and therefore the activation of the anti\u2010inflammatory pathway by VNS could decrease the inflammatory response and thereby explain its clinical effects. In addition to anticonvulsive effects, VNS might have positive effects on behavior, mood and cognition ( Vonck 2014 ). Why it is important to do this review In this review we summarise evidence from randomised controlled trials where the efficacy and tolerability of VNS for people with drug\u2010resistant partial epilepsy have been investigated in order to aid clinical decision making when considering VNS treatment within this population.The aim was to evaluate the effects of VNS on seizure frequency and its tolerability and safety when used as an add\u2010on treatment for people with medically refractory epilepsy.",
        "summary": "The randomized controlled trials comparing high versus low vagus nerve stimulation in people with drug\u2010resistant partial epilepsy may not be applicable to current clinical practice. Since these studies were published, numerous new anti\u2010epileptic medications for partial seizures have been released. Therefore, the patients in the trials may not be representative of patients who would at this time be defined as having drug\u2010resistant partial seizures. Moderate\u2010quality evidence shows that more people receiving vagus nerve stimulation using high stimulation rather than low simulation had a 50% or greater reduction in the frequency of drug\u2010resistant partial seizures. Low\u2010quality evidence suggested that withdrawals were rare in both stimulation groups, suggesting that VNS was well tolerated, although data available on this outcome were limited so the comparative overall adverse effects of high and low stimulation are unclear. Voice alteration and dyspnea occurred more frequently with high stimulation. Other adverse effects associated with VNS implantation, regardless of stimulation level, were cough, pain and paresthesia. Only two studies reported quality of life, which favored VNS stimulation, but with no apparent difference between high and low stimulation. Furthermore, these studies only covered a short treatment follow\u2010up, with the longest period of 20 weeks. All included studies were sponsored by the VNS manufacturer."
    },
    "CD003169": {
        "query": "How do multifocal and monofocal intraocular lenses compare after cataract extraction?",
        "document": "Background Description of the condition Cataract, defined as the presence of visually impairing lens opacity in one or both eyes, is present in 30% of people aged 65 years and over in the UK ( Desai 1999 ). Around 400,000 cataract extractions were performed in England in the year 2014 to 2015 ( Department of Health 2015 ). People with cataract usually present with one or more of the following symptoms: gradual reduction in visual acuity (VA), glare, change in glasses prescription and change in colour appreciation. The diagnosis may be made by the person's general practitioner or optometrist followed by referral to an ophthalmic surgeon for confirmation of the diagnosis and management. Many people with treatable visual impairment from cataract do not access health services ( Desai 1999 ). Description of the intervention Cataracts causing only mild symptoms may not need treatment, while changes in glasses prescription due to cataract may simply be managed by the provision of new glasses. Where these options are inadequate the only treatment available is surgical extraction of the cataract. This is routinely accompanied by implantation of an intraocular lens (IOL) to replace the focusing power of the natural lens. Current techniques of cataract surgery and IOL implantation allow accurate prediction of postoperative refraction. Existing standards of best\u2010corrected postoperative VA ( Desai 1993 ) are being replaced by an expectation of good uncorrected distance acuity. This has been driven partly by the change from cataract surgery using a large (10 mm) incision to small incision (2 mm to 4 mm) phacoemulsification surgery. This change is generally perceived to offer greater predictability of refractive outcomes, a necessary pre\u2010requisite for good VA without the need for glasses. Cochrane systematic reviews comparing surgical approaches have been published ( Ang 2012 ; Riaz 2013 ; de Silva 2014 ). Because standard IOLs have a fixed refractive power the focal length is also fixed (monofocal). This means that most people will require a reading addition to their distance glasses prescription ( Javitt 1997 ). While most people undergoing cataract surgery may be happy to use reading glasses, a proportion are likely to seek good unaided near vision as well as distance vision. The need for reading glasses for near vision is unlikely to be considered an important issue at present in low\u2010income countries where the burden of blindness due to cataract is so high. How the intervention might work One approach to improve near VA is to modify the IOL. There are no IOLs currently available that can change shape during accommodation in the manner of the natural crystalline lens. A fixed\u2010shape optic IOL could theoretically provide near vision if attempted accommodation resulted in forward displacement of the IOL. Efforts to design an IOL using this principle have so far been unsuccessful ( Legeais 1999 ). An IOL can also provide near and distance vision if both powers are present within the optical zone. This has been attempted using diffractive optics or with zones of differing refractive power. Both types of IOL divide light up to focus at two (bifocal) or more (multifocal) points so that the person can focus on objects at more than one distance from them. IOLs of both types are currently commercially available. Optical evaluation of multifocal IOLs has been performed in detail. Exact figures vary with the IOL tested but essentially a two\u2010 to three\u2010fold increase in the depth of field is achieved at the expense of a 50% reduction in the contrast of the retinal image ( Holladay 1990 ; Lang 1993 ). Clinical evaluation of a multifocal IOL is less clear\u2010cut. Several large studies, including non\u2010randomised comparisons with monofocal IOLs, have indicated that the quality of vision with bifocal and multifocal IOLs is good ( Gimbel 1991 ; Knorz 1993 ; Lindstrom 1993 ; Steinert 1999 ). The key question to be answered is whether the optical trade\u2010off inherent in a multifocal IOL results in better or worse visual function compared to a monofocal IOL. Objective ( Desai 1993 ) and subjective ( Desai 1996 ) improvement in vision following cataract surgery with monofocal IOL implantation is so high that any study lacking a randomised control group as a comparator will be relatively uninformative. Why it is important to do this review There is an extensive body of published data on both monofocal and multifocal IOLs describing largely successful outcomes. To draw some conclusions regarding the relative merits of the different IOL types we undertook a systematic review of the best quality data (that from randomised controlled trials).",
        "summary": "Visual acuity results from trials comparing multifocal and monofocal intraocular lenses in adults with cataracts often favor multifocal lenses but are inconsistent. In addition, increases in glare, halos, and cataract symptoms may occur with multifocal lenses. Therefore, a patient's preferred balance between visual acuity without glasses for both close and distant tasks and acceptability of symptoms of glare and halos must inform the decision. Use of multifocal intraocular lenses in adults with unilateral or bilateral juvenile or senile cataract (most studies bilateral and/or senile) frequently led to no apparent benefit in terms of distance visual acuity (unaided, corrected) when compared with monofocal lenses, but results were inconsistent across trials. Results for unaided or corrected intermediate and unaided near visual acuity were more convincing, with most trials reporting greater improvement with multifocal intraocular lenses (on average, by \u20100.57 to \u20100.08 logMARs across trials, favoring multifocal intraocular lenses). However, results for corrected near visual acuity were also inconsistent across trials, and increases in glare (254 vs 180 per 1000 people; low\u2010quality evidence), halos (286 vs 80 per 1000 people; moderate\u2010quality evidence), and cataract symptoms (by 1 point on a 5\u2010item score; two randomized controlled trials; 257 participants) with multifocal intraocular lenses make it difficult to draw conclusions as to which type of lens provides better overall performance. People should be counseled on their visual requirements postoperatively to balance their desire for better unaided intermediate and close vision with increased chance for symptoms of halos and glare with a multifocal intraocular lens. Multifocal intraocular lenses are more expensive at present (2017) and are not routinely available through the National Health Service in the UK."
    },
    "CD009016": {
        "query": "Can active body surface warming systems help to prevent complications caused by inadvertent perioperative hypothermia in adults?",
        "document": "Background Description of the condition In healthy individuals, the mean body temperature varies between 36.1\u00baC and 37.4\u00baC. Maintaining body temperature means maintaining a balance between production and loss of heat. Heat is generated continuously as a product of the body\u2019s metabolism. Regulation of temperature is done through a feedback mechanism in the central nervous system. The hypothalamus acts as a 'biological thermostat', noting temperature changes and initiating thermal regulation aimed at increasing or decreasing overall body temperature. During rest (anaesthesia would be an extreme case of rest), the greatest amount of heat comes from the metabolic activity of the brain and the other major organs. All heat generated by metabolism is dissipated to the environment (mainly through the skin) in order to maintain a stable thermal condition. In clinical doses, both sedatives and anaesthesia inhibit thermal regulatory responses (primarily vasoconstriction). The physiological thermal regulating mechanisms are not shut off but the thermal thresholds through which the usual responses start are altered. In this way, general anaesthesia produces vasodilatation by depressing vasoconstrictor responses. Since the thermal regulation mechanisms are inhibited, the central compartment goes through a progressive loss of heat, which is transmitted to the peripheral compartment. The speed of this transfer as well as the amount of heat lost depends on the difference in temperature between the two compartments. Vasodilatation in the peripheral compartment brings about the loss of heat to the environment, which as a consequence helps to cool down the central compartment. This process of caloric transfer is known as redistribution. The combination of reduced heat production and surgical, anaesthetic and environmental factors that increase heat loss can cause hypothermia in the patient. Intraoperative hypothermia is defined as a central body temperature below 36\u00baC. At the beginning of general anaesthesia, the overall body temperature does not change, since the temperature loss in the central compartment is picked up by the peripheral compartment. By the second hour of anaesthesia the heat loss in the central compartment is slower, and in this phase the loss of body heat to the environment is more important. Overall temperature decreases when more heat is lost than is generated. The people who are most susceptible to heat loss are the elderly, patients at higher anaesthetic risk (American Society of Anesthesiologists (ASA) grade 3 to 4), cachectics, burn victims, people with hypothyroidism, and those affected by corticoadrenal insufficiency. Hypothermia may increase morbidity as a result of altering various systems and functions within the organism. Cardiac complications are the principal cause of morbidity during the postoperative phase. Prolonged ischaemia is usually associated with cellular damage, and for this reason it is important to prevent factors that can lead to this complication, such as decreased body temperature. Hypothermia stimulates and amplifies adrenergic responses with the release of noradrenaline, which results in peripheral vasoconstriction and hypertension ( Sessler 1991 ; Sessler 2001 ) and increases the chances of myocardial ischaemia. Some studies have shown that intraoperative hypothermia, accompanied by vasoconstriction, constitutes an independent factor that slows wound healing and increases the incidence of surgical site infection ( Kurz 1996 ; Melling 2001 ). Even moderate hypothermia (35\u00baC) can alter physiologic coagulation mechanisms by affecting platelet function and modifying enzymatic reactions. Decreased platelet activity produces an increase in bleeding and greater need for transfusion ( Rajagopalan 2008 ). Moderate hypothermia can also reduce the metabolic rate, manifesting as a prolonged effect of certain drugs used during anaesthesia and some uncertainty about their effects. This is particularly significant in elderly people ( Heier 1991 ; Heier 2006 ; Leslie 1995 ). Patients often comment on shivering upon awakening from anaesthesia, identifying this as one of the most uncomfortable immediate postoperative experiences. Shivering is a response to cold and is the result of involuntary muscular activity, the purpose of which is to increase metabolic heat ( Sessler 2001 ). Due to the above reasons, inadvertent non\u2010therapeutic hypothermia is considered an adverse effect of general and regional anaesthesia ( Bush 1995 ; Putzu 2007 ; Sessler 1991 ). The monitoring of body temperature is essential for maintaining normothermia during surgery and for timely detection of the appearance of unintended hypothermia. As a result, the monitoring of body temperature is included as one of the items in the surgical safety checklist of the World Health Organization guidelines ( WHO 2015 ). This checklist is intended to reduce the rate of major surgical complications. Description of the intervention The goal of preserving a patient's body temperature during anaesthesia and surgery is to minimize heat loss by reducing radiation and convection from the skin, evaporation from exposed surgical areas, and cooling caused by the introduction of cold intravenous fluids. Interventions used to maintain body temperature can be classified as follows: i) Interventions that decrease loss of heat through redistribution (i.e. preoperative pharmacologic vasodilatation and prewarming the skin prior to anaesthesia). ii) Passive warming systems aimed at reducing heat loss and thus preventing hypothermia, including interventions at above environmental temperatures; passive isolation by covering the exposed body surface; and a closed or semi\u2010closed anaesthesia circuit with low flows. iii) Active warming systems aimed at transferring heat to the patient. The effectiveness of these systems depends on various factors such as the design of the device, the type of heat transfer, placement of the system over the patient and, most importantly, the total body area covered in the heat exchange. The following systems are used for active warming: infrared lights, electric blankets, mattresses or blankets with warm\u2010water circulation, forced\u2010air warming or convective air\u2010warming transfer, warming of intravenous and irrigation fluids, warming and humidifying of anaesthetic air, and carbon dioxide (CO\u2082) warming in laparoscopic surgery. How the intervention might work For the purposes of this review, we have focused only on those active warming systems that transfer heat through the skin (active body surface warming systems (ABSW)) using a mechanical system. We expect that keeping body temperature from falling under certain levels should prevent perioperative vasoconstriction, leading to less catecholamine release, and hypertension. Maintaining temperature through a mechanical heat transference (air\u2010based or water\u2010based) system should prevent perioperative complications more efficiently that just passively preventing a person's loss of heat, as happens with thermal isolation. Adequately\u2010warmed people should also maintain their platelet activity, preventing them from excessive bleeding and the need for transfusions. Why it is important to do this review The clinical effectiveness of the different types of warming devices that can be used has been assessed in a very extensive guideline commissioned by the National Institute for Health and Clinical Excellence in the UK ( NICE 2008 ). The report concludes that there is sufficient evidence of clinical effectiveness and cost effectiveness for recommendations to be made on the use of forced\u2010air warming (the most widely investigated ABSW) to prevent and treat perioperative hypothermia. Nevertheless, most of the data come from intermediate outcomes such as temperature. The report's search is only current until the year 2007, when much research, especially with new systems, has been published since that date. Given this, our review evaluates the efficacy and safety of these ABSW systems focusing exclusively on relevant clinical outcomes other than temperature. Other Cochrane reviews have provided evidence for the efficacy and safety of passive methods such as thermal insulation ( Alderson 2014 ) and non\u2010cutaneous active systems, such as warmed gases or intravenous fluids ( Birch 2011 ; Campbell 2015 ). Other reviews have also addressed pharmacological interventions to prevent specific complications derived from hypothermia, such as shivering ( Lewis 2015 ).",
        "summary": "Compared with no active warming for adults undergoing surgery, use of active warming (most commonly forced\u2010air warming) may reduce surgical site infection and complications (50 vs 137 per 1000 people; all values on average) and major cardiovascular complications (14 vs 63 per 1000 people), whilst rates of all\u2010cause mortality are low and similar in both groups. Researchers classified the certainty of evidence for these three outcomes as low. Active warming appears to reduce the volume of blood loss (by 46.17 mL; based on 1372 participants) while substantially improving thermal comfort (based on 364 participants), with fewer people experiencing chills/shivering (71 vs 182 per 1000 people; high\u2010certainty evidence); the impact of active warming on thermal sensation is uncertain. Researchers observed no clear differences between active warming and no warming in the need for intraoperative transfusion. Only three RCTs with 192 participants reported on adverse effects; none were considered attributable to active warming."
    },
    "CD002001": {
        "query": "How does low\u2010molecular\u2010weight heparin compare with vitamin K antagonists for long\u2010term treatment of people with symptomatic venous thromboembolism?",
        "document": "Background Description of the condition Venous thromboembolism (VTE) is defined as formation of thrombus in the deep veins, most commonly in the legs (deep vein thrombosis, or DVT), and/or subsequent embolisation of all or part of the thrombus to the pulmonary circulation (pulmonary embolisation, or PE). DVT of the lower limbs may be associated with localised pain, swelling, and erythema, as well as with development of pulmonary emboli (PE) and later occurrence of post\u2010thrombotic syndrome (PTS; persistent swelling, erythema, and ulceration). PE presents acutely with shortness of breath, pain on inspiration, tachycardia, and right heart overload, and, if untreated, can lead to circulatory collapse and death; over the longer term, PE can cause chronic post\u2010thrombotic pulmonary hypertension. In this era of more liberal central venous catheterisation, DVT may more often involve the upper extremities. Rarely, other venous circulation (within cerebral veins, portal and mesenteric veins, etc.) can be affected. In addition to DVT and PE, thrombus can form in the superficial veins, where it is associated with local pain and inflammation (superficial venous thrombosis). This event tends to be associated with lower rates of mortality and morbidity than are seen with DVT, although some patients may be at higher risk of DVT formation depending on the location of the clot ( Chengelis 1996 ; Nasr 2015 ). Venous thromboembolism (VTE) comprises DVT and PE and can occur spontaneously. However, risk factors for VTE are many and include periods of inactivity, dehydration, hospitalisation, trauma, clotting disorders and previous thrombosis, varicose veins with phlebitis, pregnancy, use of oral combined hormonal contraceptives, malignancy, obesity, smoking, and advanced age ( Anderson 2003 ; NICE 2010 ). The incidence of VTE in mostly Caucasian populations is between 100 and 200 per 100,000 person\u2010years ( Heit 2015 ; White 2003 ). Of these, it is estimated that 45 to 117 per 100,000 person\u2010years are due to DVT (without PE) and 29 to 78 per 100,000 person\u2010years to PE (with or without DVT) ( Heit 2015 ). Recurrent VTE occurs in approximately 7.4% of patients at one year and in 30.4% of patients by 10 years ( Cushman 2007 ; Heit 2015 ; White 2003 ). Description of the intervention The primary aim of treatment of symptomatic VTE is prevention of its recurrence, including prevention of potentially fatal PE. Clinical guidelines provide recommendations for treatment of VTE in different settings ( Kearon 2016 ; NICE 2012 ). In general, anticoagulation is the recommended treatment of choice. The recommended initial treatment consists of a direct oral anticoagulant (with or without initial parenteral anticoagulation as indicated) or a parenteral anticoagulant given in conjunction with a VKA. Long term therapy (usually for a minimum duration of three months of anticoagulation) is indicated for treatment of acute VTE. Prolonged use of a VKA has proven efficacy in comparison with placebo and low\u2010dose heparin (unfractionated heparin) for treatment of VTE ( Hull 1979 ; Lagerstedt 1985 ). Use of adjusted therapeutic doses of subcutaneous unfractionated heparin is as effective as use of a VKA for preventing recurrence of symptomatic VTE, but both require regular laboratory monitoring ( Hull 1982b ). Usual practice is to administer VKAs to achieve an international normalised ratio (INR) of 2.0 to 3.0 ( Hull 1982a ). However, use of VKAs continues to present considerable risk of major bleeding (approximately 3% to 4%) during the first three months of treatment ( Hutten 1999 ). Moreover, for some patients, it is difficult to achieve a stable INR in the therapeutic range, and this leads to increased risk of bleeding complications. How the intervention might work Long term treatment of symptomatic VTE with low\u2010molecular\u2010weight heparin (LMWH) has been proposed to minimise risk of bleeding complications. Comparison of LMWH versus unfractionated heparin for initial treatment of symptomatic VTE reveals that LMWH is associated with a reduction in major bleeding ( Hettiarachchi 1998 ), and that treatment with LMWH is less frequently complicated by thrombocytopaenia ( Warkentin 1995 ) and osteoporosis ( Kelton 1995 ; Monreal 1994 ); also, these compounds do not require laboratory monitoring. Why it is important to do this review If the efficacy and safety of LMWH are found to be comparable with those of VKAs, LMWH could be used for long term treatment of symptomatic VTE. This would be especially important for patients in whom VKAs are contraindicated or impractical, for example, pregnant women and those living in geographically inaccessible places.",
        "summary": "Randomized controlled trials (RCTs) reported no clear differences in the incidence of recurrent venous thromboembolism (VTE) nor mortality between low\u2010molecular\u2010weight heparin (LMWH) and vitamin K antagonists for long\u2010term treatment of people with symptomatic VTE; event rates for these outcomes were low (approximately 4% to 10%). Fewer major bleeding episodes seem to occur with LMWH than with vitamin K antagonists (low\u2010quality evidence); however, when restricting analysis to RCTs with a low risk of bias, there was no clear difference between groups, although event rates were very low (\u2264 3%). LMWH therefore seems to be as effective and safe as vitamin K antagonists, and may be particularly useful for people with VTE living in geographically remote areas with limited access to health facilities for laboratory testing, and for those with contraindications to vitamin K antagonists. However, increasing use of direct oral anticoagulants, which have a better safety profile than vitamin K antagonists and do not require frequent laboratory testing, may alter the practical relevance of this clinical question in the future."
    },
    "CD008302": {
        "query": "In people with residual or metastatic differentiated thyroid cancer, what are the benefits and harms of recombinant human thyrotropin (rh TSH)\u2010aided radioiodine treatment?",
        "document": "Background Description of the condition Thyroid cancer is the most common endocrine malignancy ( Kinder 2002 ; Wartofsky 2002 ). The most frequent histologic subtype of thyroid carcinoma is papillary, followed by follicular carcinoma. Together these are commonly referred to as differentiated thyroid cancer (DTC) ( Sawka 2004 ). The prognosis of DTC generally is considered favourable, with overall survival rates of 80% to 95% at 10 years for middle\u2010aged adults ( Hundahl 1998 ). Local recurrences and distant metastases are seen frequently, particularly during the initial years of follow up, but sometimes they occur many years later. Overall survival rates decline to 40% when distant metastases are present ( Degrossi 1991 ; Schlumberger 1998 ), therefore early detection and treatment of DTC recurrences and metastases is essential throughout the patient's life. Radioactive iodine plays an important role in the management of DTC patients ( Chao 2005 ). Description of the intervention Radioactive iodine (iodine\u2010131, 131 I) is a \u03b2\u2010/\u03b3\u2010emitting radionuclide with a physical half\u2010life of 8.1 days. It has a principal \u03b3\u2010ray of 364 keV and a principal \u03b2\u2010particle with a maximum energy of 0.61 MeV, giving an average energy of 0.192 MeV and a range in tissue of 0.8 mm. In many centres iodine\u2010131 is used following thyroidectomy for the ablation of remnant thyroid tissue ( Bohuslavizki 1999 ; Haugen 2004 ; Mazzaferri 1997 ; Mazzaferri 2003 ; Sawka 2004 ; Schlumberger 2004 ) and metastatic disease in DTC ( Bohuslavizki 1999 ; Schlumberger 1998 ). Iodine\u2010131 ablation is performed in patients with DTC who have undergone total or subtotal thyroidectomy, in order to destroy the remaining thyroid tissue that may have residual microscopic cancer. Post\u2010ablation scans may detect metastatic disease ( Van Nostrand 2007 ; Zidan 2004 ). A systematic review suggested that iodine\u2010131 ablation may be beneficial in decreasing the recurrence of DTC ( Jarzab 2003 ). In the treatment of residual or metastatic DTC patients, increased thyroid\u2010stimulating hormone (TSH) or thyrotropin levels are necessary to maximise selective radioiodine uptake by normal thyroid or neoplastic cells. The retention of iodine\u2010131 by functioning thyroid tissue is believed to be optimised when serum TSH concentrations are high (30 to 50 \u03bcU/mL or more, McDougall 2001 ), which can be obtained either by withdrawing levothyroxine (L\u2010T 4 ) or the administration of recombinant human thyroid\u2010stimulating hormone (rhTSH). Traditionally, withdrawal of thyroid hormones for four to six weeks has been used to attain the increase in serum TSH concentrations necessary to optimise the trapping and retention of radioiodine for diagnostic procedures, thyroid remnant ablation and treatment of patients with DTC. Thyroid hormone withdrawal (THW) typically induces symptoms of hypothyroidism that often physically and psychologically constrain active people for prolonged periods of time ( Dow 1997 ). Symptoms including cognitive impairment, emotional dysfunction and physical discomfort may significantly disrupt patients' lives, especially as a large proportion of patients are young or middle\u2010aged and in good general health ( Brans 2001 ; Dow 1997 ; Mazzaferri 2000 ). In the elderly, muscle weakness and cerebellar ataxia as a result of hypothyroidism can impair motion, increasing the risk of trauma ( Brans 2001 ). Thyroid hormone withdrawal also may pose a danger of cardiac, cerebrovascular, pulmonary or neurological complications, especially in patients with co\u2010existing disorders or metastatic involvement of these organ systems, or in frail or elderly individuals. A documented potential risk of THW is stimulation of cancer progression in tumours generally exposed to increased TSH concentrations for several weeks, both during THW and while TSH concentrations return to baseline after the resumption of thyroid hormone suppressive therapy ( Jarzab 2000a ; Robbins 2001 ; Rudavsky 1998 ; Vargas 1999 ). Indeed, this contraindication has precluded the use of iodine\u2010131 therapy in some patients with the greatest need for such treatment ( Berg 2002 ; Mazzaferri 2000 ). In addition, THW is not always effective. Even after weeks of withdrawal, TSH concentrations may not increase sufficiently in cases of persistent thyroid hormone production by large thyroid remnants or functionally active metastases, hypothalamic or pituitary dysfunction, long\u2010term steroid administration as prophylaxis against tumour compression of key anatomical structures, or of unusually slow response, particularly in the elderly ( Adler 1998 ; Jarzab 2003 ; Luster 2000a ; Perros 1999 ). Exogenous stimulation with bovine TSH was first introduced as an alternative to withdrawal of thyroid hormones but because of frequent adverse reactions and the development of neutralising antibodies it has fallen into disuse ( Schlumberger 2003 ). Recombinant human TSH contributes substantially to the diagnostic and therapeutic approaches to thyroid cancer, offering an alternative to THW by avoiding the morbidity of hypothyroidism. Clinical studies have shown that administration of rhTSH promotes radioiodine uptake and thyroglobulin production by thyroid cells, as achieved by THW, for diagnosing residual or recurrent cancer ( Haugen 1999 ; Ladenson 1997 ; Meier 1994 ). In the USA and Europe, rhTSH is approved for use before thyroglobulin testing or diagnostic radioactive iodine scintigraphy in patients on thyroid hormone suppressive therapy ( de Keizer 2004 ). Differences in the iodine kinetics have been found in remnant ablation therapy in groups of patients with DTC after endogenous TSH stimulation by THW or after application of rhTSH ( Pacini 2006 ). Stimulation with rhTSH not only increases iodine\u2010131 uptake but also significantly enhances the absorbed thyroid dose. When rhTSH is used to prepare euthyroid patients, a significant lower mean residence time of radioiodine in the whole body and blood ( Luster 2003 ; Sisson 2003 ) and a significantly longer half\u2010time in the remnant thyroid tissue were observed compared to those patients who underwent THW ( Pacini 2006 ). In studies of rhTSH\u2010aided therapy ( Barbaro 2003 ; Berg 2002 ; de Keizer 2003 ; Kovatcheva 2004 ; Jarzab 2003 ; Lippi 2001 ; Luster 2000a ; Mariani 2000 ; Pacini 2002 ; Pellegriti 2001 ; Robbins 2002 ) peak serum TSH concentrations after the administration of rhTSH ranged from 42 to 400 \u03bcU/mL in euthyroid patients and from 124 to 582 \u03bcU/mL in hypothyroid patients. Limited reports show generally minimal effects of the administration of rhTSH on serum free tri\u2010iodothyronine and free L\u2010T 4 concentrations in patients with DTC and total thyroidectomy ( Jarzab 2003 ; Luster 2000a ). Currently, rhTSH is not licensed as an adjunct to iodine\u2010131 remnant ablation. There is a trend for widening the use of rhTSH for residual ( Barbaro 2003 ; Pacini 2002 ; Pacini 2006 ; Pilli 2007 ; Robbins 2001 ) or metastatic differentiated thyroid cancer ( Berg 2002 ; de Keizer 2003 ; Goffman 2003 ; Jarzab 2000b ; McDougall 2001 ; Muller 2002 ; Pellegriti 2001 ; Robeson 2002 ). This would be particularly useful in patients with insufficient or unusually slow endogenous TSH production after THW ( Checrallah 2001 ; Colleran 1999 ; Luster 2000a ; Masiukiewicz 1999 ). Recombinant human thyrotropin is also used in patients with risk of progressive disease or potentiation of tumour compression symptoms ( Adler 1998 ; Aslam 2001 ; Chiu 1997 ; Luster 2000a ; Menzel 2003 ; Robbins 2000 ; Rudavsky 1998 ), life\u2010threatening or debilitating exacerbation or appearance of concomitant illness ( Kovatcheva 2004 ; Luster 2000a ; Taeb 2004 ) and aggravation by THW. In addition, rhTSH has been applied to patients with hyperlipidaemia ( Jarzab 2003 ), to patients in need of very frequent treatment in order to avoid quality of life impairment of nearly unremitting hypothyroidism secondary to withdrawals ( Driedger 2004 ), and to patients refusing to withdraw thyroid hormones ( Luster 2005 ; Mazzaferri 2000 ). However, rhTSH is a particularly costly drug. The common adverse effects of rhTSH include nausea, headache, weakness, vomiting, dizziness, tingling sensation, chills and fever. Less common adverse effects are allergic reactions including urticaria, rash, pruritus, flushing, and respiratory signs and symptoms. Moreover, mild extremity paraesthesia, pathological spine fracture ( Jarzab 2003 ), neck tumour oedema ( Berg 2002 ; Braga 2001 ; Goffman 2003 ; Robbins 2000 ; Vargas 1999 ), transient, moderate to severe exacerbations of bone pain in patients with bone metastases ( Berg 2002 ; Jarzab 2003 ; Lippi 2001 ), clinical thyrotoxicosis ( Jarzab 2003 ) and pneumonia ( Rudavsky 1998 ) have been reported. How the intervention might work Radioactive iodine uptake by thyroid cells is mediated by a glycoprotein located on the cell membrane: the Na + / I \u2010 symporter (NIS). The persistence of NIS expression in DTC is the rationale for the use of iodine\u2010131 as a therapeutic agent. NIS expression, as well as thyroglobulin production and iodine uptake, are TSH\u2010dependent. Recombinant human TSH is a heterodimeric glycoprotein produced by recombinant DNA technology. It is obtained following transfection of a microorganism with genes encoding human TSH \u03b1 and \u03b2 subunits; rhTSH is then purified. The amino acid sequence is identical to that of human pituitary TSH and shares some of its biochemical properties. Recombinant human TSH has been shown to stimulate thyroglobulin production and thyroid cell proliferation as well as radioactive iodine uptake by thyroid cells ( Haugen 1999 ; Ladenson 1997 ; Thotakura 1991 ; Torres 2001 ; Zanotti\u2010Fregonara 2008 ). Why it is important to do this review Recombinant human thyrotropin has the advantage of avoiding both the clinical consequences of hypothyroidism, with potential positive impacts on quality of life and work productivity, and the risk of cancer growth due to the long\u2010lasting endogenous TSH stimulation. Unfortunately, studies which address the effects of rhTSH in DTC residual and metastatic disease treatment are still scarce and the opportunity of its clinical application remains controversial. There are currently no systematic reviews of randomised studies of rhTSH\u2010aided iodine\u2010131 treatment for residual or metastatic differentiated thyroid cancer.",
        "summary": "In patients with differentiated thyroid cancer who underwent thyroidectomy, there is limited evidence showing that there is no statistically significant difference in ablation rates when using recombinant human thyroid\u2010stimulating hormone (rh TSH) before iodine\u2010131 therapy compared with withdrawing thyroid hormone 4\u20106 weeks before iodine\u2010131 therapy. However, the wide confidence interventions around the results mean we cannot be certain whether both interventions are equally effective for this outcome. The rationale for using rh TSH instead of withdrawal of thyroid hormone for this indication is to avoid weeks of hypothyroidism symptoms associated with withdrawal of thyroid hormone. However, there are limited data to assess the effects of rh TSH withdrawal of thyroid hormone on health\u2010related quality of life. There was limited evidence of no statistically significant difference in the rate of adverse events reported from rh TSH use and no statistically significant difference in the dose of iodine\u2010131 needed to treat residual differentiated thyroid cancer if rh TSH was used instead of withdrawal from thyroid hormone. Because of the limited evidence available on rh TSH, in clinical practice thyroid hormone withdrawal is still considered the gold standard after thyroidectomy in these patients. rh TSH therapy tends to be used when withdrawal from thyroid hormone cannot be tolerated by the patient or if the induced hypothyroid status could worsen existing medical conditions. There were no data found on patients with metastatic differentiated thyroid cancer."
    },
    "CD005949": {
        "query": "How does higher compare with lower amino acid intake in parenteral nutrition for very low birth weight or very preterm infants?",
        "document": "Background Description of the condition Nutrition is important for survival, growth, and development. Sick newborn and preterm infants frequently are not able to be fed enterally, necessitating parenteral fluid and nutrition. Despite advances in neonatal care, postnatal growth failure continues to be a ubiquitous problem among preterm neonates. In a 1995 to 1996 cohort of very low birth weight (VLBW) infants in the National Institute of Health and Child Development Research Network, 22% of the cohort was small for gestational age (SGA) at birth; however by 36 weeks' postmenstrual age (PMA), 97% of the cohort was below the 10th percentile in weight ( Lemons 2001 ). Although inadequate nutritional support increases risks of postnatal growth failure and neurodevelopmental impairment among preterm infants ( Lucas 1998 ), aggressive nutritional support might place them at higher risk of protein intolerance, development of metabolic syndrome with insulin resistance, and cardiovascular disease in later childhood and adulthood ( Ehrenkranz 2006 ; Embleton 2001 ; Ong 2007 ). Description of the intervention Parenteral nutrition is widely used to prevent growth failure and malnutrition when nutritional support is provided to sick neonates who are unable to tolerate enteral intake owing to prematurity or the nature of their illness ( AAP 2009 ; EPSGHAN 2005 ; Fusch 2009 ). An idealised optimal nutritional goal for neonates is one that duplicates normal in utero foetal growth rates ( AAP 1998 ). Maximal weight\u2010specific protein gain occurs before 32 weeks' gestation ( Micheli 1993 ), and the foetus uses amino acids as a major energy source ( Gresham 1971 ; Lemons 1976 ). Postnatally, nutrition is generally introduced gradually over the first week of life because of concerns about nutrient intolerance by extreme preterm infants or very ill neonates. Lipids and glucose are frequently used at rates that exceed in utero delivery rates, but amounts of amino acid are lower than those provided at in utero delivery rates. Increasing amino acid intake during parenteral nutrition provided shortly after birth has the potential to increase protein accretion rates and growth in newborn infants. Amino acid intake may be increased during parenteral nutrition by providing increased start or initial intake of amino acids, increased rate of grading of amino acids, increased final intake of amino acids, or a combination of these strategies. Amino acid intake, particularly in the early transitional phase of a preterm infant's life, is limited by the range of fluid load and protein intake that an adapting or sick infant can deal with, as well as by the stability of the parenteral nutrition formulation ( EPSGHAN 2005 ). How the intervention might work A concern associated with high amino acid intake in parenteral nutrition involves protein intolerance as reflected by higher ammonia and blood urea levels. These higher levels may reflect effective use of amino acids rather than protein intolerance ( Thureen 1999 ). In contrast, low initial amino acids have been associated with postnatal malnutrition and have produced measurable growth failure at hospital discharge ( Ehrenkranz 1999 ; Lucas 1994 ; Ziegler 1991 ). Low early protein intake is also associated with poor long\u2010term developmental outcomes ( Lucas 1998 ). Prevention of a negative nitrogen balance is achieved in preterm infants by providing amino acids at a rate of 1 to 1.5 g/kg/d ( Kashyap 1994a ; Rivera 1993 ; Thureen 2003 ; van Lingen 1992 ). Achieving a body composition that more closely resembles foetal body composition may require a higher amino acid intake. In the extremely low birth weight infant, achieving intrauterine protein accretion rates may require up to 3.85 g/kg/d of protein ( Ziegler 1994 ). Evidence suggests that preterm infants may have a higher protein turnover rate relative to term infants ( Hay 1996 ). Animal studies such as Lemons 1976 and human studies such as Gresham 1971 have shown that amino acids are a significant source of energy during intrauterine life. In addition to protein intake, energy is required for protein anabolism ( Kashyap 1994 ). Intake of 25 to 40 kcal of non\u2010protein energy per gram of protein allows optimal protein accretion ( Cauderay 1988 ). When energy availability from a non\u2010protein nitrogen source is limited, protein anabolism is decreased and protein is used for energy. When energy is limited and protein is used as an energy source, optimal protein synthesis cannot occur ( Kashyap 1994 ). On the other hand, increasing non\u2010protein nitrogen calories without increasing protein intake is also not helpful. Preterm and term infants showed an increase in protein synthesis of a similar magnitude with parenteral nutrition, whereas increasing intravenous glucose administration did not decrease proteolysis despite a threefold increase in insulin concentration ( Denne 1996 ). Potential benefits of higher protein intake also include greater growth of lean tissue and bone mass, thereby preventing postnatal growth failure and leading to improved glucose tolerance, synthesis of hormones and enzymes, and maintenance of oncotic pressure ( Fomon 1993 ). In an animal study, higher protein intake was shown to accelerate maturation of the renal tubules ( Jakobsson 1990 ; Thureen 2003 ). Deficiency of protein in infants leads to growth failure causing oedema and decreased resistance to infection ( Nayak 1989 ). Risks of higher protein intake include increased concentrations of amino acids (especially tyrosine and phenylalanine), metabolic acidosis, hyperammonaemia, and elevated blood urea nitrogen ( Micheli 1993 ; Senterre 1983 ). This risk is more pronounced with increasing prematurity. High protein intake could lead to cholestasis, and the phosphate content of amino acid solutions may increase the neonate's tendency toward hypocalcaemia ( Andronikou 1983 ). Renal hypertrophy and increased circulating insulin\u2010like growth factor\u20101 have been reported secondary to high protein intake ( Murray 1993 ). High protein intake in early life may increase risks of long\u2010term obesity and development of diabetes ( Raiha 2001 ; Rolland 1995 ; Scaglioni 2000 ). Therefore, it is important for researchers and care providers to consider the consequences of early nutrition. Why it is important to do this review Despite significant advances in neonatal care, postnatal growth failure is an event of major concern. Potential benefits of higher amino acid intake during parenteral nutrition of improved nitrogen balance, growth, and infant health may be outweighed by the infant's ability to utilise high intakes of parenteral amino acid, especially in the days after birth, resulting in high concentrations of amino acids, ammonia, and urea, and an exacerbation of metabolic acidosis. It is important to determine the optimal amount of amino acid intake via parenteral nutrition for the growth and health of newborn infants.",
        "summary": "Supplementing parenteral nutrition for very preterm/low birth weight infants with amino acids may confer some benefits but may increase the risk of metabolic acidosis. Evidence is of low to very low certainty, precluding firm conclusions. Low\u2010 to very low\u2010certainty evidence suggests no clear and consistent differences between very preterm/low birth weight infants who received higher (> 2 to > 4 g/kg/d) and those who received lower (\u2264 1 to \u2264 3 g/kg/d) amino acid concentrations in their total parenteral nutrition (TPN) in major clinical outcomes, including growth, mortality, neurodevelopmental disability to discharge, necrotizing enterocolitis, chronic lung disease, and developmental delay, including cerebral palsy at 18 to 24 months of age. Although some evidence shows that infants with higher amino acid concentrations were less likely to be in the 10th percentile for weight (on average, 273 vs 368 per 1000 infants), had slightly greater head circumference growth to discharge (mean difference 0.09 cm/week), and were less likely to develop retinopathy of prematurity (on average, 70 vs 158 per 1000 infants), reviewers found great uncertainty of these estimates owing to the very low certainty of evidence for these outcomes. Risk of metabolic acidosis appeared to be increased with higher amino acid intake (on average, 195 vs 95 per 1000 infants, although results did not reach statistical significance), and results show no clear differences in rates of cholestasis between groups."
    },
    "CD009179": {
        "query": "How do opioids compare with non\u2010opioid analgesics for the management of acute pancreatitis pain?",
        "document": "Background Description of the condition Acute pancreatitis was defined in 1992 at the Atlanta Symposium ( Bradley 1993 ) as an acute inflammatory process of the pancreas that may also involve adjacent tissues and/or remote organ systems. Mild acute pancreatitis was defined as that associated with minimal organ dysfunction, whereas severe acute pancreatitis was defined as that associated with organ failure and/or local complications (necrosis, abscess, or pseudocyst) accompanied by adverse prognostic scores ( Banks 2006 ). The incidence rate of acute pancreatitis ranges between 5 and 80 per 100,000 people per year, with the highest incidence recorded in the United States and Finland ( Banks 2002 ). In 75% to 80% of sufferers, the aetiology of acute pancreatitis is identified. In developed countries, the most frequent causes are bile duct obstruction (38%) and alcohol abuse (36% to 44%). The mechanisms by which bile duct obstruction or alcohol consumption initiate acute pancreatitis are not completely known. It seems, however, that a common pathogenic pathway might be related to inappropriate activation of trypsinogen to trypsin and to a lack of prompt elimination of active trypsin inside the pancreas ( Wang 2009 ; Whitcomb 2006 ; Whitcomb 2008 ). Other less common causes of pancreatitis are elevated triglyceride levels, cancer, viral and bacterial infections, surgery, peptic ulcers, pancreas divisum, medications and other genetic, metabolic and autoimmune causes. Abdominal pain is the most common symptom of acute pancreatitis and is usually accompanied by nausea, vomiting and fever. Acute, constant and intense abdominal pain might last for several days, is mostly experienced in the epigastric region or the right upper quadrant and may radiate to the back. Physical examination often reveals severe upper abdominal tenderness at times associated with guarding ( Carroll 2007 ; Frossard 2008 ). It is generally accepted that a diagnosis of acute pancreatitis requires at least two of the following three features: 1) abdominal pain characteristic of acute pancreatitis; 2) serum amylase and/or lipase greater than three times the upper limit of normal; and 3) characteristic findings of acute pancreatitis on abdominal scan ( Banks 2006 ). Contrast\u2010enhanced computerised tomography (CECT) can be done after admission to confirm diagnosis of disease (87% to 90% sensitivity and 90% to 92% specificity), or after four days, to assess local complications and to score the disease. Most cases of acute pancreatitis are mild and self\u2010limiting, but 20% of cases develop severe disease with local complications, such as necrosis, pseudocyst or abscess of the gland, and/or extrapancreatic complications ( Bradley 1993 ). Several risk scales, general or specific, are used to classify disease severity and survival, including Computed Tomography Severity Index (CTSI), Ranson's criteria, Imrie scoring system, Acute Physiology And Chronic Health Evaluation (APACHE II), and the Sequential Organ Failure Assessment (SOFA) ( Carroll 2007 ; Frossard 2008 ). General mortality is estimated to be around 2% to 3%, but can reach 80% ( Johnson 2005 ). While mortality in sterile pancreatic necrosis is 10%, infected necrosis generates a mortality of 25%. Nearly half of deaths occur during the first one to two weeks after admission because of multiple organ failure from systemic inflammatory response. Deaths beyond this time are also due to multiple organ failure, but are secondary to infected pancreatic necrosis. Description of the intervention Treatment of acute pancreatitis depends mainly on the severity of the progression but almost all cases will need supportive treatment, such as analgesics. Several types of opioids exist under the N02A Anatomical Therapeutic Chemical ATC code ( ATC Classification ). This group comprises strong analgesics of the opiate type and analgesics with similar structure or action. Opioids can be classified by their actions: agonist (e.g. morphine, hydromorphone, fentanyl), partial agonist (e.g. buprenorphine), agonist\u2010antagonist (e.g. pentazocine), and antagonist opioids (e.g. naloxone). Pure opioid agonists are the most potent analgesics ( Trescot 2008 ). These drugs are stronger pain relievers than non\u2010opioids; oral 650 mg paracetamol or aspirin is oral dose equianalgesic to 30 mg codeine, 50 mg meperidine or 5 mg morphine. Apart from pain relief, opioid uses include treatment of opioid dependence, cough suppressants, epidural analgesia or as an antispasmodic. Opioids are commonly used to manage pain in acute pancreatitis. However, it has been suggested that, apart from meperidine, opioids may mask the resolution of the disease and increase pain due to their spasmogenic effect, which in turn increases intraluminal pressure in the sphincter of Oddi ( Isenhower 1998 ). This increased bile pressure appears to be related to the dose and plasma concentration of the opioid, and is apparently mediated by the Mu (\u03bc) receptor. However, the clinical significance of this increased pressure is uncertain, because many studies are anecdotal observations, with small numbers of participants without known pancreatic disease, and there is no clear evidence from controlled clinical trials that would support this theory ( Cebri\u00e1n 2003 ). How the intervention might work Treatment with analgesics for abdominal pain in acute pancreatitis probably does not modify the course of disease or mortality. However, the treatment of pain as a symptom improves comfort and patient\u2010reported outcomes. An opioid is a psychoactive chemical that works by binding to opioid receptors; Mu (\u03bc) with Mu1 and Mu2 subtypes receptors stimulated by pure opioid agonists, Kappa (\u03ba) and Delta (\u03b4). These receptors are found principally in the central and peripheral nervous system and the gastrointestinal tract. The opioid drugs produce analgesia by actions at several levels of the nervous system, in particular, inhibition of neurotransmitter release on presynaptic neuronal terminals in the spinal cord, considered to be the major mechanism of action responsible for the clinical effects of opioids, and inhibition of postsynaptic neurons, preventing the ascending transmission of the pain signal. Why it is important to do this review All people suffering from pain with acute pancreatitis would be considered for at least one type of analgesic (e.g. paracetamol, non\u2010steroidal anti\u2010inflammatory drugs, opioids). No clear advantage for any particular type of analgesia has been demonstrated in the treatment of abdominal pain in people with acute pancreatitis ( Banks 2006 ). We have been unable to identify any meta\u2010analysis or systematic reviews comparing opioids versus other drugs for this condition. The aim of this review is to clarify the appropriate use of opioids for abdominal pain management in acute pancreatitis.",
        "summary": "Opioid and non\u2010opioid analgesics seemed to lead to similar reductions in pain and be associated with similar rates of adverse effects when used for managing acute pancreatitis pain, based on randomized controlled trials (RCTs) comparing intravenous (iv) buprenorphine, subcutaneous morphine or iv pentazocine (opioids) with iv procaine or iv metamizole (non\u2010opioids). Therefore the choice of analgesic is patient and facility dependent. Low\u2010quality evidence showed similar improvements with opioids compared with non\u2010opioids in pain intensity at 2 days (19 per 100 people (95% CI 7 to 50) with opioid compared with 38 per 100 people with non\u2010opioid analgesic) and supplementary analgesic option offered at 2\u20104 days (30 per 100 people (95% CI 21 to 42) with opioid compared with 73 per 100 people with non\u2010opioid analgesic). Drug\u2010related adverse events such as nausea, vomiting, sedation and somnolence were also similar in both groups as suggested by low\u2010quality evidence. Of note, most trials were underpowered and included mostly men (65%)."
    },
    "CD000547": {
        "query": "In women undergoing surgery for uterine fibroids, how do preoperative gonadotropin\u2010releasing hormone analogues (GnRHa) compare with placebo or no treatment?",
        "document": "Background Description of the condition Uterine fibroids (also known as myomas or leiomyomas) are the most common benign tumour of the female reproductive tract which are thought to affect approximately 20% to 40% of women of reproductive age ( Jacoby 2010 ; Wallach 1992 ), although it is possible prevalence may be even higher (70% to 80%) ( Baird 2003 ). Fibroids are classified according to their anatomic location as subserosal, intramural and submucosal types ( Yang 2011 ). Many fibroids are asymptomatic but a proportion of women have heavy menstrual bleeding (30%), anaemia, dysmenorrhoea, pelvic pain and pressure symptoms (34%), reduced quality of life and reduced fertility (27%) ( Buttram 1981 ). The standard treatment for symptomatic uterine fibroids are surgical and radiological interventions. Fibroids are the most common indication for hysterectomy ( Merrill 2008 ); less invasive procedures include myomectomy (in women wishing to preserve their fertility), hysteroscopic removal, uterine artery embolisation and other radiological interventions ( Patel 2014 ). Fibroids represent one of the most frequent indications for major surgery in premenopausal women ( Carls 2008 ) and as such they constitute a major public health cost. Description of the intervention Some medical therapies are currently being investigated as stand\u2010alone treatments for fibroids, but the role of this review was to investigate medical therapies before surgery. These preoperative medical treatments include gonadotropin\u2010releasing hormone analogues (GnRHa), progestins, selective oestrogen receptor modulators, dopamine agonists, prostaglandin analogues and selective progesterone receptor modulators. Since the 1980s, GnRHa treatments, which induce a state of hypo\u2010oestrogenism by suppressing pituitary ovarian function, have been investigated for women with fibroids. The main effects of this treatment are the temporary control of bleeding and reduction of fibroid and uterine size, but side effects include menopausal symptoms and bone loss with long\u2010term use. After therapy is stopped, there is re\u2010growth of both the tumours and the uterus to almost their pretreatment size, and in most women, a recurrence of symptoms ( Matta 1989 ). Thus, they have been approved only for short\u2010term use, as a preoperative adjunct to surgery. Other potential hormonal therapies have also been investigated as preoperative treatment. Progestins have been used to reduce heavy menstrual bleeding induced by fibroids but have thromboembolic and metabolic risks ( Jourdain 1996 ). Selective oestrogen receptor modulators (SERMs) are approved for the prevention and treatment of osteoporosis but preclinical studies suggest they may inhibit the proliferation of fibroid cells, consequently limiting their growth ( Jirecek 2004 ). Selective progesterone receptor modulators (SPRMs), such as asoprisnil, mifepristone and ulipristal acetate, have more recently been investigated, and in 2012, ulipristal acetate was licensed by the European Medicines Agency for the treatment of symptomatic fibroids over a maximum of three months for preoperative management ( P\u00e9rez Lopez 2014 ). How the intervention might work Although the pathogenesis of fibroids is not well established, it has been recognised that fibroid growth and maintenance are stimulated by oestrogen and affected by hormonal cyclic changes ( Friedman 1990 ). Oestradiol and progesterone receptors have been identified in myomatous tissue ( Tamaya 1985 ; Wilson 1980 ). Because of this dependence of fibroids on steroid hormones, it follows that medications to reduce the levels of gonadal steroids might be options for the treatment of uterine fibroids. If a state of reduced oestrogen secretion could be induced, this would result in the reduction in growth of fibroids and even their regression. Additionally, as progesterone is known to promote the growth of fibroids, modulating the progesterone pathway by acting on progesterone receptors in myometrial tissue may control heavy menstrual bleeding and reduce fibroid bulk ( Donnez 2012a ). Pretreatment with medical therapy before hysterectomy is considered particularly useful for women with severe anaemia and to reduce blood loss during surgery. Other indications have included large fibroids or other factors that make surgery technically difficult ( West 1992 ). Pretreatment with medical therapy may also enable greater use of vaginal hysterectomy ( Stovall 1991 ) compared to abdominal hysterectomy or even more conservative surgical options such as laparoscopic or hysteroscopic removal. Conservative surgery, or myomectomy, has generally been used for women who wish to preserve or enhance their fertility but is often regarded as a more difficult procedure than hysterectomy, with a high risk of postoperative pyrexia (fever), pelvic haematoma formation and postoperative adhesions. Moreover, intraoperative haemorrhage can necessitate emergency blood transfusion or hysterectomy. Myomectomy may be performed via laparotomy, laparoscopy or hysteroscopy and the method must be distinguished in the evaluation of pretreatment with medical agents. Potential benefits of preoperative medical treatments are reduction in blood loss during the operation, ease of operability, better anatomical reconstruction and the possibility of using a transverse (Pfannenstiel\u2010type) rather than vertical midline incision at laparotomy. However, concern has been expressed that the fibroid capsule would become less evident and may be missed, tumours will not 'shell out' cleanly and the excision may be more difficult ( Friedman 1989 ; Stovall 1989 ). A less invasive surgical option, hysteroscopic resection, is often used in women with submucous fibroids. This option offers advantages over myomectomy such as reduced trauma, shorter hospitalisation and recovery times and decreased risk of adhesion formation. GnRH analogues have been used preoperatively before this surgery for some time, but robust evidence to support this practice is weak ( Parazzini 1998 ). Controlled non\u2010randomised studies have been undertaken but have reported conflicting results ( Campo 2005 ; Perino 1993 ). Why it is important to do this review Fibroids represent one of the most frequent indications for major surgery in premenopausal women. GnRH analogues, and more latterly other types of medical therapy, have been investigated before surgery for uterine fibroids to improve intraoperative and postoperative outcomes. It is important to determine precisely the specific advantages and disadvantages of this practice compared to no presurgical therapies and to compare the effectiveness of individual presurgical therapies.",
        "summary": "For women with symptomatic fibroids, low\u2010quality evidence suggests that preoperative GnRHa reduce uterine volume (on average, by 175.34 mL) and fibroid volume (on average, by 6 mL to 155 mL) when compared with placebo or no treatment. Low\u2010quality evidence also suggests that women undergoing hysterectomy may be less likely to experience a postoperative complication if they have taken preoperative GnRHa (on average, 116 vs 195 per 1000 women). However, moderate\u2010quality evidence shows that these potential benefits may be at the cost of an increase in preoperative adverse events, such as hot flashes, dizziness, vaginitis, change in breast size, and sweating (on average, 793 vs 579 per 1000 women). Results for other outcomes (intraoperative blood loss, duration of surgery, requirement for blood transfusion) were limited by inability to combine trial results in meta\u2010analyses and/or by low participant numbers, and results were inconsistent across trials."
    },
    "CD000307": {
        "query": "How does fluphenazine decanoate compare with other neuroleptics in people with schizophrenia?",
        "document": "Background Description of the condition One in every 10,000 people per year are diagnosed with schizophrenia, with a lifetime prevalence of about 1% ( Jablensky 1992 ). It often runs a chronic course with acute exacerbations and often partial remissions. The neuroleptic group of drugs is the mainstay treatment for this illness ( Dencker 1980 ). These are generally regarded as highly effective, especially in controlling such symptoms as hallucinations and fixed false beliefs (delusions) ( Kane 1986 ). They seem to reduce the risk of acute relapse. Anti\u2010psychotic drugs are usually given orally ( Aaes\u2010Jorgenson 1985 ), but compliance with medication given by this route may be difficult to quantify. Problems with treatment adherence are common throughout medicine ( Haynes 1979 ). Those who suffer from long\u2010term illness such as schizophrenia are less likely to take medication regularly if experiencing adverse effects ( Kane 1998 ), or if they experience cognitive impairments ( David 1994 ) and erosion of insight. The development of depot injections in the 1960s and initial clinical trials ( Hirsch 1973b ) gave rise to extensive use of depots as a means of long\u2010term maintenance treatment. Description of the intervention Fluphenazine was one of the first oral antipsychotics to be produced in a depot form. Two forms of the depot, a decanoate (Modecate) and an enanthate (Moditen) are available. The decanoate is more commonly prescribed ( Marder 1990 ) and lasts about four to six weeks in the body, while a single dose of the enanthate is shorter acting (one to three weeks). Evidence also suggests that the decanoate may produce slightly less adverse effects than its enanthate counterpart ( Kurland 1970 ). However, in comparison with newer depot formulations fluphenazine decanoate has been reported to cause greater extrapyramidal adverse effects ( Knights 1979 ) and to significantly lower mood ( De Alarcon 1969a ). How the intervention might work Depots mainly consist of an ester of the active drug held in an oily suspension. This is injected intramuscularly and is slowly released. Depots may be given every one to six weeks. Individuals may be maintained in the community with regular injections administered by community psychiatric nurses, sometimes in clinics set up for this purpose ( Barnes 1994 ). The use of depots eradicates covert non\u2010compliance. Why it is important to do this review A systematic review undertaken over a decade ago suggested that, for those with serious mental illness, stopping anti\u2010psychotics resulted in 58% of people relapsing, whereas only 16% of those who were still on the drugs became acutely ill within a one\u2010year period ( Davis 1986 ). Evidence also points to the fact that experiencing a relapse of schizophrenia lowers a person's level of social functioning and quality of life ( Curson 1985 ). Relapse prevention has also enormous financial implications. For example, within the UK, a Department of Health burden of disease analysis in 1996 indicated that schizophrenia accounted for 5.4% of all National Heath Service inpatient expenditure, placing it behind only learning disability and stroke in magnitude ( DoH 1996 ).",
        "summary": "Data from largely underpowered trials indicates that fluphenazine decanoate does not seem to differ in safety or efficacy from other first generation antipsychotics (given as oral or long\u2010acting injectable formulations) in patients with schizophrenia but no firm conclusions can be drawn. There was randomized controlled trial evidence assessing fluphenazine compared with second generation antipsychotics. At six months or later, rates of clinically significant global change, relapse, and dropouts were similar in patients receiving fluphenazine decanoate and oral neuroleptics according to very\u2010low quality clinical trial data. Results were similar in comparison with other depot neuroleptics. No differences in mental state were seen between patients on depot fluphenazine versus other oral and injectable drugs after one year, though small differences of unclear clinical significance favoring other depot neuroleptics were found at shorter follow\u2010up up to 6 months. Adverse effect data are mixed; very\u2010low quality evidence suggests that, compared with oral agents, fluphenazine decanoate may offer a 50% lower risk of movement disorders, while, according to one small trial (51 patients), it may increase \u2018toxicity\u2019 (348 per 1000 on the decanoate vs. 71 per 1000 on oral drugs). Movement disorders occurred at similar rates across depot formulations, but \u2018general\u2019 adverse effect risk was slightly elevated with fluphenazine (873 per 1000 on fluphenazine decanoate vs. 640 per 1000 on other depot formulations)."
    },
    "CD003748": {
        "query": "What are the benefits and harms of cilostazol in people with intermittent claudication?",
        "document": "Background Description of the condition Lower limb peripheral arterial disease (PAD) is a manifestation of atherosclerosis in the lower extremities, affecting 20% of people over 70 years of age and 4% to 12% of the population aged 55 to 70 years ( Dormandy 1999 ; PAD 2003 ). Patients with PAD commonly complain of intermittent claudication, which is characterised by pain in the legs or buttocks that occurs with exercise and subsides with rest, and occurs in 40% of PAD patients ( Dormandy 1999 ). Despite the relatively benign prognosis for the affected limb, the symptoms of intermittent claudication are an indicator for systemic atherosclerosis. Compared with age\u2010matched controls, people with intermittent claudication have a three\u2010 to six\u2010fold increase in cardiovascular mortality ( Leng 1996 ). The majority of patients with intermittent claudication are treated with best medical management ( Khan 2005 ), and the mainstay of treatment for patients with PAD is cardiovascular risk factor modification. This consists of smoking cessation, prescribed exercise ( Watson 2008 ), antiplatelet treatment, lipid\u2010lowering therapy and control of blood pressure and diabetes. Only two\u2010thirds of compliant patients will achieve symptomatic relief of intermittent claudication after three to six months. Some patients may not be able to comply with prescribed exercise due to associated comorbidity or social reasons. As angioplasty or surgery are only used in severe, disabling or progressive intermittent claudication, these symptomatic patients may benefit from adjunctive therapy. Description of the intervention Cilostazol, with the trade name Pletal, is a phosphodiesterase\u2010III inhibitor that has antiplatelet and antithrombotic actions ( Sallustio 2010 ). Cilostazol also acts on smooth muscle cells as a vasodilator with beneficial effects on triglycerides and high\u2010density lipoproteins ( Chapman 2003 ). Cilostazol is indicated for intermittent claudication but there is also evidence to suggest that cilostazol may have a role in reducing restenosis after endovascular therapy and coronary stenting ( Iida 2008 ; Lee 2013 ).The suggested dose of cilostazol for intermittent claudication is 100 mg taken orally twice daily. Cilostazol is contraindicated in patients with congestive heart failure and those with renal or hepatic impairment ( Chapman 2003 ; Dawson 2001 ). How the intervention might work Antiplatelet therapy is effective in long term secondary prevention of vascular events in patients at high risk of vascular disease, including those who have had ischaemic stroke or acute myocardial infarction, and a benefit of antiplatelet treatment in patients with intermittent claudication in the reduction of vascular events has been previously observed ( ATT 2002 ; PAD 2003 ; Robless 2001 ). It is unclear exactly how cilostazol works to improve claudication, but the mechanism is most likely multifactorial, involved with several of cilostazol's actions, specifically vasodilation, possible beneficial inhibition of platelet aggregation, and altering a patient's lipid profile ( Chapman 2003 ; Rizzo 2011 ; Ueno 2011 ). Why it is important to do this review In practice, compliance with best medical treatment is poor and most people remain symptomatic with intermittent claudication. Until recently there have been three options; supervised exercise, angioplasty or bypass surgery. Compliance with supervised exercise is poor, the durability of angioplasty is unproven and surgery carries significant morbidity and mortality ( Fowkes 1998 ). Many pharmacological agents have been advocated for the symptomatic treatment of intermittent claudication but until recently none have gained acceptance. Cilostazol has been approved for the treatment of intermittent claudication and has been shown to improve pain\u2010free walking distance ( Barnett 2004 ). If cilostazol is found to reduce the symptoms of claudication, as well as cardiovascular risk in patients with PAD, it would be beneficial for patients whose options are otherwise limited, and possibly extreme. This is an update of a review first published in 2007 ( Robless 2007 ).",
        "summary": "Randomized controlled trials (RCTs) comparing 50 to 150 mg cilostazol given orally twice daily versus placebo reported benefits of cilostazol in terms of walking distance (average improvement of 20 to 31 m in pain\u2010free walking distance, and 32 to 52 m in maximum walking distance, over placebo). The ankle\u2010brachial index was higher with cilostazol (on average, by 0.06), but data were available only for the 100 mg twice\u2010daily dose. RCTs reported no differences between 50 or 100 mg twice\u2010daily cilostazol and placebo in the incidence of myocardial infarction or stroke, but more people reported headache, diarrhea, and palpitations with both cilostazol doses. When comparing cilostazol with pentoxifylline, RCTs reported similar effectiveness but noted that more people suffered adverse events (headache, abnormal stool, palpitations) with cilostazol. Reviewers did not assess the quality of the evidence, and risk of bias of these RCTs is unclear as most did not report methods used, making it difficult to assess the reliability of trial results. Data on progression to surgery were not available."
    },
    "CD011888": {
        "query": "What are the benefits and harms of aspirin 1000 mg for adults with episodic tension\u2010type headache?",
        "document": "Background This review is based on a template for reviews of drugs used for acute treatment of episodic tension\u2010type headache (TTH) in adults. The aim is for all reviews to use the same methods. Headaches are a commonly reported problem in community\u2010based surveys worldwide. The lifetime prevalence of headache is estimated to be greater than 90% ( Steiner 2004 ), and the annual prevalence is estimated to be 46% in the general adult population ( Stovner 2007 ). Variations in reported prevalence of TTH may result from differences in study design, population, inclusion or exclusion of cases of infrequent episodic TTH, overlap with probable migraine, cultural and environmental differences, or even genetic factors ( Sahler 2012 ). TTH is more common than migraine, a finding replicated across the world ( Oshinaike 2014 ; Vos 2012 ). The management of people with non\u2010migrainous headaches is, however, largely neglected ( Rasmussen 2001 ; Steiner 2011 ), and may be fragmented by the involvement of clinicians from different medical specialities (neurology; ear, nose, and throat; ophthalmology; psychiatry). Because headache is rarely life\u2010threatening and headache pain is generally mild to moderate in intensity, people often self\u2010medicate and do not seek formal care from health services ( Rasmussen 2001 ). People with TTH have more work absence than people without headaches ( Lyngberg 2005 ), which may lead to loss of productivity ( Cristofolini 2008 ; Pop 2002 ). Headache\u2010related characteristics include significant problems with headache management, disability, pain, worry, and dissatisfaction with care, as well as greater use of medical services and worse general health ( Harpole 2005 ). Headache can be either primary (cause not known) or secondary (due to other systemic or local causes) ( Green 2009 ). TTH belongs to the group of primary headaches and is the most common type; the large number of people affected imposes a significant burden on the healthcare system ( Stovner 2007 ). Generally, episodes of TTH are mild to moderate in intensity and self\u2010limiting, but in a small group of people they may be more severe and disabling ( Green 2009 ). People with longer lasting or more severe headaches may seek help in a clinical setting, but the majority do not do so, which can result in inadequate and inappropriate management ( Kernick 2008 ). In one Canadian community\u2010based telephone survey to determine medication patterns of 274 people with frequent headache (of all types) aged 18 to 65 years, only 1% used prescription medication. The majority reported using over\u2010the\u2010counter (OTC; non\u2010prescription) analgesics (56% used paracetamol (acetaminophen) and 15% used aspirin), and the perceived effectiveness of OTC medication was approximately 7 on a scale of 0 to 10 ( Forward 1998 ). Until recently, professional strategies for the management of TTH were largely extrapolated from those used for migraine. The World Health Organization (WHO) essential drug list, for example, does not include indications for the management of TTH ( WHO 2015 ). In 2010, the British Association for the Study of Headache (BASH; BASH 2010 ) and the European Federation of Neurological Societies (EFNS; Bendtsen 2010 ) updated or published guidelines for the management of TTH. The two guidelines reflect ongoing systematic efforts to bridge the gap between clinical trial evidence and clinical practice with the aim of improving practice. While these guidelines represent a step forward, they fail to take into account the quality and methodological limitations of individual studies. Description of the condition TTH has been known by several names, including tension headache, muscle contraction headache, psychomyogenic headache, stress headache, ordinary headache, essential headache, idiopathic headache, and psychogenic headache ( IHS 2004 ). TTH is diagnosed mainly by the absence of features found in other headache types, especially migraine. The third edition of the International Classification of Headache Disorders distinguishes between episodic and chronic subtypes of TTH ( IHS 2013 ). Chronic TTH is diagnosed when headache occurs on 15 or more days per month on average for three or more months (180 or more days per year); otherwise TTH is considered to be episodic. Acute treatment with analgesics is more appropriate for episodic TTH, while both pharmacological and non\u2010pharmacological treatments are used for managing chronic TTH. Structural changes in the brain have been reported in people with chronic TTH ( Fumal 2008 ). Furthermore, management of TTH in children and adolescents poses a clinically diverse situation (establishing diagnoses, dosages, nature of preparation, pharmacodynamics, etc.; Monteith 2010 ). For these reasons, this review focused on the acute treatment of episodic TTH in adults. Episodic TTH is subdivided into infrequent and frequent subtypes ( IHS 2013 ). Infrequent episodic TTH is defined by the following criteria. Lifetime history of at least 10 episodes occurring on fewer than one day per month (fewer than 12 days per year) and satisfying criteria 2 through to 4 below. Headache lasting from 30 minutes to seven days. Headache has at least two of the following characteristics: bilateral location; pressing or tightening (non\u2010pulsating) quality; mild or moderate intensity; not aggravated by routine physical activity such as walking or climbing stairs. Both: no nausea or vomiting (anorexia may occur); no more than one of photophobia or phonophobia. Not attributed to another disorder. Lifetime history of at least 10 episodes occurring on fewer than one day per month (fewer than 12 days per year) and satisfying criteria 2 through to 4 below. Headache lasting from 30 minutes to seven days. Headache has at least two of the following characteristics: bilateral location; pressing or tightening (non\u2010pulsating) quality; mild or moderate intensity; not aggravated by routine physical activity such as walking or climbing stairs. Both: no nausea or vomiting (anorexia may occur); no more than one of photophobia or phonophobia. Not attributed to another disorder. Frequent episodic TTH is diagnosed when there is a lifetime history of least 10 episodes occurring on at least one day but fewer than 15 days per month for at least three months (at least 12 and fewer than 180 days per year), and when criteria 2 through to 5, above, are also met. The Global Burden of Diseases Study 2010 reported global prevalence of TTH as 21%, making it the second most prevalent condition after dental caries, and slightly more prevalent than migraine ( Vos 2012 ). The exact pathogenesis of TTH is still unknown and is said to be multifactorial, including central dysfunction of pain processing pathways and peripheral myofascial factors. There is a general agreement that peripheral myofascial nociception disturbances play a greater role in the pathogenesis of both frequent and infrequent episodic TTH ( Bendtsen 2016 ; Fern\u00e1ndez\u2010de\u2010las\u2010Pe\u00f1as 2010 ; Fumal 2008 ). Description of the intervention Medicines derived from willow bark, which is rich in salicylate, have been used for centuries for treating pain, fever, and inflammation. In the mid\u201019th century, chemists first synthesised acetylsalicylic acid, and by the end of the century, Bayer had patented and was selling the drug, which the company called aspirin, around the world. Aspirin is used to treat mild to moderate pain, including migraine headache pain; inflammatory conditions such as rheumatoid arthritis; and, in low doses, it is used as an antiplatelet agent in cardiovascular disease. Its efficacy for treating acute pain has been well demonstrated ( Moore 2011 ). It is a potent gastrointestinal irritant, and may cause discomfort, ulcers, and bleeding. It may aIso cause tinnitus at high doses, and it is no longer used in children and adolescents, in whom it may cause Reye's syndrome (swelling of the brain that may lead to coma and death) ( Glasgow 2006 ). Its use as an analgesic and antipyretic agent has declined, largely due to concerns about these adverse events, as newer products have become available. However, in some countries, it may be the only drug readily available, and for some conditions, such as migraine, some people report it to be an effective and reliable treatment ( Kirthi 2013 ). Aspirin is available in various strengths, ranging from 75 mg to about 1.5 g ( Drugs.com ). For the treatment of pain, the British National Formulary recommends 300 mg to 900 mg orally every four to six hours as needed (maximum 4 g daily) or 450 mg to 900 mg rectally (maximum 3.6 g daily), in adults ( BNF 2016 ). Aspirin 500 mg is commonly available in some parts of the world. For oral administration, aspirin is available in three formulations: standard tablet, soluble tablet, and enteric\u2010coated tablet. How the intervention might work Aspirin irreversibly inhibits cyclo\u2010oxygenase enzymes, which are needed for prostaglandin and thromboxane synthesis. Prostaglandins mediate a variety of physiological functions such as maintenance of the gastric mucosal barrier, regulation of renal blood flow, and regulation of endothelial tone. They also play an important role in mediating inflammatory and nociceptive processes. Suppression of prostaglandin synthesis is believed to underlie the analgesic effects of aspirin ( Vane 1971 ). Why it is important to do this review Episodic TTH is ubiquitous, affecting a large proportion of adults. Despite being generally mild to moderate in intensity, headache results in considerable suffering to the affected person and contributes overall to a significant loss of productivity to society ( Mannix 2001 ; Rasmussen 2001 ; Steiner 2004 ; Stovner 2007 ). The treatment of episodic TTH is essentially pharmacological. Seeking relief, people generally self\u2010medicate with one or more medicines, and OTC medicines are often used ( Forward 1998 ). Aspirin is a readily accessible OTC analgesic; as a generic drug, it could be a drug of choice or the first\u2010line drug for management of TTH, particularly in low\u2010resource settings. It has been shown to work in individual studies and one systematic review ( Moore 2014 ; Steiner 2003 ). Authors of two guidelines on the management of TTH have reviewed the effectiveness of treatment modalities. In both they adopted a consensus methodology. The BASH guidelines are based on a limited review of studies ( BASH 2010 ). The EFNS guidelines are based on a more detailed and thorough search of the literature ( Bendtsen 2010 ). Moreover, the EFNS guidelines represent an improvement over the BASH guidelines in that they used a standard, published protocol for developing management guidelines ( Brainin 2004 ). That protocol strongly recommends active and frequent consultation of the Cochrane Library, and this suite of reviews is being carried out to provide relevant evidence (see Derry 2015 ; Stephens 2016 ; Veys 2016 ). One non\u2010Cochrane systematic review by Verhagen and colleagues followed methods similar to those used in Cochrane Reviews and evaluated the efficacy and tolerability of analgesics for the acute treatment of episodes of TTH in adults, but the authors analysed the non\u2010standard measure \"pain relief or recovery over 2 to 6 hours\" as the main efficacy outcome ( Verhagen 2006 ). Reviews explicitly adopting Cochrane methods and evaluating the more focused outcomes recommended in the International Headache Society (IHS)'s recently updated guidelines for controlled trials of drugs in TTH ( IHS 2010 ) are clearly important. One survey of TTH study methods and reporting demonstrated that these are seldom adhered to in clinical trials, but did report a variety of outcomes, including IHS\u2010preferred outcomes, for aspirin, ibuprofen, ketoprofen, and paracetamol ( Moore 2014 ).",
        "summary": "For adults with episodic tension\u2010type headache, low\u2010certainty evidence suggests that, compared with placebo, aspirin 1000 mg may result in less need for rescue medication (on average, 158 vs 339 per 1000 people), with no apparent impact on adverse events. For the other outcomes, event rates were low and certainty of evidence was very low, precluding any conclusions."
    },
    "CD004532": {
        "query": "Does routine surgery as an adjunct to chemotherapy provide any benefit during the treatment of spinal tuberculosis?",
        "document": "Background Incidence Tuberculosis is the most common infectious disease in the world. Every year 10 million new people are infected ( WHO 2005 ). While tuberculosis commonly infects the lungs, it is located in the spine in one to two per cent of people ( Watts 1996 ). Pathology Tuberculosis of the spine is potentially serious. The infection can cause pain and destroy the bone making the vertebral bodies collapse, thereby flexing the spine forward (kyphosis) ( Figure 1 ). Sometimes a nerve root may be compressed causing pain along the root or deficit, but more commonly spinal cord compression may lead to myelopathy (loss of feeling and muscle control) or paraplegia. Even lung function may be compromised ( Smith 1996 ). If there is a sharp angle in the spine due to bony destruction, loss of neurological function may manifest only after years, even if the tuberculosis has been cured adequately ( Hsu 1988 ; Rajeswari 1997a ; Luk 1999 ). This is the result of chronic compression of the spinal cord or a local reactivation. Late paraplegia due to spinal cord compression is a major problem because an operation at this stage is complex and prone to major complications often without subsidence of the neurological deficit ( Moon 1997 ). If the bone has fully fused in a normal position after the primary illness period, this late consequence is thought not to occur ( Leong 1993 ). Lateral radiograph of the spine shows a kyphosis angle because two vertebral bodies were destroyed by tuberculosis; the bodies have fused, and further deterioration of the angle is unlikely. The angle is measured by drawing lines parallel to the healthy vertebral bodies above and below the fused bodies Most experts believe that a kyphosis over 30\u00b0 is likely to generate back pain and to deteriorate ( Kaplan 1952 ; Rajeswari 1997b ; Wimmer 1997 ; Parthasarathy 1999 ( see ICMR/MRC 1989 )). Vertebral body bone loss is a measure of destruction of the bone as seen on lateral radiographs. It is expressed as units (U), 1.0 U meaning a complete vertebral body and 0.0 U meaning no bone loss; for example, if two bodies are partially destroyed, one lost 50% of its volume and the other 25%, the bone loss is 0.75 U. It has been claimed to predict the final kyphosis angle ( Rajasekaran 1987 ). Diagnosis Diagnosis of spinal tuberculosis in endemic areas is made mainly using radiographs. Active disease is diagnosed when there is loss of the thin cortical outline and rarefaction of the affected vertebral bodies ( MRC 1974a ). Ideally there is a positive culture from the site of the lesion. Treatment Tuberculosis in general is curable. The mainstay of treatment is chemotherapy with at least isoniazid, rifampicin, and pyrazinamide. The American Thoracic Society recommends six months of chemotherapy for spinal tuberculosis in adults and 12 months in children because reliable data are lacking on shorter treatment duration ( Bass 1994 ). The British Thoracic Society recommends six months of treatment irrespective of age ( BTS 1998 ). In their recent review of the literature, Van Loenhout\u2010Rooyackers and colleagues found that six months of treatment is probably sufficient for everyone ( Van Loenhout 2002 ). Goals of treatment In tuberculosis, treatment is considered to be successful when the person is cured, is no longer infectious, and does not suffer relapse. However, some additional unique problems are encountered in spinal tuberculosis, namely, kyphosis angle and neurological deficit. Treatment in spinal tuberculosis is directed toward controlling or correcting the kyphosis angle thereby restoring the balance of the spine, restoring normal neurology, preventing pain, achieving early bony fusion (healing), preventing local recurrence of spinal tuberculosis, and preventing bone loss. Furthermore, people need to regain their previous activity level to enable them to resume their normal lives, school, jobs, and sports. Human immunodeficiency virus (HIV) increases the risk of reactivation of a latent focus and progression of the disease to a more atypical and severe course. Studies directed specifically at spinal tuberculosis and HIV conclude that good clinical outcomes can be expected irrespective of the HIV status and the availability of antiretroviral therapy ( Leibert 1996 ; Govender 2000 ). Another report mentions that people with HIV are not a homogeneous group, and that results \u2212 especially complications like wound infections \u2212 worsen during the end stage of the disease ( Jellis 1996 ). Role of surgery There is controversy in the literature about the necessity of additional surgical intervention to spinal tuberculosis treatments. This difference of opinion goes back to 1960 when Hodgson and Stock advocated surgical treatment ( Hodgson 1960 ), and Konstam and colleagues advocated conservative treatment ( Konstam 1958 ; Konstam 1962 ). Conservative treatment consists of only medication and sometimes additional non\u2010operative measures (physical therapy, orthosis, and bed rest). Surgery can basically be divided into two procedures. The first is a debridement. This is a procedure that comprises surgical removal of the infected material. No attempt is made at stabilizing the spine. The second form, which is more extensive, is a debridement with stabilization of the spine (spinal reconstruction). The reconstruction has always been performed with bone grafts. Today, countries with sufficient resources perform stabilization using artificial materials like steel, carbon fibre, or titanium (instrumentation). Although randomized controlled trials investigating indications are lacking, many authors consider the following indications for surgical intervention: (1) neurological deficits (with an acute or non\u2010acute onset) caused by compression of the spinal cord; (2) spinal instability caused by destruction or collapse of the vertebrae, destruction of two or more vertebrae, or kyphosis of more than 30\u00b0; (3) no response to chemotherapeutic treatment; (4) non\u2010diagnostic biopsy; and (5) large paraspinal abscesses ( Vidyasagar 1994 ; Chen 1995 ; Nussbaum 1995 ; Rezai 1995 ; Boachie\u2010Adjei 1996 ; Watts 1996 ; Moon 1997 ). Some authors even advocate surgery in mild cases of spinal tuberculosis ( Leong 1993 ; Luk 1999 ; Turgut 2001 ). Potential benefits of surgery are less kyphosis, immediate relief of compressed neural tissue, quicker relief of pain, a higher percentage of bony fusion, quicker bony fusion, less relapse, earlier return to previous activities, and less bone loss. It may also prevent late neurological problems due to kyphosis of the spine if fusion has not occurred ( Hsu 1988 ; Leong 1993 ). Surgery requires expertise, good anaesthesia, and excellent peri\u2010operative care. It also requires hospitalization, and is expensive and potentially dangerous. Complications can occur during the operation or postoperatively. Complications of spinal surgery can be divided into several groups: reconstruction\u2010related, vascular, neurological, visceral, and wound\u2010related. Reconstruction failures can be breakage of the graft, screws and rods, loss of correction, and failure of fusion ( Jutte 2002 ). Vascular problems during surgery can be massive bleeding, haematoma formation, and thromboembolism. Neurological damage of surgery can be nerve root lesion, dura tears, spinal cord infarction, and plexus lesions. Visceral damage, especially ureteric lesions, can occur. Wound infections happen in 1% to 6% of spinal surgeries ( Fardon 2002 ). Considering the potential complication rate, surgery should only be performed if there is a clear benefit.",
        "summary": "There was insufficient randomized controlled trial evidence to judge the effects of adding surgical debridement to chemotherapy in children and adults being treated for spinal tuberculosis. The only two trials identified used obsolete or specialized TB treatment regimens, and the analyses were small and underpowered. When surgical debridement plus chemotherapy (p\u2010amino salicylic acid or rifampicin plus isoniazid) was compared with the same chemotherapy regimen alone in children and adults with spinal tuberculosis (TB; 71% had one or two vertebrae involved; location of the spinal lesion was thoracic/thoracolumbar [T1 to T10/T11 to L2] in 51% and lumbar/lumbosacral [L2 to S1] in 49%), there was no apparent difference between groups in terms of deterioration in Kyphosis angle >10\u00b0, improvements in neurological deficit, bony fusion, absence of spinal TB, regained activity level, bone loss or adverse events. However, all of the analyses had a small number of participants (between 10 and 262), and it is therefore impossible to determine whether there were no differences between groups, or whether there were just too few participants to be able to detect differences that were there."
    },
    "CD012310": {
        "query": "What are the effects of continuous local anesthetic wound infusion for adults undergoing midline laparotomy for colorectal resection?",
        "document": "Background Description of the condition Open colorectal resection is associated with substantial postoperative pain. It is the definitive treatment for a broad range of benign and malignant conditions of the large bowel, including colorectal cancer, inflammatory bowel disease, diverticular disease, and large bowel obstruction. In 2012, the worldwide annual incidence of colorectal cancer was 1.4 million. It was the second most common cancer in women after breast cancer, and the third most common cancer in men after lung and prostate cancer ( WCR 2014 ). In the UK, approximately 60% of people with colorectal cancer will undergo major colorectal resection ( NBOCA 2015 ). Since advancement of laparoscopic surgical techniques, the number of cases of elective, open resections for colorectal cancer has decreased in high\u2010income countries, such as the UK and Australia ( BCCA 2015 ; NBOCA 2015 ). In Australia, the rate of open resection decreased from approximately 70% in 2009 to approximately 40% in 2014 ( BCCA 2015 ). Nevertheless, open resection remains necessary in many settings, such as in people with locally advanced disease or unfavourable anatomy, for unplanned emergency cases, or in poorly resourced communities ( Amin 2015 ; Plummer 2011 ; Ray\u2010Offor 2014 ; SAGES 2012 ). Open colorectal resection is commonly performed through a midline incision in the abdominal wall. Midline incisions provide easy, quick and excellent exposure of the abdominal cavity, and are particularly useful for complex, exploratory or urgent procedures. However, midline incisions transect nerve fibres crossing the abdominal wall in a mediocaudal direction, which results in more postoperative pain compared to other incisions ( Brown 2005 ; Burger 2002 ; Grantcharov 2001 ). Standard elective open colorectal resection typically requires a postoperative hospital stay of 6 to 10 days on average ( Walter 2009 ; Wind 2006 ). The main factors hindering early recovery and discharge are thought to include postoperative pain and delayed return of bowel function ( Kehlet 2008 ). Multimodal analgesia aims to achieve more effective pain relief and reduce adverse events through the additive or synergistic effects of different analgesic agents or routes of administration ( Buvanendran 2009 ; Jin 2001 ; Kehlet 1999 ). However, up to 70% of people undergoing major abdominal surgery still experience moderate to severe postoperative pain and opioid\u2010related adverse events, such as nausea, vomiting and ileus, despite a multimodal analgesia protocol involving patient\u2010controlled analgesia (PCA) with opioids combined with opioid\u2010sparing agents ( Apfelbaum 2003 ; Gan 2014 ; Sommer 2008 ). Description of the intervention New pump or balloon devices allow local anaesthetics, such as bupivacaine, levobupivacaine and ropivacaine, to be continuously infused into tissues surrounding an incisional wound via a multi\u2010lumen, indwelling catheter, placed by the surgeon prior to wound closure. For abdominal surgery, the wound catheter may be positioned within the subcutaneous (suprafascial), musculofascial or preperitoneal (subfascial) layers of the anterior abdominal wall. How the intervention might work Local anaesthetics produce analgesic effects by decreasing the excitability of peripheral nociceptive nerve fibres by inhibiting voltage\u2010gated sodium channels ( Butterworth 1990 ). Local anaesthetics also possess anti\u2010inflammatory and antimicrobial properties ( Hollmann 2000 ; Johnson 2008 ). Continuous wound infusion allows the direct and sustained action of a local anaesthetic within tissues surrounding an incisional wound, inhibiting parietal nociception. Use of other drugs in multimodal analgesia is needed for complete analgesia by coverage of visceral nociception. When used within a multimodal analgesia protocol, continuous local anaesthetic wound infusion may reduce postoperative pain, reduce opioid consumption and postoperative opioid\u2010related adverse events, and reduce the length of hospital stay. Furthermore, continuous local anaesthetic wound infusion may be an effective alternative to analgesic modalities such as epidural analgesia or peripheral nerve blocks, especially in situations where these techniques are impractical, difficult, poorly tolerated, or contraindicated ( Rawal 2012 ). Why it is important to do this review Previous systematic reviews and meta\u2010analyses included a broad range of surgeries, including gastrointestinal surgery, obstetric and gynaecological surgery, urological surgery, cardiothoracic surgery and orthopaedic surgery ( Gupta 2011 ; Liu 2006 ), and abdominal incisions, including midline, paramedian, oblique and laparoscopic ( Karthikesalingam 2008 ; Ventham 2014 ). The validity and relevance of pooling outcomes from different surgical procedures have been questioned, since the mechanisms and intensity of pain, the placebo response and the treatment effects differ between different surgical procedures and surgical incisions ( Beaussier 2012 ; Espitalier 2013 ; Gerbershagen 2013 ; Gerbershagen 2014 ; Gray 2005 ). There is increasing recognition of the need for evidence\u2010based guidelines for procedure\u2010specific pain management ( Joshi 2013 ; Kehlet 2007 ). At present, there is no systematic review and meta\u2010analysis examining the procedure\u2010specific outcomes of continuous local anaesthetic wound infusion after midline laparotomy for colorectal resection.",
        "summary": "For adults (mean age 56 to 70 years) undergoing midline laparotomy for elective colorectal resection, administration of continuous local anesthetic via a multi\u2010lumen catheter embedded within or adjacent to the incisional wound for 48 to 96 hours reduces pain intensity on the first postoperative day, both at rest (by 0.59 points on a 10\u2010point scale; high\u2010certainty evidence; all results on average) and on movement (by 1.13 points on a 10\u2010point scale; low\u2010certainty evidence), which is accompanied by a reduction in the use of patient\u2010controlled opioid analgesia (by 11.98 mg morphine equivalent; moderate\u2010certainty evidence). The impact of continuous local anesthetic on pain intensity on the second and third postoperative days at rest or on movement is less clear, but there was a reduction in use of patient\u2010controlled opioid analgesia on day 2 (by 9.64 mg morphine equivalent; moderate\u2010certainty evidence). Moderate\u2010 to high\u2010certainty evidence revealed other benefits of continuous local anesthetic to be shorter time to first bowel movement (by 0.67 days) and shorter hospital stay (by 1.17 days); there was no clear benefit in time to ambulation. Rates of adverse events were generally low and comparable across groups. Rates of adverse events were generally low and comparable across groups; the rate of postoperative ileus seems to be higher than other adverse events, but the analysis included only 155 people and the result was very imprecise making it difficult to judge the impact of local anesthetics."
    },
    "CD000039-0": {
        "query": "What are the benefits and harms of blood pressure\u2010lowering therapies in people with acute stroke?",
        "document": "Background Description of the condition Stroke is the third most common cause of death and the most common cause of disability in the western world. Acute stroke, whether due to infarction or haemorrhage, is associated with high blood pressure in 75% of patients, of whom 50% have a previous history of high blood pressure ( Britton 1986 ; Oppenheimer 1992 ). After a stroke, blood pressure falls in most patients over a week although a third of patients remain hypertensive ( Wallace 1981 ; Britton 1986 ; Harper 1994 ). A number of small studies have assessed the relationship between blood pressure ( Marshall 1959 ; Adams 1965 ; Droller 1965 ; Bourestom 1967 ; Marquarsden 1969 ; Carlberg 1993 ) and outcome. A meta\u2010analysis of these studies found that elevated blood pressure was associated with a poor outcome ( Willmot 2004 ). Data from 17,398 participants in the International Stroke Trial identified a U\u2010shaped relationship such that both low and high blood pressure were associated independently with increased early death and later death or dependency ( Leonardi\u2010Bee 2002 ), a finding that has been replicated by others ( Castillo 2004 ; Vemmos 2004 ). High blood pressure is also associated with an increased early recurrence of stroke ( Leonardi\u2010Bee 2002 ; Sprigg 2006 ). The mechanisms underlying high blood pressure in stroke are complex but pre\u2010existing hypertension, hospitalisation stress, activation of the sympathetic renin\u2010angiotensin\u2010aldosterone, cortisol and natriuretic peptide neuroendocrine systems, and the Cushing reflex (raised blood pressure secondary to raised intracranial pressure) all contribute ( Myers 1982 ). In ischaemic stroke, high blood pressure also appears to adversely affect outcome through increasing the risk of cerebral oedema, but not haemorrhagic transformation ( Leonardi\u2010Bee 2002 ). Haematoma expansion is related to high blood pressure in people with intracerebral haemorrhage (ICH) although this relationship may be confounded by stroke severity and time to presentation ( Fujii 1994 ; Kazui 1997 ; Fujii 1998 ; Bath 2003 ). Description of the intervention Although debated more than 29 years ago, it still remains unclear whether high blood pressure should ( Spence 1985 ) or should not ( Yatsu 1985 ) be treated acutely following stroke. Recent guidelines recommend that acute lowering of blood pressure should be delayed for several days or even weeks unless blood pressure is greater than 220/120 mmHg, blood pressure is greater than 200/100 mmHg with end organ involvement (hypertensive encephalopathy, aortic dissection, cardiac ischaemia, pulmonary oedema, acute renal failure), or blood pressure is greater than 200/120 mmHg with primary ICH, are present ( O'Connell 1994 ; EUSI 2004 ; AHA\u2010HS 2010 ; RCP 2012 ; AHA\u2010IS 2013 ). Though the evidence is weaker, guidelines now recommend that patients who have elevated blood pressure and are otherwise eligible for treatment with recombinant tissue plasminogen activator may have their blood pressure lowered so that systolic blood pressure (SBP) is less than or equal to 185 mmHg and diastolic blood pressure (DBP) is less than or equal to 110 mmHg before thrombolysis using intravenous labetalol, nitroprusside or nicardipine and it should be maintained below 180/105 mmHg for at least the first 24 hours after therapy ( AHA\u2010IS 2013 ). Unfortunately, such guidelines are inconsistent and are based on theoretical arguments and individual case reports, and not on the results of systematic overviews or large intervention trials of blood pressure manipulation in acute stroke. Nevertheless, a number of case reports and series have suggested that active lowering of blood pressure in people with primary intracranial haemorrhage and ischaemic stroke may improve ( Dandapani 1995 ; Chamorro 1998 ) or worsen ( Graham 1975 ; Britton 1980 ; Fischberg 2000 ) outcome. Low blood pressure is not common in acute stroke but it, like high blood pressure, is associated with a poor outcome ( Leonardi\u2010Bee 2002 ). Possible reasons for low blood pressure include potentially reversible conditions such as hypovolaemia, sepsis, impaired cardiac output secondary to cardiac failure, arrhythmias or cardiac ischaemia, and aortic dissection ( Sprigg 2005 ). Guidelines recommend that causes of hypotension in the setting of acute stroke should be sought with the view to correcting reversible causes such as hypovolaemia and cardiac arrhythmias ( AHA\u2010IS 2013 ). Since cerebral autoregulation is lost following stroke ( Strandgaard 1973 ; Burke 1986 ; Paulson 1990 ), such that cerebral blood flow becomes dependent on systemic blood pressure, some researchers have hypothesised that blood pressure should be increased to improve cerebral perfusion ( Sandercock 1992 ) and a case series ( Rordorf 1997 ) and a pilot randomised trial of phenylephrine ( Hillis 2003 ) reporting this approach have been published. How the intervention might work Although the different drugs assessed work in a variety of ways, all lower (or elevate \u2010 phenylephrine) blood pressure. Why it is important to do this review We are reviewing this topic in three parts ( Bath 1997 ). Assessment of trials in which the primary aim of the intervention was to alter blood pressure in people with acute stroke with the aim of improving clinical outcome. A Cochrane review of blood pressure intervention in stroke published in 2001 ( BASC 2001 ) updated the original review published in 1997. It was again updated in 2008 to include more information from 13 trials published between 2003 and 2008 including a total of 1153 participants ( BASC 2009 ). With a relatively small amount of data, there was insufficient evidence to evaluate the effect of altering blood pressure during the acute phase of stroke. The present review includes all new trials completed and published since 2008. The total number of participants is now 17,011, a 14\u2010fold increase since the review in 1997 and 2008. Although many of the data are from trials testing blood pressure alteration in the acute phase (\u2264 48 hours), some recent trials have examined specific questions such as lowering blood pressure in ICH ( INTERACT pilot 2008 ; INTERACT\u20102 2013 ), with angiotensin receptor antagonists ( SCAST 2011 ), or glyceryl nitrate ( ENOS 2014 ) or in the pre\u2010hospital setting ( PIL\u2010FAST 2013 ; RIGHT 2013 ). Furthermore, two trials ( COSSACS 2010 ; ENOS 2014 ) have investigated whether to continue or stop temporarily pre\u2010stroke antihypertensive therapy. This systematic review includes these data and provides up\u2010to\u2010date evidence. Assessment of trials where vasoactive drugs were administered to people with acute stroke and where clinical outcome was measured. Drugs include: alpha receptor antagonists, angiotensin converting enzyme inhibitors (ACEI), angiotensin II receptor inhibitors (ARA), beta receptor antagonists, calcium channel blockers (CCB), diuretics, magnesium, naftidrofuryl, nitric oxide donors (nitrates), papaverine, pentoxifylline, prostacyclin, serotonin receptor antagonists, sympathomimetics, theophylline (and mimetics), thromboxane A2 antagonists, vinpocetine, and their derivatives. Aggregated patient data are analysed separately for drugs which lower and elevate blood pressure ( BASC 2000 ). Work on this analysis is ongoing. Work on this analysis is ongoing through the international Blood pressure in Acute Stroke Collaboration. In brief, individual patient data from the trials included in Part 1 and Part 2 are being collated with the intention of extending analyses, particularly in subgroups of participants and interventions.",
        "summary": "Blood pressure (BP)\u2010lowering therapies administered within the first 6 hours after an acute stroke may reduce the combined outcome of death/dependency and may slightly improve quality of life compared with placebo. Randomized controlled trials (RCTs) comparing placebo versus guideline\u2010based BP target (systolic BP < 180 mmHg or mean arterial pressure between 110 and 130 mmHg) found that use of BP\u2010lowering therapy within 6 hours of an acute ischemic stroke or intracerebral hemorrhage resulted in fewer people dying/becoming dependent (on average, 513 vs 549 per 1000 people; three RCTs, 3506 participants) and slightly improved quality of life (on average, by 0.06 points on the 5\u2010point EuroQoL Group Quality of Life Questionnaire; three RCTs, 6881 participants). Contrary to these results, RCTs observed no differences in mortality and worse disability/dependency among survivors given BP\u2010lowering therapy (on average, 18 points lower on the Barthel Index; one RCT with 273 participants) when reporting these two outcomes separately. Data show no differences between groups in terms of neurological deterioration. Analysis of overall data (including all times to administration, which varied from ultra\u2010acute/prehospital to within 168 hours) or other subgroups (type of BP\u2010lowering therapy, type of stroke, location of stroke, ultra\u2010acute/prehospital treatment, treatment within 48 hours, treatment within 168 hours) revealed no differences between groups in assessed outcomes. Reviewers did not assess clinically relevant adverse effects such as repeated stroke or other cardiovascular outcomes."
    },
    "CD007160": {
        "query": "In people with confirmed or suspected acute myocardial infarction, what are the effects of oxygen therapy?",
        "document": "Background Description of the condition Coronary heart disease (CHD) is an important cause of death worldwide. Over 7 million people every year die from CHD, accounting for 12.8% of all deaths ( WHO 2011 ). It is the single most common cause of death before the age of 75 in Europe ( Townsend 2015 ), and in the USA it accounted for around one of every seven deaths in 2011 ( Mozaffarian 2015 ), although deaths from cardiovascular disease and CHD in men and women have fallen in most developed countries. For example, rates of CHD deaths per million in men without diabetes in England fell by more than half between 1995 and 2010 ( Ecclestone 2015 ). According to the Euro Heart Survey of acute myocardial infarction (AMI) in 47 countries ( Puymirat 2013 ), in\u2010hospital mortality was 6.2%. Approximately 45% of the reduction in CHD mortality is attributable to improvement in medical therapies for coronary disease ( Capewell 2000 ). A common manifestation of CHD, often the first, is AMI. The third Global MI Task Force defines AMI as \"any evidence of myocardial necrosis in a clinical setting consistent with acute myocardial ischaemia\" ( Thygesen 2012 ). Myocardial ischaemia is usually the result of spontaneous complications of atherosclerosis (plaque rupture, ulceration, fissuring, erosion or dissection) resulting in coronary thrombosis (type 1 AMI). Other categories of AMI include: those produced by underlying CHD with an ischaemic imbalance attributable to a wide range of factors including endothelial dysfunction, coronary spasm, coronary embolism, tachy\u2010/brady\u2010arrhythmias and hypo\u2010 and hypertension (type 2 AMI); sudden cardiac death induced by myocardial ischaemia (type 3 AMI); and AMI occurring in the context of invasive coronary procedures such as percutaneous coronary intervention (PCI), in\u2010stent thrombosis, or coronary artery bypass grafting (CABG), categorised as subtypes 4a, 4b and 5 of AMI. By far the most common types of AMI are types 1 and 2, to such an extent that their incidence may be used as proxy variables to estimate the prevalence of CHD in the general population. Hereafter we will use the term 'AMI' to refer the type 1 and type 2 AMI. Myocardial injury may be detected through: highly sensitive biochemical markers such as troponin (I or T), or the MB fraction of the creatine kinase (CK\u2010MB); electrocardiographic changes; or imaging techniques such as echocardiography, magnetic resonance imaging (MRI) or radionuclide imaging. Necessary criteria to diagnose AMI in a clinical context include a change (rise and/or fall) in cardiac biomarker values, together with at least one of the following: ischaemic symptoms, typical electrocardiographic changes, or abnormalities in the structure or wall motion of the heart identified by imaging techniques. Moreover, the recognition that acute coronary syndromes represent a spectrum of pathophysiological processes rather than a uniform type of 'heart attack' has led to publication of separate guidelines with different therapeutic options for AMI presenting with persistent ST\u2010segment elevation (STEMI) and non\u2010STEMI (NSTEMI) presentations. The in\u2010hospital mortality rate of unselected STEMI patients according to the Euro Heart Survey, published by the European Society of Cardiology, varies between 6% and 14% ( Mandelzweig 2006 ). The most serious complications of AMI are cardiogenic shock, heart failure, ventricular fibrillation and recurrent ischaemia. Around 8% of people with AMI develop cardiogenic shock ( Babaev 2005 ), but this remains present in 29% of those people on admission to hospital. The Global Registry of Acute Coronary Events (GRACE) reported that heart failure occurred in 15.6% of people with STEMI and 15.7% of those with NSTEMI, but heart failure was present in only 13% of these patients on admission to hospital ( Steg 2004 ). Ventricular fibrillation occurred in 1.9% of people with AMI ( Goldberg 2008 ), and 21% of those with acute coronary syndromes presented with recurrent ischaemia ( Yan 2010 ), about half of whom experienced this outcome in the first 24 hours. Other possible complications of AMI include pericarditis, mitral insufficiency, arrhythmias and conduction disturbances. The cornerstone of contemporary management of people with STEMI is reperfusion therapy, with either primary percutaneous coronary intervention (PCI) or thrombolytic treatment if less than 12 hours has elapsed from the onset of symptoms. Other recommended treatments in international guidelines include morphine, oxygen (O 2 ), nitrates and aspirin (MONA) ( O'Connor 2010 ; O'Gara 2013 ; Steg G 2012 ). Some of these treatments have a well\u2010established research base, while others do not ( Nikolaou 2012 ; O'Driscoll 2008 ; SIGN 2010 ). Description of the intervention Inhaled oxygen at normal pressure delivered by face mask or nasal cannula, at any concentration. How the intervention might work Myocardial infarction occurs when the flow of oxygenated blood in the heart is interrupted for a sustained period of time. The rationale for providing supplemental oxygen to a person with AMI is that it may improve the oxygenation of the ischaemic myocardial tissue and reduce ischaemic symptoms (pain), infarct size and consequent morbidity and mortality. Why it is important to do this review Although it is biologically plausible that oxygen is helpful, it is also biologically plausible that it may be harmful. Potentially harmful mechanisms include the paradoxical effect of oxygen in reducing coronary artery blood flow and increasing coronary vascular resistance, measured by intracoronary Doppler ultrasonography ( McNulty 2005 ; McNulty 2007 ); reduced stroke volume and cardiac output ( Milone 1999 ); other adverse haemodynamic consequences, such as increased vascular resistance from hyperoxia; and reperfusion injury from increased oxygen free radicals ( Rousseau 2005 ), which may also have adverse electrophysiological effects, triggering lethal arrhythmias ( Xie 2009 ). A systematic review of human studies that included non\u2010randomised studies did not confirm that oxygen administration diminishes acute myocardial ischaemia ( Nicholson 2004 ). Indeed, some evidence suggested that oxygen may increase myocardial ischaemia ( Nicholson 2004 ). Another narrative review of oxygen therapy also sounded a cautionary note ( Beasley 2007 ). It referenced a randomised controlled trial (RCT) conducted in 1976 showing that the risk ratio (RR) of death was 2.89 (95% confidence interval (CI) 0.81 to 10.27) in participants receiving oxygen compared to those breathing air ( Rawles 1976 ). While this suggested that oxygen may be harmful, the increased risk of death could easily have been a chance finding. A systematic review looked at the effect of oxygen on infarct size in people with AMI and concluded that \"[t]here is little evidence by which to determine the efficacy and safety of high flow oxygen therapy in MI. The evidence that does exist suggests that the routine use of high flow oxygen in uncomplicated AMI may result in a greater infarct size and possibly increase the risk of mortality\" ( Wijesinghe 2009 ) . Despite this lack of robust evidence of effectiveness prior to the publication of our 2010 Cochrane review of the evidence, international guidelines widely recommended oxygen administration ( AARC 2002 ; AHA 2005 ; Anderson 2007 ; Antman 2002 ; ILCOR 2005 ; Van de Werf 2008 ). Some guidelines were more cautious; for example, the European guideline did not recommend routine oxygen use in acute coronary syndrome (ACS) ( Bassand 2007 ), and the Scottish Intercollegiate Guidelines Network (SIGN) guidance only recommended oxygen use in hypoxaemia (< 90% saturation), noting that there was no clinical evidence for its effectiveness and referring to animal models that showed a reduction in infarct size ( SIGN 2007 ). Guidelines published since the 2010 Cochrane review have tended to move to a more cautious position reflecting the lack of evidence. In 2010, for example, the American Heart Association Guidelines for Cardiopulmonary Resuscitation and Emergency Cardiovascular care stated that: \"EMS providers administer oxygen during the initial assessment of patients with suspected ACS. However, there is insufficient evidence to support its routine use in uncomplicated ACS. If the patient is dyspnoeic, hypoxaemic, or has obvious signs of heart failure, providers should titrate therapy, based on monitoring of oxyhaemoglobin saturation, to 94% (class I, level of evidence: C). Updated SIGN guidance states, \"A Cochrane review found no conclusive evidence from randomised controlled trials to support the routine use of inhaled oxygen in patients with AMI. There is no evidence that routine administration of oxygen to all patients with the broad spectrum of acute coronary syndromes improves clinical outcome or reduces infarction size\" ( SIGN 2010 ). In 2011 an addendum to the National Heart Foundation of Australia/Cardiac Society of Australia and New Zealand Guidelines for the Management of Acute Coronary Syndromes (ACS), authors stated that \"There is currently insufficient evidence to formulate clear recommendations about oxygen therapy . . . Definitive trials are needed to answer this question\" ( Chew 2011 ). Similarly, the 2012 ESC guidelines for STEMI, citing the Cochrane review, now state: \"Oxygen (by mask or nasal prongs) should be administered to those who are breathless, hypoxic, or who have heart failure. Whether oxygen should be systematically administered to patients without heart failure or dyspnoea is at best uncertain. Noninvasive monitoring of blood oxygen saturation greatly helps when deciding on the need to administer oxygen or ventilator support\" ( Steg G 2012 ). The 2013 ACCF/AHA Guideline for the Management of ST Elevation Myocardial Infarction shows a similar change in emphasis: \"Few data exist to support or refute the value of the routine use of oxygen in the acute phase of STEMI, and more research is needed. A pooled Cochrane analysis of 3 trials showed a 3\u2010fold higher risk of death for patients with confirmed AMI treated with oxygen than for patients with AMI managed on room air. Oxygen therapy is appropriate for patients who are hypoxaemic (oxygen saturation < 90%) and may have a salutary placebo effect in others. Supplementary oxygen may, however, increase coronary vascular resistance. Oxygen should be administered with caution to patients with chronic obstructive pulmonary disease and carbon dioxide retention\". ( O'Gara 2013 ). The British Heart Foundation (BHF), in response to the doubts about oxygen use raised by Beasley 2007 , originally stated in an article in The Guardian in 2007 that \"[t]he current practice of giving high\u2010flow oxygen is an important part of heart attack treatment. Best practice methods have been developed and refined over the years to ensure the best possible outcome for patients. There is not enough evidence to change the current use of oxygen therapy in heart attacks\". Five years after the publication of the first Cochrane Review, the use of oxygen in AMI and across the spectrum of coronary acute syndromes is still controversial ( Shuvy 2013 ). We think that, given the evidence cited, it would have been more appropriate to conclude that despite decades of use there is inadequate clinical trial evidence to unequivocally support routine administration of oxygen. The BHF subsequently stated that the 2010 Cochrane review \"highlights the need for more research into the effects of oxygen when it is given during a heart attack. Until recently, heart attack patients were routinely treated with oxygen but we simply do not have enough evidence to know if that treatment is beneficial or harmful\" ( BHF 2010 ). Despite the attention given to the uncertainty around the role of oxygen since our 2010 Cochrane review, practice appears to vary, possibly because the evidence base informing current guideline recommendations remains uncertain. A survey of 231 cardiac care units in the UK undertaken shortly after the 2010 review reported that only a third adhered to guideline recommendations to titrate oxygen to saturation rather than administer routinely, and practice was no different in hospitals that had formal oxygen therapy policies versus those that did not ( Ripley 2012 ). With the lack of collective certainty about the use of oxygen, a number of clinical trials are now underway or have recently been reported to reassess this treatment. In general, practice should not be based on tradition but on proven benefit and safety. Given that the 1976 trial was suggestive of potential harm from oxygen in suspected AMI ( Rawles 1976 ), it is important to systematically review and update the evidence base for current and future guidance regarding the role of oxygen therapy in heart attack patients, and if necessary, to undertake further research to clarify whether this intervention does more harm than good. If the only robust evidence is suggestive of potentially serious harm, even if the result is not statistically significant, it reinforces our opinion that this intervention should not be routinely used, however sound the pathophysiological reasoning.",
        "summary": "In people with confirmed or suspected acute myocardial infarction (AMI), very low\u2010 to low\u2010quality evidence fails to demonstrate any clinical benefit in routine use of oxygen compared with room air when evaluating mortality, opiate use, or complications of AMI (cardiac failure, recurrent MI or major bleeding) at four weeks\u2019 follow\u2010up."
    },
    "CD012902": {
        "query": "Can antibiotic prophylaxis help prevent cerebrospinal fluid\u2013shunt infection?",
        "document": "Background Description of the condition Hydrocephalus is a condition in which cerebrospinal fluid (CSF) accumulates in the cerebral ventricles and subarachnoid spaces, resulting in dilatation of the ventricular system and an increase in intracranial pressure ( Rekate 1999 ). There are two basic forms of hydrocephalus: non\u2010communicating and communicating. Non\u2010communicating hydrocephalus is caused by structural blockage of the CSF within the ventricular system and is the most common form of hydrocephalus in children. Communicating hydrocephalus can be caused either by the excessive production of CSF by the plexus choroideus or by inadequate resorption of CSF by the subarachnoid villi. Treatment of hydrocephalus comprises the drainage of excessive CSF through the implantation of shunts (a tube to drain cerebrospinal fluid from the brain to a body cavity, usually to the abdominal cavity), external drains (a tube to drain cerebrospinal fluid out of the body) or an endoscopic third ventriculostomy (a procedure in which cerebrospinal fluid circulation is restored by making a passage through the floor of the third ventricle). The choice of treatment and its efficacy differs according to the individual's age and the aetiology of the condition ( Fu 2002 ; Hebb 2001 ; Limbrick 2014 ). The main complication of CSF\u2010shunt surgery is the incidence of CSF\u2010shunt infection (average incidence 3% to 20%) ( Borgbjerg 1995 ; Drake 1998 ; Greenberg 2010 ; James 2014 ; Kestle 2011 ; Kestle 2016 ; Konstantelias 2015 ; Simon 2009 ); the highest proportion of infections is seen in infants ( Bondurant 1995 ; Casey 1997 ). The symptoms associated with a shunt infection can be non\u2010specific, such as fever, nausea, lethargy (a lower level of consciousness), anorexia (a lack of appetite for food) or irritability. Symptoms of shunt infections in children tend to be more distinctive, such as high fever with or without concomitant meningitis, and rapid neurological deterioration. Up to 29% of individuals presenting with shunt malfunction have been shown to have a shunt infection, as confirmed by positive CSF cultures (a method to multiply micro\u2010organisms in order to determine the type of organism) ( Greenberg 2010 ). Shunt infections can be treated by long\u2010term administration of antibiotics, but in most cases shunt revision is required ( Greenberg 2010 ; Simon 2010 ). Both antibiotics and shunt revision can lead to longer hospital stays, additional complications and greater associated costs ( Attenello 2010 ; Sciubba 2007 ); hence minimising shunt infections would be beneficial to both patients and to the healthcare system. Description of the intervention Currently, antibiotics used for the prevention of shunt infections can be administered in five ways: orally; intravenously; intrathecally; topically; and via the implantation of antibiotic\u2010impregnated shunt catheters. Antibiotics given via the oral route are used as add\u2010on therapy in the treatment of CSF\u2010shunt infections, but are rarely used to prevent CSF\u2010shunt infections ( Frame 1984 ). Intravenous pre\u2010operative antibiotics are widely used as shunt infection prophylaxis, and appear to lower the risk of such infections ( Klimo 2014 ); however, these systemic antibiotics infiltrate the central nervous system poorly, and so intravenous antibiotics are often combined with antibiotics administered via one of the other routes. Ragel 2006 found that the addition of intrathecal gentamicin and vancomycin to intravenous cefazolin reduced the shunt infection rate to 0.42% (from 5.4% in the intrathecal gentamicin plus intravenous cefazolin administered to the control group). Intrathecal antibiotics are usually administered intraoperatively; however, Moussa 2016 used a shunt containing a reservoir in which a prophylactic antibiotic was injected. They showed that an additional administration of antibiotics one week after surgery resulted in a lower shunt infection rate than intra\u2010operative administration alone. Another option is the administration of topical antibiotics. This route is partly similar to intrathecal administration but provides the opportunity of covering the entire drainage route (including the extracranial pathway), since the drain is drenched in an antibiotic agent. A relatively new technique in the field of shunt infection prevention is the antibiotic\u2010impregnated catheter. These catheters, which are impregnated with two antibiotic agents, slowly release antibiotics over a period of days, and they have been shown to significantly reduce the rate of shunt infections ( Konstantelias 2015 ). However, such catheters are relatively expensive when compared with the previously mentioned administration routes: for example the incremental cost of topical vancomycin are USD 10 per surgery versus USD 400 for antibiotic\u2010impregnated catheters ( van Lindert 2018 ). In addition, Konstantelias 2015 found that antibiotic\u2010impregnated shunt catheters had a higher probability of colonisation by strains of bacteria that are more virulent than coagulase\u2010negative staphylococci (CoNS), which can result in a more severe infection. Another concern regarding the use of antibiotic\u2010impregnated shunt catheters was noted by James 2014 , who found that individuals who needed shunt replacement after the implantation of an antibiotic\u2010impregnated shunt catheter were more prone to infections than those who were initially treated with other types of shunt. How the intervention might work Foreign materials, such as a shunt or an external drain, are prone to infection when placed in the body, and once infected, the management can be challenging. Antibiotic agents have a bactericide or bacteriostatic function that helps to eliminate bacterial infections. These functions are also useful when the prevention of bacterial colonisation is the aim. Although a ventriculoperitoneal shunt can be in situ for years, most shunt infections occur within two months of surgery ( Greenberg 2010 ). The source of the infection is usually bacteria from the individual\u2019s own skin ( Yogev 1985 ). Hence contamination of the shunt takes place during, or early after, surgery, which suggests that the perioperative administration of antibiotic prophylaxis could be effective in the prevention of shunt infections. Why it is important to do this review CSF\u2010shunt infection is a major problem in individuals (including children) with hydrocephalus, with a reported proportion of shunt infection rates of 3% to 20% ( Greenberg 2010 ). Shunt infections have a very high impact, both clinically (repeat surgery, prolonged hospitalisation, neurological deterioration) and economically (we estimated that each infection is associated with incremental costs equating to EUR 30,000). Hence a reduction in the shunt infection rate is urgently needed. Prophylactic antibiotics are currently the main prevention strategy in use; however, the best route of administration for the prevention of shunt infection remains to be determined ( Ratilal 2008 ). This Cochrane Review has the potential to establish whether antibiotic prophylaxis has a positive effect on the proportion of CSF\u2010shunt infections and could direct which route of administration is the most effective. Some other aspects of antibiotic treatment (duration of treatment, dose and intervals between doses) are also unknown, but due to the potentially wide variety in these three factors, we did not include them in our review.",
        "summary": "Low\u2010certainty evidence suggests that in adults and children receiving a cerebrospinal fluid\u2013shunt, antibiotic treatment may result in fewer people developing shunt infection compared with placebo or standard care (50 vs 91 per 1000 people; all values on average and calculated using median event rates) within a mean follow\u2010up of 6.4 months. Subgroup analysis of intravenous administration was consistent with the main analysis. Although analyses showed some evidence of lower rates of infection between intrathecal administration and placebo or standard care (55 vs 75 per 1000 people with infection within 10 months; low\u2010certainty evidence) as well as between antibiotic\u2010impregnated catheters and catheters not impregnated (60 vs 167 per 1000 people with infection within nine months; very low\u2010certainty evidence), these results are likely to be underpowered, and differences between groups did not reach statistical significance. Adverse events were not assessed."
    },
    "CD005117": {
        "query": "How does diacerein affect outcomes in people with osteoarthritis?",
        "document": "Background Description of the condition Osteoarthritis (OA) is the most prevalent musculoskeletal disease ( ACR 2000 ; Picavet 2003 ). The World Health Organization (WHO) Scientific Group on Rheumatic Diseases estimates that 10% of the world\u2019s population aged 60 or older have significant clinical problems attributed to OA ( Woolf 2003 ). As the incidence and prevalence of OA increase with age, the increase in life expectancy will result in an increase in OA in the future ( Sun 2007 ; Woolf 2003 ), making this disease an ever growing public health problem. More than 10% of the US adult population had clinical OA in 2005, and in 2009, OA was the fourth most common cause of hospitalisation. OA is the leading indication for joint replacement surgery; 905,000 knee and hip replacements were performed in 2009 at a cost of $42.3 billion ( Murphy 2012 ). Obesity is a strong risk factor for OA of the knee and hip ( Murphy 2012 ). OA remains an enigmatic disease. It is defined as a condition characterised by focal areas of loss of articular cartilage within the synovial joints, associated with hypertrophy of the bone (osteophytes and subchondral bone sclerosis) and thickening of the capsule ( Lawrence 1998 ; Zhang 2001 ). Recently, OA has been relabeled as a whole organ disease because pathological abnormalities such as periarticular muscle weakness, lax ligaments, low\u2010grade synovitis, meniscal degeneration and neurosensory system alteration are often present in these patients ( Bijlsma 2012 ). Description of the intervention Treatments for OA include pharmacological and non\u2010pharmacological therapies and surgical procedures. Pharmacological therapies consist of topical agents, oral (systemic) agents, adjunct therapies and nutraceuticals ( Bellamy 2006 ; Towheed 2006 ; Towheed 2008 ). Although some drugs and/or compounds have been available for several decades and are integrated as standard practice in many countries, their efficacy has been demonstrated only over the past decade. Revision of drug registries by health authorities in various European countries in the 1990s led to appropriate clinical trials for available drugs (such as avocado extract), as well as drugs in development at that time (such as diacerein). This action of health authorities greatly improved knowledge regarding the level of evidence and characteristic treatment effects of these drugs (onset of action, carry\u2010over effect) ( Hochberg 2001 ). Current therapies for OA, including non\u2010steroidal anti\u2010inflammatory drugs (NSAIDs), although effective against symptoms of the disease, are palliative and do not stop disease progression. However, promising agents and compounds have been shown to reduce the severity of the disease, as well as the symptoms. Among them is diacerein, an oral interleukin (IL)\u20101beta inhibitor. Its active derivative, rhein, is an anthraquinone found in plants of the genus Cassia . It has moderate anti\u2010inflammatory and analgesic activities ( Spencer 1997 ). How the intervention might work Although OA is considered a non\u2010inflammatory disease, numerous studies have shown that inflammatory cytokines provide essential biochemical signals that simulate chondrocytes to release cartilage\u2010degrading enzymes. In addition, cytokines can be produced by synovial tissue cells and subchondral osteoblasts. IL\u20101beta and tumour necrosis factor (TNF)\u2010alfa are key cytokines in the catabolic process of cartilage ( Berembaum 2010 ). In vitro and in vivo studies have demonstrated that diacerein acts not only on cartilage but in all tissues involved in the pathogenesis of OA, including synoviocytes, the synovial membrane, subchondral bone and chondrocytes. Besides its inhibitory effects on IL\u20101, diacerein reduces other important mediators such as metalloproteinases, nitric oxide, ADAMTS (a disintegrin and metalloproteinase with thrombospondin motifs)\u20104 and ADAMTS\u20105 ( Pelletier 2010 ). Why it is important to do this review Currently, clinical management of OA typically entails a combination of treatment options to reduce pain and improve tolerance to functional activity. Existing pharmacological therapies for OA help to reduce symptoms but are only moderately effective and leave patients with substantial pain and functional burden ( Hunter 2011 ). Starting in 1982 ( Lingetti 1982 ), several trials tested diacerein for the treatment of OA, and since 1994, the drug has been marketed around the world, except in the United States of America. Based on the findings of several studies, it has been proposed that diacerein is a slow\u2010acting, symptom\u2010modifying and perhaps disease/structure\u2010modifying drug for OA. However, the importance of diacerein as an option for the treatment of OA needs to be clarified. Despite the long time elapsed since its discovery, published studies have not defined a clear place for the use of diacerein in the treatment of this disease as a symptom modifier or as a disease\u2010modifying agent that could retard the loss of cartilage. We performed a review of these studies to gather up\u2010to\u2010date evidence to clarify the role of diacerein in the treatment of OA.",
        "summary": "Diacerein is a slow\u2010acting anthroquinone whose therapeutic effect is mediated by blockade of interleukin\u20101 beta. In patients with hip or knee osteoarthritis diacerein does not seem effective at improving outcomes and has an increased risk of adverse events. The available randomized controlled trial evidence is limited by risk of bias, heterogeneity of study design and inconsistency in outcome reporting. Randomized controlled trials, including up to 500 participants, show that in patients with hip and knee osteoarthritis, diacerein has not been found to produce any better improvements in measures of pain, physical function and quality of life than either non\u2010steroidal anti\u2010inflammatory therapy (NSAID) or symptomatic slow\u2010acting drugs for osteoarthritis (SYSADOA, e.g. intra\u2010articular injections of hyaluronic acid). Compared with placebo, low\u2010quality evidence shows that diacerein use was associated with a lesser rate of progression of radiographic damage at the hip, but not the knee. However, this radiographic benefit was not associated with any improvement in simultaneous measures of pain (at > 6 months) or physical function. The use of diacerein is limited by a high frequency of gastrointestinal adverse effects. Low\u2010quality evidence shows that patients treated with diacerein had a substantially increased risk of developing diarrhea compared with either placebo, NSAIDs or SYSADOA and an increased risk of developing bowel motility disorders compared with NSAIDs. In 2014, the European Medicines Agency recommended that the use of diacerein be limited to patients with hip or knee osteoarthritis and that diacerein should not be used in patients aged over 65 or who have a history of liver disease. Furthermore, patients should stop diacerein immediately if diarrhea develops whilst on therapy."
    },
    "CD011129": {
        "query": "How does early palliative care compare with standard oncological care in adults with advanced cancer?",
        "document": "Background Research has led to remarkable improvements in cancer treatment, but at the time of diagnosis, some patients still have a reduced life expectancy. Incurable cancer can pose an enormous challenge for patients, their families, and medical professionals, and can affect patients' quality of life in many ways ( Addington\u2010Hall 1995 ). Interventions tailored to improve the physical and psychological well\u2010being of people with cancer are of utmost importance. Palliative care comprises an \"approach that improves the quality of life of patients and their families facing the problem associated with life\u2010threatening illness, through the prevention and relief of suffering by means of early identification and impeccable assessment and treatment of pain and other problems, physical, psychosocial and spiritual\" ( WHO 2013 ). Interdisciplinary care and caregiver support assist healthcare professionals in delivering the essential elements of palliative care by managing the patient's quality of life and controlling symptoms ( Hui 2013a ). However, although early access is inherent in the definition of palliative care, usual practice is still limited to the terminal phase of illness. Description of the condition With an incident rate of 14.9 million cases and 8.2 million deaths in 2013, malignant neoplastic diseases remain one of the leading causes of death worldwide ( Global Burden of Disease Cancer Collaboration 2015 ). Globally, the most common entities and causes of cancer\u2010related mortality, measured as disability\u2010adjusted life\u2010years (DALYs), are breast cancer in women and lung cancer in men. Cancer incidence has been estimated to increase yearly by 1%, with the growing population worldwide and the demographic shift towards an ageing population in developed countries serving as the paramount factors for future cancer burden ( Boyle 2008 ). Despite significant progress in our understanding of the risk factors for cancer, development of methods for early identification of some cancers or precancerous diseases, and sound advances in the treatment of many cancers previously deemed fatal (e.g. breast, prostate, melanoma, Hodgkin's disease), cancer continues to cause the premature death of many individuals (particularly cancers of the pancreas, lung, brain, and stomach) ( Prigerson 2015 ; Quaresma 2015 ). At the time of diagnosis, chances of curative treatment are often minimal owing to advanced disease. The American Cancer Society defines advanced cancer as \"cancers that cannot be cured\", and metastatic cancer as tumours that \"have usually spread from where they started to other parts of the body\" ( American Cancer Society 2013 ). However, not all advanced cancers are metastatic. For example, brain tumours may be considered advanced because they are not curable, and life\u2010threatening, even in the absence of metastasis. In addition, the survival rate of patients remains very poor, especially for metastatic lung cancer and for pancreatic and biliary tract malignancies. Because death is anticipated in many of these cases, it is essential that appropriate treatment plans are developed to improve survival, while aiming for a subjectively worthwhile quality of life. Both symptom control and disease\u2010modifying therapy are needed in these situations. By causing a major decline in physical efficiency and persistent chronic pain, advanced cancer regularly puts the physical and psychological integrity of patients at high risk. In many cases, appropriate execution of necessary medical treatments and of the daily routine at home demands continuous familial and often additional external support. Symptoms such as pain, fatigue/drowsiness, low appetite and/or anorexia\u2010cachexia syndrome, dysphagia, nausea, diarrhoea, constipation, shortness of breath, and mental confusion are often independent prognostic factors for predicting life expectancy in people with recently diagnosed incurable cancer ( Trajkovic\u2010Vidakovic 2012 ). In addition, patients and their caregivers may be concerned about burdensome existential ruminations leading to psychological distress on both sides, with long\u2010term risk of severe impairment in physical and psychological health among patients and caregivers, as well as declining resources of social support ( Mehnert 2014 ; Singer 1999 ; Sklenarova 2015 ). Such developments within the family often promote conflict about responsibilities regarding decision making concerning therapeutic and everyday challenges. Economic consequences frequently comprise, for example, reduced family income or considerable out\u2010of\u2010pocket medical spending, leading to financial hardship for patients and their families ( Elkin 2010 ; Zhang 2009 ). Owing to these strains, professional support gains extraordinary importance in alleviating physical discomfort and in contributing to improved quality of life among patients. Description of the intervention Palliative care is provided to reduce suffering and improve quality of life among patients and their caregivers. In recent years, the term 'early palliative care' was introduced to differentiate palliative care treatments applied early in the course of a life\u2010threatening disease from palliative care delivered mainly with high symptom burden or in the terminal phase of illness, as was the established clinical practice. In cases of advanced cancer, early palliative care is provided alongside active disease treatment such as chemotherapy or radiotherapy. A typical treatment protocol for investigators in early palliative care trials encompasses communication with the patient about illness and prognosis, symptom assessment and management, support for coping, and regular follow\u2010ups. According to the latest consensus definition of palliative care, such treatment is called 'early' when it is administered within eight weeks of diagnosis of advanced cancer ( Ferrell 2017 ). Other qualitatively identified elements include relationship and rapport building, development of coping skills, understanding of the illness, and discussion of available cancer treatments, including end\u2010of\u2010life planning ( Yoong 2013 ). A prerequisite for palliative care in such an early situation is readiness of health care professionals to engage in coherent and empathetic communication with the patient ( de Haes 2005 ; Dowsett 2000 ; Meyers 2003 ; Morrison 2004 ; Sinclair 2006 ). Early palliative care commonly is focussed on outlining realistic and attainable goals of treatment ( van Mechelen 2013 ) and facilitating patient choices by providing adequate information and assessment of patient values and preferences with regard to advance care planning ( Levy 2016 ). The inherent belief is that symptoms can be prevented or can be managed more easily when treated early, thereby improving the patient's quality of life. Most treatments involve education, evidence\u2010based methods used for symptom control, and psychosocial support. In essence, early palliative care is based on a proactive attitude and usually is provided to patients without high symptom burden or unmet psychosocial needs. Researchers have identified the following models of palliative care ( Hui 2015a ). Solo practice model: This model ascribes responsibility for cancer diagnosis and treatment as well as palliative care exclusively to the primary oncologist. Co\u2010ordinated care model: As is often observed in common clinical practice, the primary oncologist in collaboration with the primary nursing team offers and co\u2010ordinates supportive/palliative care. As part of this so\u2010called congress model, primary providers refer patients to various specialists, who address domains of palliative care (other physicians, clinical nurse specialists, social workers, chaplains, psychotherapists, and clinical psychologists or psychiatrists). Integrated care model: In this model, oncologists routinely refer patients to specialist palliative care teams early in the disease trajectory, rather than excluding involvement of other specialists. Solo practice model: This model ascribes responsibility for cancer diagnosis and treatment as well as palliative care exclusively to the primary oncologist. Co\u2010ordinated care model: As is often observed in common clinical practice, the primary oncologist in collaboration with the primary nursing team offers and co\u2010ordinates supportive/palliative care. As part of this so\u2010called congress model, primary providers refer patients to various specialists, who address domains of palliative care (other physicians, clinical nurse specialists, social workers, chaplains, psychotherapists, and clinical psychologists or psychiatrists). Integrated care model: In this model, oncologists routinely refer patients to specialist palliative care teams early in the disease trajectory, rather than excluding involvement of other specialists. Regardless of the model selected, early palliative care can be delivered across a breadth of settings, including community centres, hospitals, and inpatient hospice units. Community hospice services may also support patients at an earlier stage of disease in the day care/outpatient setting. Comparator arms in early palliative care trials generally consist of usual oncology care. This may include referral to or application of palliative measures at any time along the disease trajectory as initiated by an oncologist, patient, or family member. However, referral to or application of palliative measures are not usually offered actively to all patients. How the intervention might work With a focus on intensified doctor\u2010patient communication, early palliative care may lead to higher levels of social support and may increase the likelihood of acceptance of the diagnosis and illness severity. These effects, along with the augmented satisfaction of the patient\u2010physician relationship, may improve the patient's openness to symptom control and psychosocial interventions, thereby reducing distress. Reduced distress itself is associated with improved quality of life and is consistently associated with survival ( Gotay 2008 ; Irwin 2013 ; Pinquart 2010 ). Furthermore, patients and family members undergoing early palliative care are better informed about treatment directives and end\u2010of\u2010life decisions, which promotes higher self\u2010efficacy and a greater sense of control of decisions with respect to a person's individual values ( McClain 2003 ). On the one hand, better symptom control and psychosocial function could promote better adherence with reasonable treatment plans. On the other hand, palliative care is linked to less aggressive cancer treatment, such as reduced use of questionable chemotherapy and less treatment time in intensive care units ( Earle 2008 ). This tendency to de\u2010escalate treatment intensity in final, irreversible health conditions, together with extension of outpatient and community palliative care services, is important for patients' well\u2010being as well as to socioeconomics ( Lowery 2013 ; Smith 2003 ). Why it is important to do this review Evidence for the effects of late palliative care is ambiguous because the time required to establish beneficial effects may be too short ( El\u2010Jawahri 2011 ; Gomes 2013 ; Higginson 2010 ; Zimmermann 2008 ). Palliative interventions applied early, around the time of diagnosis of incurable advanced cancer, may be more favourable for improving symptom and disease management ( Levy 2016 ), leading some investigators to believe that a paradigm shift has occurred (e.g. Kamal 2016 ; Kelley 2010 ; Schenker 2015 ). To date, although several reviews on early palliative care interventions for patients with advanced cancer have been published ( Bauman 2014 ; Davis 2015 ; El\u2010Jawahri 2011 ; Gomes 2013 ; Greer 2013 ; Higginson 2010 ; Hui 2015b ; Parikh 2013 ; Salins 2016 ; Smith 2012 ; Tassinari 2016 ; von Roenn 2011 ; Zambrano 2016 ; Zhi 2015 ; Zimmermann 2008 ), to our knowledge, no meta\u2010analysis has been carried out. An overview of interventions applied within this framework has not been provided, and uncertainty remains about the general impact of such interventions on patient\u2010 and caregiver\u2010related outcomes.",
        "summary": "Early palliative care appears to compare favorably with standard oncological care in people with advanced cancer. In fact, early palliation may be associated with slightly to moderately lower symptom intensity and better health\u2010related quality of life, with no apparent impact on overall survival or depression. Data on adverse effects are scarce and inconclusive. However, as all evidence is of very low to low quality, no conclusions can be drawn."
    },
    "CD000060": {
        "query": "In children with acute asthma, what are the effects of combined inhaled anticholinergics and short\u2010acting beta2\u2010agonists as initial treatment?",
        "document": "Background Description of the condition Asthma is caused by inflammation in the airways and bronchoconstriction, which makes it difficult to breathe and leads to wheezing and breathlessness. The underlying inflammation causes the lining of the airways to secrete mucus, which also obstructs the free flow of air through the lungs. The bronchoconstriction is alleviated by using bronchodilators such as short acting beta 2 \u2010agonists (SABAs), while the underlying inflammation can be treated with regular inhaled corticosteroids. However, symptoms may flare up in response to an asthma trigger (e.g. virus, dust, pollen) leading to an asthma exacerbation. It is not known why the inflammation, secretions and bronchoconstriction occur. Asthma exacerbations can be life threatening and may require more medications than are used on a day\u2010to\u2010day basis. Description of the intervention The initial management of acute paediatric asthma exacerbations in children focuses on the rapid relief of bronchospasm using inhaled or nebulized bronchodilators ( BTS 2011 ; GINA 2011 ). Children who do have moderate or severe asthma and those who do not respond sufficiently to bronchodilators to relieve symptoms require the addition of oral or intravenous glucocorticoids ( BTS 2011 ; GINA 2011 ; Lougheed 2012 ). SABAs are clearly the most effective bronchodilators due to their rapid onset of action and the magnitude of achieved bronchodilation ( Sears 1992 ; Svedmyr 1985 ; Teoh 2012 ). Anticholinergic agents, such as ipratropium bromide and atropine sulfate, have a slower onset of action and weaker bronchodilating effect, but may specifically relieve cholinergic bronchomotor tone and decrease mucosal edema and secretions ( Chapman 1996 ; Gross 1988 ; Silverman 1990 ). Thus, the combination of inhaled anticholinergics with SABAs may yield enhanced and prolonged bronchodilation. National and international guidelines recommend the addition of anticholinergics to inhaled SABAs in people with an acute severe asthma exacerbation and suggest consideration of this therapy in people with moderate exacerbations ( BTS 2011 ; GINA 2011 ). Why it is important to do this review The first version of this Cochrane review published in 1997 found a significant reduction in hospital admissions in school\u2010aged children with severe exacerbations receiving intensive anticholinergic treatment. The review was updated in 2000 with the addition of three new trials further strengthening the initial conclusions ( Plotnick 2000 ). With the identification of seven new studies, the 2012 update aims to examine the conclusions of the earlier version of this review and explore if characteristics of participants or treatment are associated with increased benefit.",
        "summary": "Combined multiple fixed\u2010dose inhaled anticholinergics and short\u2010acting beta2\u2010agonists (SABAs) are more effective than SABAs for the initial treatment of children with moderate to severe asthma exacerbation. In children and adolescents (18 months to 18 years) presenting to an emergency department with acute exacerbation of asthma, there is high\u2010quality evidence that combined inhaled anticholinergics plus SABAs decrease hospital admissions compared with SABAs alone. Effects were consistent in children who also received systemic corticosteroids and in children with moderate to severe asthma exacerbation, although no effect was seen in children with mild asthma exacerbation. Children given combined inhaled anticholinergics and SABAs also improved their FEV 1 and clinical score, with fewer adverse effects (decreased tremor and nausea, but no difference in vomiting), compared with children given SABAs alone. Optimal dosing and duration of combination therapy has not been studied; most trials used a multiple fixed\u2010dose protocol of inhaled anticholinergics (e.g. 3 doses of 250 mcg or 2 doses of 500 mcg of ipratropium bromide nebulizer over 60\u201090 minutes) plus SABAs."
    },
    "CD009672": {
        "query": "What are the effects of hormone therapy on sexual function in postmenopausal women who may or may not be having menopausal symptoms?",
        "document": "Background Description of the condition Menopause is the permanent end of menstruation and fertility but, even before the true onset of menopause, women may experience menopausal symptoms and changes in their menstrual cycle in a period called perimenopause ( NAMS 2012 ). Natural menopause is defined as occurring after 12 consecutive months of spontaneous amenorrhoea with no obvious pathologic cause. Induced menopause is defined as the permanent cessation of menstruation after either surgical removal of the ovaries or iatrogenic ablation of ovarian function (for example chemotherapy or pelvic radiation therapy) ( NAMS 2012 ). The most common symptoms associated with menopause are hot flushes, night sweats, sleep disturbance, vaginal atrophy, and dyspareunia ( NAMS 2012 ). In order to alleviate these symptoms, some women start using menopausal hormone therapy (HT) ( NAMS 2012 ; Santen 2010 ). According to the Diagnostic and Statistical Manual of Mental Disorders, DSM IV ( APA 2000 ), sexual dysfunction is defined by disturbances in sexual desire and by psychophysiological changes that characterize the sexual response cycle, causing marked distress and interpersonal difficulty. Sexual functioning is of great importance for quality of life, as approximately 75% of middle\u2010aged American women consider sexual activity as being of moderate to extreme importance ( Cain 2003 ). Despite its importance, female sexual function is not easy to define or investigate because it depends on several factors such as health and well\u2010being, cultural habits, socioeconomic status, relationship issues, and existence and health of the partner ( Davis 2009 ). Female sexual dysfunction might be evaluated in different domains, including sexual interest and arousal, orgasm and pain ( Binik 2010 ). Although sexual function declines throughout the menopause transition ( NAMS 2012 ; Rosen 2011 ), it is unclear whether this is caused by the low estrogen levels, aging, or both ( da Silva Lara 2009 ; Nappi 2009a ). Description of the intervention HT is a generic term that refers to any type of hormone therapy used during menopause for alleviation of menopause\u2010related symptoms ( Santen 2010 ). The use of HT has been studied in various settings. Clinical benefit has been observed when HT is used for the relief of hot flushes and night sweats ( Formoso 2012 ; MacLennan 2004 ) and for the relief of vaginal atrophy symptoms ( Suckling 2006 ). The most commonly used types of HT are estrogens alone, estrogens in combination with progestogens, synthetic steroids (for example tibolone), and selective estrogen receptor modulators (SERMs) (for example raloxifene). Other substances, such as androgens and phytoestrogens, might be used for the relief of some menopausal\u2010related symptoms; however, we will not evaluate the effect of these drugs in this review because they are not widely accepted as HT. How the intervention might work Women with night sweats may find that their subsequent poor sleep patterns lead to tiredness and loss of energy which in addition to vaginal symptoms may affect women\u2019s self esteem, self confidence and sexuality( Graziottin 2005 ; McKinley 2008 ). A proportion of peri and postmenopausal women might, at least in theory, benefit from HT in some aspects of this complex net of symptoms since HT improves vulvovaginal atrophy and vasomotor symptoms ( NAMS 2012 ; Santen 2010 ). Why it is important to do this review Sexual function is considered to be a very important factor in women\u2019s health, and medical counselling about HT should balance the benefits and risks ( NAMS 2012 ; Santen 2010 ). Information regarding how HT can influence sexual function is important when considering the use of these drugs ( Wierman 2010 ). The true effect of HT on sexual function is difficult to comprehend based on the literature published so far; partially because of the different drugs and doses, different tools to evaluate sexual function, and the particular population studied in each trial. That highlights the need for a systematic review of the best available evidence to facilitate evidence\u2010based decisions.",
        "summary": "The randomized controlled trial evidence for the effects of hormone therapy on sexual function in postmenopausal women, some of whom were experiencing menopausal symptoms, was of low quality, and where benefits were observed, these were small and unlikely to be clinically meaningful. Low\u2010quality evidence suggests that postmenopausal women (up to 60 to 65 years old) with amenorrhea for at least 12 months had better sexual function scores, higher libido/desire/interest in sex, greater arousal, and greater satisfaction at up to 6 months\u2019 follow\u2010up with 2.5mg/day tibolone compared with placebo/no treatment. The improvements seen were small. There were no apparent differences between groups in dyspareunia or number of orgasms. Frequency of sexual activity, pleasure/enjoyment in sex, and adverse effects were not evaluated. Low\u2010quality evidence suggested no apparent differences between groups in sexual function score when raloxifene (60 to 120 mg/day) was compared with placebo/no treatment in a heterogeneous population of postmenopausal women (31 to 80 years) with amenorrhea for at least two years and osteoporosis. There were also no evidence of a difference detected between 60 to 120 mg/day raloxifene and placebo/no treatment in frequency of sexual activity, libido/desire/interest in sex, arousal, number of orgasms, Pleasure/enjoyment of sex, or dyspareunia; quality assessments were not conducted, but both trials had methodological flaws. Satisfaction and adverse effects were not assessed. In postmenopausal women (aged 50 to 69 years) with amenorrhea, dyspareunia (Rosen\u2019s female sexual function index) was less with estrogen plus progestogens (regimens varied) at 6 months (on average by 0.9 points on the 36\u2010point score; 1 RCT, 104 women) but not 12 months (2 RCTs, 1323 women), compared with placebo/no treatment. The 6\u2010month analysis had very small numbers of participants, and even though a statistically significant difference was detected, the difference was not clinically important. There were no apparent differences between groups in sexual function score, libido/desire/interest, arousal, number of orgasms, or satisfaction. Frequency of sexual activity, pleasure/enjoyment in sex, and adverse effects were not evaluated. No trials compared selective estrogen receptor modulators (SERMs) plus estrogens with placebo or no treatment."
    },
    "CD009796": {
        "query": "In people with fibromyalgia, what are the effects of cognitive behavioral therapies?",
        "document": "Background Description of the condition The key symptoms of fibromyalgia (FM) are chronic widespread pain associated with cognitive dysfunction, physical fatigue and sleep disturbances ( H\u00e4user 2008 ; Wolfe 2010 ). Patients often report high disability levels and poor quality of life along with extensive use of medical care ( Winkelmann 2011 ; Wolfe 1997 ). Lacking a specific laboratory test, diagnosis is established by a history of the key symptoms and the exclusion of somatic diseases sufficiently explaining the key symptoms ( H\u00e4user 2008 ; Wolfe 2010 ). For a clinical diagnosis the 1990 and 2010 American College of Rheumatology (ACR) criteria ( Wolfe 1990 ; Wolfe 2010 ) and the Association of the Medical Scientific Societies in Germany (AWMF) diagnostic criteria ( H\u00e4user 2010 ) can be used. In the past other standardised criteria have been used to diagnose FM ( Smythe 1981 ; Yunus 1981 ). FM is estimated to affect 1% to 2% of people in the United States ( Lawrence 2008 ) and 2.1% to 2.9% in Europe ( Branco 2010 ; Wolfe 2013 ). The definite aetiology (causes) of this syndrome remains unknown. A model of interacting biological and psychosocial variables in the predisposition, triggering and development of the chronicity of FM has been suggested ( Sommer 2012a ). Depression ( Forseth 1999 ), genes (e.g. 5\u2010hydroxytryptamine 2A receptor 102T/C polymorphism) ( Lee 2012 ), obesity combined with physical inactivity ( Mork 2010 ), physical and sexual abuse in childhood ( H\u00e4user 2011 ), sleep problems ( Mork 2012 ) and smoking ( Choi 2010 ) predict future development of FM. Depression and post\u2010traumatic stress disorder worsen FM symptoms ( Dell' Osso 2012 ; Lange 2010 ). Several factors are associated with the pathophysiology (functional changes associated with or resulting from disease) of FM, but the relationship is unclear. The functional changes include alteration of pain processing in the brain, reduced reactivity of the hypothalamus\u2010pituitary\u2010adrenal axis to stress, increased pro\u2010inflammatory and reduced anti\u2010inflammatory cytokine profiles (produced by cells involved in inflammation), and disturbances in neurotransmitters such as dopamine and serotonin ( Sommer 2012a ). Prolonged exposure to stress, as outlined above, may contribute to these functional changes ( Bradley 2009 ). Current treatments for FM are not curative. Drugs ( H\u00e4user 2013 ; Moore 2011 ; Tort 2012 ) and exercise therapies ( Busch 2007 ) aim to relieve symptoms and improve quality of life and functional abilities. Description of the intervention Behavioural and cognitive behavioural psychological therapies are the dominant contemporary psychological treatments for a wide range of health problems, including chronic pain ( Morley 2011 ). Behavioural and cognitive behavioural psychological therapies are used to manage chronic pain by attempting to change negative thoughts about pain, and introduce behaviour modification, including self\u2010management techniques, to improve function and cope with pain. However, there is no universally accepted definition of which techniques constitute behavioural and cognitive behavioural psychological therapies ( Morley 2011 ). Due to the broad variety of behavioural and cognitive behavioural psychological therapy techniques, we use in the following context the term 'cognitive behavioural therapies' (CBTs). For the purposes of this review we will consider the following techniques ( Jensen 2011 ). Operant therapy, which requires techniques to increase activity, the inclusion of significant others to reduce reinforcement of pain behaviours, and the reduction of pain\u2010contingent medication ( Fordyce 1976 ; Thieme 2003 ). Traditional cognitive behavioural therapy (CBT), which requires monitoring of one's own thoughts, feelings and behaviours with respect to the target symptom (e.g. by a symptom diary) and the promotion of alternative ways of coping with the target symptom (also labelled as problem\u2010solving techniques, self management, coping skills), through methods such as activity participation and skill\u2010building or practice opportunities ( Bennett 2006 ). Self management education programmes, which require information on the clinical picture of FMS, cognitive and behavioural skills mastery to manage pain and limitations of daily activities, and modelling as supplied by the facilitators to target cognitive, behavioral and emotional change ( Burckhardt 2005b ; Warsi 2003 ). Acceptance\u2010based CBTs, which include acceptance and commitment therapy, or contextual CBT or mindfulness\u2010based cognitive therapy. All these therapies use acceptance techniques (e.g. mindfulness meditation training) to facilitate a separation between 'self' and one\u2019s thoughts, feelings and pain experience, and encourage patients to base their actions on their most important values as opposed to their immediate feelings, thoughts and pain ( Veehof 2011 ). Operant therapy, which requires techniques to increase activity, the inclusion of significant others to reduce reinforcement of pain behaviours, and the reduction of pain\u2010contingent medication ( Fordyce 1976 ; Thieme 2003 ). Traditional cognitive behavioural therapy (CBT), which requires monitoring of one's own thoughts, feelings and behaviours with respect to the target symptom (e.g. by a symptom diary) and the promotion of alternative ways of coping with the target symptom (also labelled as problem\u2010solving techniques, self management, coping skills), through methods such as activity participation and skill\u2010building or practice opportunities ( Bennett 2006 ). Self management education programmes, which require information on the clinical picture of FMS, cognitive and behavioural skills mastery to manage pain and limitations of daily activities, and modelling as supplied by the facilitators to target cognitive, behavioral and emotional change ( Burckhardt 2005b ; Warsi 2003 ). Acceptance\u2010based CBTs, which include acceptance and commitment therapy, or contextual CBT or mindfulness\u2010based cognitive therapy. All these therapies use acceptance techniques (e.g. mindfulness meditation training) to facilitate a separation between 'self' and one\u2019s thoughts, feelings and pain experience, and encourage patients to base their actions on their most important values as opposed to their immediate feelings, thoughts and pain ( Veehof 2011 ). How the intervention might work CBTs include interventions that are based on the premise that chronic pain and other symptoms of FM are maintained and influenced by emotional and cognitive (conscious intellectual activities such as thinking, reasoning or remembering) as well as behavioural factors. A typical treatment protocol for traditional CBT will involve methods aimed directly at assessing the thoughts associated with pain, the extent of avoidance of unpleasant thoughts and of painful experiences, and the consequences of these. A common focus is on strongly held beliefs about pain and their relationship with behaviour, which typically worsens the situation in the shorter or longer term. Behavioural methods focus on the identification of behaviour that is contingent on pain, or upon events which provide pain relief or comfort, and the development of behaviour that is contingent instead on goal achievement related to the values of the individual with pain ( Bennett 2006 ; Williams 2012 ). Most CBTs include education (information on the etiology of the disease including importance of psychological factors; treatment options; working mechanisms of psychological and drug therapies). Why it is important to do this review The significance of CBTs in the management of FM still needs to be determined. Systematic narrative and quantitative reviews on CBTs in FM have had divergent results. Koulil ( Koulil 2007 ) concluded from six randomized controlled trials (RCTs) that the effects on pain, disability and mood were limited, and that it was mostly CBTs within a multi\u2010component approach that yielded improvements. Bennett concluded from six RCTs that CBT as a single treatment modality did not offer any distinct advantage over well\u2010planned group programmes of education or exercise, or both ( Bennett 2006 ). Thieme and coworkers concluded from 14 studies that CBTs were superior to controls in most key domains of FMS post\u2010treatment and at follow\u2010up ( Thieme 2009 ). A recent Cochrane review on the efficacy of psychological therapies in chronic pain syndromes included only six studies with FM patients and did not present a subgroup analysis of FM patients ( Williams 2012 ). Another recent review on psychological therapies in FM concluded that CBTs were effective in relieving FM symptoms and superior to other psychological therapies. However, this review included a combination of CBTs with aerobic exercise (multi\u2010component therapies) and did not compare the results of CBTs with control groups ( Glombiewski 2010 ). A meta\u2010analysis on CBTs in FM found that CBTs were superior to controls in reducing depressed mood post\u2010treatment but not superior in reducing pain, fatigue, sleep and limitations in quality of life post\u2010treatment and at follow\u2010up. This systematic review included trials with mindfulness\u2010based stress reduction (MBSR) and excluded trials with self management approaches ( Bernardy 2010 ). Evidence\u2010based guidelines on the management of FM have given different grades of recommendation for CBTs. The American Pain Society ( Burckhardt 2005a ) gave the highest grade of recommendation to CBTs based on a qualitative systematic review. The European League Against Rheumatism only gave a weak (expert opinion) recommendation for CBTs based on a quantitative systematic review ( Carville 2008 ). The Association of the Scientific Medical Societies in Germany gave an open recommendation based on a quantitative systematic review ( K\u00f6llner 2012 ). The Canadian Pain Society gave a strong recommendation of CBTs based on a quantitative systematic review ( Fitzcharles 2012 ).",
        "summary": "Low\u2010quality evidence suggested that traditional CBT improved pain, negative mood, and fatigue and reduced disability, compared with a range of other interventions at both the end of a median ten weeks of treatment and after 6 months follow\u2010up in patients with fibromyalgia (diagnosed by 1990 ACR criteria).The population was predominately women with long\u2010standing (over 5 years) disease. Quality of life was improved at the end of treatment. There was insufficient data to assess sleep problems or longer term effects on quality of life. Withdrawals for any reason during the treatment phase were similar in both groups. Subgroup analyses assessing different types of CBT (e.g. telephone interview) separately tended to be too small to detect clinically meaningful differences even if present, making it difficult to draw conclusions."
    },
    "CD011474": {
        "query": "How does single\u2010dose ibuprofen 400 mg compare with placebo for treatment of adults with episodic tension\u2010type headache?",
        "document": "Background Headaches are a commonly reported problem in community\u2010based surveys worldwide. The lifetime prevalence of headache is estimated to be greater than 90% ( Steiner 2004 ), and the annual prevalence rate is estimated to be 46% in the general adult population ( Stovner 2007 ). Variations in reported prevalence may result from differences in study design, population, inclusion or exclusion of cases of infrequent episodic tension\u2010type headache (TTH), overlap with probable migraine, cultural and environmental differences, or even genetic factors ( Sahler 2012 ). TTH is more common than migraine, a finding replicated across the world ( Oshinaike 2014 ; Vos 2012 ). The management of people with headaches is, however, largely neglected, and may be fragmented by the involvement of clinicians from different medical specialities (neurology; ear, nose and throat; ophthalmology; psychiatry) ( Rasmussen 2001 ). Because headache is rarely life\u2010threatening and headache pain is generally mild to moderate in intensity, people often self medicate and do not seek formal care from health services ( Rasmussen 2001 ). Headache can be either primary or secondary (due to other systemic or local causes) ( Green 2009 ). TTH belongs to the group of primary headaches and is seen in nearly one\u2010third of those suffering from headaches; the large number of people affected imposes a significant burden on the healthcare system ( Stovner 2007 ). Generally, episodes of TTH are mild to moderate in intensity and self limiting, but in a small group of people they may be more severe and disabling ( Green 2009 ). People with longer\u2010lasting or more severe headaches may seek help in a clinical setting, but the majority of people do not do so, resulting in inadequate and inappropriate management ( Kernick 2008 ). In a community\u2010based telephone survey to determine the medication patterns of 274 frequent headache sufferers, only 1% used prescription medication. The majority of people with headaches reported using over\u2010the\u2010counter (OTC) analgesics (56% using paracetamol (acetaminophen) and 15% aspirin), and the perceived effectiveness of OTC medication was approximately 7 on a scale of 0 to 10 ( Forward 1998 ). Strategies for the management of TTH used to be extrapolated from those applied for migraine. The World Health Organization (WHO) essential drug list, for example, does not include indications for the management of TTH ( WHO 2007 ). In 2010, both the British Association for the Study of Headache (BASH) and the European Federation of Neurological Societies (EFNS) updated or published guidelines for the management of TTH ( BASH 2010 ; Bendtsen 2010 ); there is also German and Austrian guidance ( Haag 2011 ). The guidelines reflect ongoing systematic efforts to bridge the gap between clinical trial evidence and clinical practice with the aim of improving practice. People with TTH have more work absence than people with migraine or people without headaches ( Lyngberg 2005 ); there is also considerable loss of productivity ( Cristofolini 2008 ; Pop 2002 ). Headache\u2010related characteristics include significant problems with headache management, disability, pain, worry and dissatisfaction with care, as well as greater use of medical services and worse general health ( Harpole 2005 ). Description of the condition Tension\u2010type headache (TTH) is known by several names, including tension headache, muscle contraction headache, psychomyogenic headache, stress headache, ordinary headache, essential headache, idiopathic headache, and psychogenic headache ( IHS 2013 ). TTH is diagnosed mainly by the absence of features found in other headache types, especially migraine. The third edition of the International Classification of Headache Disorders (ICHD\u20103 beta) separates TTH into episodic and chronic varieties ( IHS 2013 ). Chronic TTH is diagnosed when headache occurs on 15 days or more per month on average, for 3 months or more (180 or more days per year); otherwise TTH is considered to be episodic. Acute treatment with analgesics is more appropriate for episodic TTH, while both pharmacological and non\u2010pharmacological treatments are used for managing chronic TTH. Structural changes in the brain have been reported in chronic TTH ( Schmidt\u2010Wilcke 2005 ). Further, management of TTH in children and adolescents poses a clinically diverse situation (establishing diagnoses, dosages, nature of preparation, pharmacodynamics) ( Monteith 2010 ). For all these reasons, the proposed review will focus on the acute treatment of episodic TTH in adults. Episodic TTH is subdivided into infrequent and frequent types ( IHS 2013 ). Infrequent episodic TTH is defined by the following criteria. At least 10 episodes occurring on less than 1 day per month (fewer than 12 days per year) and satisfying criteria 2 through 4. Headache lasting from 30 minutes to 7 days. Headache has at least two of the following characteristics. bilateral location. pressing or tightening (non\u2010pulsating) quality. mild or moderate intensity. not aggravated by routine physical activity such as walking or climbing stairs. Both of the following. no nausea or vomiting (anorexia may occur). no more than one of photophobia or phonophobia. Not better accounted for by another International Classification of Headache Disorders (ICHD\u20103 beta) diagnosis. At least 10 episodes occurring on less than 1 day per month (fewer than 12 days per year) and satisfying criteria 2 through 4. Headache lasting from 30 minutes to 7 days. Headache has at least two of the following characteristics. bilateral location. pressing or tightening (non\u2010pulsating) quality. mild or moderate intensity. not aggravated by routine physical activity such as walking or climbing stairs. Both of the following. no nausea or vomiting (anorexia may occur). no more than one of photophobia or phonophobia. Not better accounted for by another International Classification of Headache Disorders (ICHD\u20103 beta) diagnosis. Frequent episodic TTH is defined as at least 10 episodes of headache occurring on at least 1 day but fewer than 15 days per month for at least three months (at least 12 and fewer than 180 days per year), and when criteria 2 through 5, above, are also met. The one\u2010year prevalence of infrequent episodic TTH in a Danish study of 4000 people aged 40 years was 48%, while that for frequent episodic TTH was 34% ( Russell 2005 ). The overall annual prevalence of TTH in the United States was estimated to be 38%, with a higher incidence among women (prevalence ratio of 1.2 ( Schwartz 1998 )). In Canada, the prevalence was 29% ( Edmeads 1993 ). A study conducted in Chile reported that TTH constituted 72% of all recurrent headaches, with a prevalence of 27% (95% confidence interval (CI), 25 to 29). Nearly one\u2010quarter (24%) had episodic TTH, and the prevalence was greater amongst women than men (35% versus 18%) ( Lavados 1998 ). The global prevalence of TTH was given as 21% in the Global Burden of Diseases Study 2010, making it the second most prevalent condition after dental caries, and slightly more prevalent than migraine ( Vos 2012 ). The exact pathogenesis of TTH is still unknown and is said to be multifactorial, including central dysfunction of pain\u2010processing pathways and peripheral myofascial factors. There is a general agreement that peripheral myofascial nociception disturbances have a role in the pathogenesis of both frequent and infrequent episodic TTH ( Fern\u00e1ndez\u2010de\u2010Las\u2010Pe\u00f1as 2010 ; Fumal 2008 ). Description of the intervention Ibuprofen is a propionic acid derivative with analgesic, anti\u2010inflammatory and antipyretic properties. Ibuprofen was developed in the 1960s and is used extensively throughout the world for relief of pain and inflammation in both acute and chronic conditions. It is available OTC in most countries, usually as 200 mg tablets, with 1200 mg as the recommended maximum daily dose for adults. Under medical supervision, up to 3200 mg daily may be taken in divided doses, though the usual prescribed dose is up to 2400 mg. Ibuprofen salts (arginine, lysine, sodium) and some other fast\u2010acting formulations are known to be more effective than standard acid formulations in treating acute postoperative pain, largely because of earlier absorption and higher plasma concentrations ( Moore 2014a , Moore 2015 ). Ibuprofen has been widely used in treating arthritis, dental pain, menstrual cramps, and a variety of other acute pain conditions; the usual recommended adult dose for acute pain is 400 mg up to three times daily. OTC medications are less expensive, more accessible, and have favourable safety profiles relative to many prescription treatments. Ibuprofen is effective for migraine headache treatment ( Rabbie 2013 ), and the success rate with ibuprofen 400 mg in obtaining a headache response (no worse than mild pain at two hours) in treating acute migraine is similar to that found for some triptans ( Moore 2013a ). How the intervention might work Clinicians prescribe nonsteroidal anti\u2010inflammatory drugs (NSAIDs) on a routine basis for mild\u2010to\u2010moderate pain in a range of conditions. NSAIDs are the most commonly prescribed analgesic medications worldwide, and their efficacy for treating acute pain has been well demonstrated ( Moore 2011 ). They reversibly inhibit the activity of cyclooxygenase (prostaglandin endoperoxide synthase) (COX), now recognised to consist of two isoforms (COX\u20101 and COX\u20102), the enzyme mediating production of prostaglandins and thromboxane A2 ( FitzGerald 2001 ). Prostaglandins mediate a variety of physiological functions such as maintenance of the gastric mucosal barrier, regulation of renal blood flow, and regulation of endothelial tone. They also play an important role in inflammatory and nociceptive processes. However, relatively little is known about the mechanism of action of this class of compounds aside from their ability to inhibit cyclooxygenase\u2010dependent prostanoid formation ( Hawkey 1999 ). Fast\u2010acting ibuprofen and diclofenac formulations used for a short time (five days) result in fewer people with erosive gastro\u2010duodenal lesions than aspirin ( Hawkey 2011 ). In single\u2010dose studies in acute postoperative pain and migraine, adverse events are no more common with ibuprofen than with placebo ( Derry 2009 ; Rabbie 2013 ). Ibuprofen inhibits both COX isoforms, and suppression of prostaglandin synthesis is believed to underlie the analgesic effects of ibuprofen. Why it is important to do this review Episodic TTH is ubiquitous, affecting a large proportion of adults. Despite being generally mild to moderate in intensity, headache results in considerable suffering to the affected individual and contributes overall to a significant loss of productivity to society ( Mannix 2001 ; Rasmussen 2001 ; Steiner 2004 ; Stovner 2007 ). Seeking relief, people generally self\u2010medicate with one or more medicines, and OTC medicines are often used ( Forward 1998 ). Ibuprofen is a readily accessible OTC analgesic. As a generic drug, ibuprofen could be the drug of choice or the first\u2010line drug for management of TTH, particularly in low\u2010resource settings. It has been shown to work in individual studies ( Schachtel 1996 ). Two recently published guidelines on the management of TTH have reviewed the effectiveness of treatment modalities. Both adopted a consensus methodology. The BASH guidelines are based on a limited review of studies ( BASH 2010 ). The EFNS guidelines are based on a more detailed and thorough search of the literature ( Bendtsen 2010 ). Moreover, the EFNS guidelines represent an improvement over the BASH guidelines in that they used a standard published protocol for developing management guidelines ( Brainin 2004 ). That protocol strongly recommends active and frequent consultation of The Cochrane Library . However, there are no published Cochrane reviews on the management of acute episodic TTH. A non\u2010Cochrane systematic review by Verhagen and others followed methods similar to those used in Cochrane reviews and evaluated the efficacy and tolerability of analgesics for treatment of acute episodes of TTH in adults, but it analysed the non\u2010standard measure \"pain relief or recovery over 2 to 6 hours\" as the main efficacy outcome ( Verhagen 2006 ). Reviews explicitly adopting Cochrane methods and evaluating the more focused outcomes recommended in the IHS's recently updated guidelines for controlled trials of drugs in TTH are clearly important ( IHS 2010 ). A survey of TTH study methods and reporting demonstrates that these are seldom adhered to in clinical trials, but does report a variety of outcomes, including IHS\u2010preferred outcomes, for aspirin, ibuprofen, ketoprofen and paracetamol ( Moore 2014b ).",
        "summary": "Ibuprofen is more effective than placebo for reducing pain in adults with episodic tension\u2010type headache, although it should be noted that response rates were comparatively low. Moderate\u2010certainty evidence shows that more patients will be pain\u2010free at two hours with ibuprofen than with placebo (on average, 179 vs 118 per 1000 people). Little or no difference was noted between groups in the proportion of people who were pain\u2010free at one hour, although the analysis may have been underpowered. For the outcome of \u201cpatient global evaluation,\u201d moderate\u2010certainty evidence favors ibuprofen over placebo (on average, 400 vs 230 people rated treatment as \u201dvery good\u201d or \u201dexcellent\u201d). Low\u2010certainty evidence suggests that fewer people used rescue medications over six hours with ibuprofen than with placebo (on average, 175 vs 294 per 1000 people). Adverse events were rare in both groups, and there was little or no difference between groups (high\u2010certainty evidence)."
    },
    "CD003452": {
        "query": "Does randomized controlled trial evidence support the use of orthodontic treatment in children with prominent upper front teeth (class II malocclusion)?",
        "document": "Background Description of the condition Orthodontics is the branch of dentistry concerned with the growth of the jaws and face, the development of the teeth and the way the teeth and jaws bite together. It also involves treatment of the teeth and jaws when they are irregular or bite in an abnormal way or both. There are many reasons why the teeth may not bite together correctly. These include the position of the teeth, jaws, lips, tongue, or cheeks; or may be due to heredity, a habit or the way people breathe. The need for orthodontic treatment can be decided by looking at the effect any particular tooth position has on the life expectancy of the teeth, or the effect that the appearance of the teeth has on how people feel about themselves, or both ( Shaw 1991 ). Prominent upper front teeth (Class II malocclusion) may be due to any combination of the jaw, tooth and lip position. The upper jaw (maxilla) can be too far forward or, more usually, the lower jaw (mandible) is too far back. The upper front teeth (incisors) may stick out if the lower lip catches behind them or due to a habit (e.g. thumb sucking). This gives the patient an appearance that may be a target for teasing ( Shaw 1980 ) and bullying ( Seehra 2011 ), which impacts on quality of life ( Johal 2007 ; Silva 2016 ). When front teeth stick out (more than 3 mm to 5 mm), they are two to three times more likely to be injured ( Frujeri 2014 ; Nguyen 1999 ). Prominent upper front teeth (Class II malocclusion) is one of the most common problems seen by orthodontists and affects about a quarter of 12\u2010year\u2010old children in the UK ( Holmes 1992 ). However, there are racial differences: prominent upper front teeth (Class II malocclusion) are most common in whites of Northern European origin and least common in black and oriental races and some Scandinavian populations ( El\u2010Mangoury 1990 ; Proffit 1993 ; Silva 2001 ). Description of the intervention Several dental brace (orthodontic) treatments have been suggested to correct prominent upper front teeth (Class II malocclusions). Some treatments aim to move the upper front teeth backwards (with or without the extraction of teeth) whilst others aim to modify the growth of the upper or lower jaw or both to reduce the prominence of the upper front teeth. Treatment can involve the use of one or more types of orthodontic brace. How the intervention might work Some braces apply a force directly to the teeth and can either be removed from the mouth or fixed to the teeth, with special glue, during treatment. Other types of brace are attached, via the teeth, to devices (headgear) that allow a force to be applied to the teeth and jaws from the back of the head. Treatment is usually carried out either early (early treatment), when a mixture of baby and adult teeth are present (around seven to 11 years of age) or later (adolescent treatment) when all the adult teeth have come into the mouth (around 12 to 16 years of age). In severe cases and some adults, orthodontic treatment may need to be combined with jaw surgery to correct the position of one or both jaws. Why it is important to do this review Cochrane Oral Health undertook an extensive prioritisation exercise in 2014 to identify a core portfolio of titles that were the most clinically important ones to maintain in the Cochrane Library ( Worthington 2015 ). This review was identified as a priority title by the orthodontic expert panel ( Cochrane OHG priority review portfolio ). The correction of prominent upper front teeth is one of the most common treatments performed by orthodontists. Even though we have several brace types to correct prominent upper front teeth, new braces are being introduced in the market to overcome the drawbacks of previous ones and there is a need to establish the relative effectiveness of the different braces that can be used. It is very important that we identify the most effective type of brace to give the best available treatment.",
        "summary": "Early orthodontic treatment may lower the incidence of new incisal trauma, but effects on other outcomes are unclear. Insufficient data are available to assess the effectiveness of late orthodontic treatment. In children with class II division malocclusion I (mean age 10 years), early orthodontic treatment with a functional appliance or with headgear commencing at 7 to 11 years of age proved better than observation in reducing the final overjet (by 4.17 mm and 1.07 mm, respectively) and the final ANB (by 0.89\u00b0 and 0.72\u00b0, respectively) after two years (phase I) but had no effect on these outcomes at 2.5 years (phase II) (trials including up to 400 participants). However, moderate\u2010 to low\u2010certainty evidence shows a lower incidence of new incisal trauma at 2.5 years (phase II) with a functional appliance or headgear compared with observation (on average, 192 vs 298 per 1000 people, and 106 vs 136 per 1000 children, respectively). When headgear was compared with functional appliances for early orthodontic treatment, functional appliances outperformed headgear in reducing the final overjet after two years by 0.75 mm (2 trials, 271 participants), but results for other outcomes were similar in both groups at 2 years and 2.5 years. As for late orthodontic treatment (initiated at age 11 to 16 years), low\u2010certainty evidence suggests that functional appliances might be more effective than observation in reducing the final overjet (by 5.46 mm) and the final ANB (by 0.53\u00b0). Reviewers found no studies that assessed people's satisfaction, incidence of new incisal trauma, or adverse effects."
    },
    "CD004269": {
        "query": "How does prophylactic platelet transfusion after chemotherapy and stem cell transplantation affect outcomes in patients with hematological disorders?",
        "document": "Background Description of the condition Platelet transfusions are used in modern clinical practice to prevent and treat bleeding in thrombocytopenic patients with bone marrow failure secondary to chemotherapy or stem cell transplantation. The ready availability of platelet concentrates has undoubtedly made a major contribution in allowing the development of intensive treatment regimens for haematological disorders (malignant and non\u2010malignant) and other malignancies. The first demonstration of the effectiveness of transfusions of platelets was performed in 1910 ( Duke 1910 ). However, it was not until the 1970s and 1980s that the use of platelet transfusions became standard treatment for thrombocytopenic patients with bone marrow failure ( Blajchman 2008 ). Alongside changes in supportive care, the routine use of platelet transfusions in patients with haematological disorders since that time has led to a marked decrease in the number of haemorrhagic deaths associated with thrombocytopenia ( Slichter 1980 ), however this has led to a considerable increase in the demand for platelet concentrates. Currently, platelet concentrates are the second most frequently used blood component. About 266,000 adult doses per year are transfused in the UK ( Taylor 2010 ), costing about \u00a350 million per year. Administration of platelet transfusions to patients with haematological disorders now constitute a significant proportion (up to 67%) of all platelets issued ( Cameron 2007 ; Greeno 2007 ; Pendry 2011 ), and the majority of these (69%) are given to prevent bleeding ( Estcourt 2011a ). Description of the intervention Despite the obvious beneficial effect that platelet transfusions have had on the management of patients with haematological malignancies with severe thrombocytopenia who are actively bleeding, questions still remain on how this limited resource should be used to prevent severe and life\u2010threatening bleeding ( Estcourt 2011b ). Prophylactic platelet transfusions for patients with chemotherapy\u2010induced thrombocytopenia became standard practice following the publication of several, small, randomised controlled trials in the late 1970s and early 1980s ( Higby 1974 ; Murphy 1982 ; Solomon 1978 ). This review does not focus on the absolute need for platelet transfusions in this patient population but instead reviews the transfusion strategies that most effectively balance the benefits of their use against their risks.This review focused on four different types of prophylactic platelet transfusion trial. The standard practice in most haematology units across the developed world has been to use prophylactic transfusions, in line with guidelines ( BCSH 2003 , Slichter 2007 ). The experimental intervention was to only give platelet transfusions when bleeding occurred. Prophylactic platelet transfusions are typically given when blood platelet counts fall below a given trigger level. Studies compared different platelet count thresholds to trigger the administration of prophylactic platelet transfusions.The current consensus is that patients should receive a platelet transfusion when the platelet count is <10x10 9 /L, unless there are other risk factors for bleeding, such as sepsis, concurrent use of antibiotics or other abnormalities of haemostasis ( BCSH 2003 ; Schiffer 2001 ; Slichter 2007 ). The experimental interventions were higher or lower platelet transfusion thresholds. This is the number of platelets given (platelet dose) during a standard platelet transfusion. For adults, the usual dose given is a single apheresis unit or a pool of four to six whole blood\u2013derived platelets, with the absolute number of platelets in the range of 300 x 10 9 to 600 x 10 9 ( Stanworth 2005 ). The experimental interventions were low dose or high dose platelet transfusion strategies. The standard practice in most haematology units across the developed world has been to use prophylactic transfusions, in line with guidelines ( BCSH 2003 ; Slichter 2007 ). The experimental intervention was to give an alternative treatment, such as artifical platelet substitutes, platelet\u2010poor plasma, rFVIIa or fibrinogen. How the intervention might work A retrospective review of almost 3000 thrombocytopenic adult patients over a 10\u2010year period showed no relationship between the first morning platelet count, or the lowest platelet count of the day, and the risk of bleeding ( Friedmann 2002 ). This has raised the question as to whether a threshold\u2010defined prophylactic platelet transfusion approach is appropriate. Further support for the absence of a relationship between the severity of thrombocytopenia and bleeding came from a review of case reports of severe intracranial haemorrhage. These cases were described in trials of prophylactic platelet transfusions. No clear evidence could be found for an association between the occurrence of major intracranial bleeding and absolute platelet count just prior to the onset of severe bleeding ( Stanworth 2005 ). Thus, the overall benefit of a prophylactic platelet transfusion policy over a policy to use platelets only therapeutically, using a platelet count threshold, has not been established. A recent study, using an historical control, assessed a therapeutic platelet transfusion strategy after autologous transplantation. Only 19% of the patients had clinically relevant bleeding of minor or moderate severity, and no severe or life\u2010threatening bleeding was documented, this was a comparable rate to the historical control (20%). One\u2010third of all transplants, and 47% after high\u2010dose melphalan, were performed without any platelet transfusion. The numbers of platelet transfusions were reduced by 50% compared with their historical control ( Wandt 2006 ). In an interim report of a randomised controlled trial ( Wandt 2009 ), platelet transfusions could be significantly reduced (by 27%) in the therapeutic transfusion arm compared with prophylactic transfusion. 46% of patients in the therapeutic arm did not need any platelet transfusions. However, the incidence of clinically relevant bleeding was significantly higher (28.7% vs 9.5%), this is not surprising as this was the trigger for transfusion in the experimental arm. Therefore a therapeutic platelet transfusion strategy may be safe and feasible. A large randomised controlled trial that hopes to answer this question has just completed recruitment of patients \u2013 TOPPs trial ( Blajchman 2008 ; Stanworth 2010 ). The dose of the platelet product transfused was based upon the perceived need to raise the patient's platelet count above a certain safe threshold. Over the years, our understanding of bleeding in thrombocytopenic patients has advanced and there is now evidence to suggest that patients require only approximately 7100 platelets/\u00b5L per day to maintain haemostasis ( Hanson 1985 ). Platelets have been shown to provide an endothelial supportive function by plugging gaps in the endothelium of otherwise intact blood vessels. Animal studies have shown that thrombocytopenia is associated with the gradual thinning of the vessel wall endothelium over time, and that, if thrombocytopenia persists, gaps gradually occur between adjacent endothelial cells ( Blajchman 1981 ; Kitchens 1975 ; Nachman 2008 ). This thinning and fenestration of the endothelium is accompanied with the on\u2010going and increased use of circulating platelets to prevent the loss of red blood cells (RBCs) through these gaps. A mathematical model predicted that smaller, more frequent doses of platelets would be as effective as higher doses of platelets in maintaining patients' platelet counts above an agreed threshold ( Hersh 1998 ). This raised the question of whether thrombocytopenic bleeding could be prevented with a lower platelet dose ( Tinmouth 2003 ). Such a strategy has potential economic and resource advantages, as fewer platelet transfusions might be required and donor exposures might be reduced. Several smaller studies tried to address this question but only one of them used bleeding as a primary outcome measure ( Tinmouth 2004 ). This showed that the low\u2010dose prophylactic regimen was just as effective as the standard dose and resulted in a 25% reduction in the number of platelets transfused. There have been two recent larger trials. One was stopped early because of an excess of WHO grade 4 bleeding ( Heddle 2009 ), and the large PLADO trial confirmed the earlier finding by Tinmouth ( Slichter 2010 ). Efforts were also made to establish a threshold for the use of prophylactic transfusions based on the platelet count. It became standard practice to transfuse platelets at platelet counts below 20 x 10 9 /L, in an attempt to prevent bleeding ( Beutler 1993 ). This practice was partly based on the findings of non\u2010randomised studies, such as Gaydos 1962 . This showed that gross haemorrhage (haematuria, haematemesis and melaena) was present more frequently at platelet counts below 5 x 10 9 /L than when the platelet count was between 5 x 10 9 /L and 100 x 10 9 /L. This study and others like it did not clearly support the use of a threshold for prophylactic platelet transfusion of 20 x 10 9 /L, nor was any threshold effect seen. The routine use of platelet transfusions from the 1970s, in patients with haematological malignancies, resulted in a decreased mortality rate due to bleeding (less than 1% of patients) ( Slichter 1980 ). However, the widespread use of a threshold platelet count of 20 x 10 9 /L for prophylactic platelet transfusions led to a marked growth in demand for platelet concentrates ( Sullivan 2002 ). This stimulated research to address whether the threshold could be safely lowered to 10 x 10 9 /L ( Rebulla 1997 , reviewed in Stanworth 2004 ). The consensus formulated from these trials was that patients should receive a platelet transfusion when the platelet count is <10x10 9 /L, unless there are other risk factors for bleeding, such as sepsis, concurrent use of antibiotics or other abnormalities of haemostasis ( BCSH 2003 ; Schiffer 2001 ; Slichter 2007 ). There have been calls for a further reduction in the threshold to 5 x 10 9 /L. In the 1970s it had been shown, using faecal blood loss as an indicator of bleeding, that the increased risk of bleeding was at the 5 x 10 9 /L threshold ( Slichter 1978 ). However, the ability to decrease the platelet threshold to this level may be compromised by the inaccuracy of automated platelet counters at very low platelet counts ( Segal 2005 ). Although platelet mass has been used as a transfusion trigger for neonatal platelet transfusions ( Gerday 2009 ), different platelet count thresholds have been the only known trigger used in patients with a haematological disorder. Most clinical research has focused on the optimal dose for platelet transfusion or the threshold level of platelet counts for prophylactic platelet transfusions, rather than questioning the underlying assumption that prophylactic platelet transfusions are necessary or effective. The most recent RCTs have established that many patients develop bleeding at some stage during the period of greatest risk, frequently defined as a period of thrombocytopenia ( Heddle 2009 ; Slichter 2010 ). This bleeding covers a spectrum of bleeding, from skin changes to, less commonly, intracranial haemorrhage. In Slichter 2010 , patients had similar rates of bleeding (17%) with morning platelet counts within the range of 6 to 80 x 10 9 /L. This means that there are a significant number of bleeding episodes that are not being effectively treated by prophylactic platelet transfusions. Treatments that target other parts of the clotting cascade may be as effective at treating bleeding as prophylactic platelet transfusions. A bleeding assessment has been seen as a more clinically relevant measure of the effect of platelet transfusions than surrogate markers such as platelet increment. Any review that uses bleeding as a primary outcome measure needs to assess the way that the trials have recorded bleeding. Unfortunately, the way bleeding has been recorded and assessed has varied markedly between trials ( Cook 2004 ; Heddle 2003 ). Retrospective analysis of bleeding leads to a risk of bias because bleeding events may be missed, and only more severe bleeding is likely to have been documented. Prospective bleeding assessment forms provide more information and are less likely to miss bleeding events. However, different assessors may grade the same bleed differently and it is very difficult to blind the assessor to the intervention. The majority of trials have used the WHO system for grading bleeding, or a modification of this system ( Koreth 2004 ). One limitation of all the scoring systems that have been based on the WHO system is that the categories are relatively broad and subjective. This means that a small change in a patient's bleeding risk may not be detected. Another limitation is that the modified WHO categories are partially defined by whether a bleeding patient requires a blood transfusion. The threshold for intervention may vary between clinicians and institutions and so the same level of bleeding could be graded differently in different institutions. The definition of what constitutes clinically significant bleeding has varied between studies, although the majority of more recent platelet transfusion studies ( Heddle 2009 ; Stanworth 2010 ; Slichter 2010 ) now classify it as WHO grade 2 or above there has been greater heterogeneity in the past ( Cook 2004 ; Koreth 2004 ). The difficulties with assessing and grading bleeding may limit the ability to compare results between studies and this needs to be kept in mind when reviewing the evidence for the effectiveness of prophylactic platelet transfusions. Why it is important to do this review Although considerable advances have been made in platelet transfusion therapy in the last 40 years, three major areas continue to provoke debate. Firstly, what is the optimal prophylactic platelet dose to prevent thrombocytopenic bleeding? Secondly, which threshold should be used to trigger the transfusion of prophylactic platelets? Thirdly, are prophylactic platelet transfusions superior to therapeutic platelet transfusions for the prevention and/or control of life\u2010threatening thrombocytopenic bleeding? The initial formulation of this Cochrane review attempted to answer these questions, but there was insufficient evidence available at the time for any definitive conclusions to be drawn ( Stanworth 2004 ). This update of the review reassessed the literature, to ascertain whether any recent studies can provide us with the evidence to answer those questions. There has been another recent systematic review of the optimal dose for prophylactic platelets ( Cid 2007 ), but this is now out\u2010dated because several new large studies have recently been completed. This review did not assess whether there are any differences in the efficacy of apheresis versus whole\u2010blood derived platelet products, nor did it assess differences between ABO identical and ABO non\u2010identical platelet transfusions. This is because both these topics have been covered by recent systematic reviews ( Heddle 2008 ; Shehata 2009 ).",
        "summary": "In people with hematological disorders, the effects of prophylactic platelet transfusion after chemotherapy and stem cell transplantation are uncertain. Based on randomized controlled trial evidence, no definitive answers could be obtained for the following questions: how do prophylactic platelet transfusions compare with therapeutic transfusions? Which threshold should be used to trigger the transfusion of prophylactic platelets? What is the optimal prophylactic dose schedule to prevent bleeding? No definite answers could be given for any of these questions. When prophylactic platelet transfusion was compared with therapeutic transfusion, low\u2010quality evidence showed similar effects with both interventions in some outcomes (numbers of participants with at least one clinically significant bleeding event, number of days with a clinically significant bleeding event and mortality secondary to bleeding). Other outcomes (mean number of red cell transfusion per patient, remission rates, adverse events) could not be properly assessed as only single studies with small sample sizes contributed to the analyses. Moderate\u2010quality evidence shows similar numbers of participants with at least one clinically significant bleeding event with a high platelet count transfusion trigger (<20 or <30 \u00d7 109/L) compared with the usual low platelet count transfusion trigger (<10 \u00d7 109/L). However, the number of days with a clinically significant bleeding event was higher when a lower platelet count trigger was used but the mean number of platelet transfusions per patient was lower. When different dose schedules of prophylactic platelet transfusion were compared, no evidence of a difference was observed for any outcomes (high\u2010 to low\u2010quality of evidence). Wheezing was worse with high dose, but not low dose, when compared with standard dose (1272 participants)."
    },
    "CD002901": {
        "query": "What are the benefits and harms of digitalis for treating heart failure in patients with sinus rhythm?",
        "document": "Background Until the past three decades, no full\u2010scale randomized trials of digitalis versus placebo were conducted, in part because of concerns about the use of placebo controls to replace the active agent. However, several clinical studies have appeared, showing that digitalis in many instances could be safely withdrawn in participants with HF ( Dall 1970 ; Fonrose 1974 ; Gheorghiade 1983 ; Hull 1977 ; Johnston 1979 ; McHaffie 1978 ; Starr 1969 ). At the same time, studies appeared suggesting that digitalis may have favorable short\u2010term effects on exercise tolerance, symptoms, and cardiovascular event rates ( Arnold 1980 ; Dobbs 1977 ; Firth 1980 ; Fleg 1991 ; Kirsten 1973 ; O'Rourke 1976 ; Vogel 1977 ). Some concern has focused on possible long\u2010term toxicity from digitalis, as well as other inotropic agents. The latter was suggested by studies of treatment with the phosphodiesterase inhibitors milrinone ( Packer 1991 ) and enoximone ( Cowley 1994 ), as well as other agents such as the inodilator vesnarinone ( Cohn 1998 ). Digitalis itself was implicated in several nonrandomized trials suggesting that patients with HF treated with digitalis might show excessive mortality rates, particularly when the underlying clinical diagnosis was ischemic heart disease ( Bigger 1985 ; Byington 1985 ; Moss 1981 ). Thus conflicting evidence shows that some studies have suggested that digitalis might be discontinued without adverse effects, others have indicated that beneficial effects might indeed be present, and still others have suggested that digitalis might have long\u2010term toxic effects. Over the past 31 years, information has become available that helps to settle these points. Since 1982, 12 randomized controlled trials of digitalis in HF participants have been published ( Blackwood 1990 ; Dig captopril 1988 ; Dig milrinone 1989 ; Dig xamoterol 1988 ; DIMT 1993 ; Fleg 1982 ; Guyatt 1988 ; Lee 1982 ; PROVED 1993 ; Pugh 1989 ; RADIANCE 1993 ; Taggart 1983 ), indicating that digitalis improves clinical outcomes\u2014a conclusion reached in a meta\u2010analysis of the first seven of these trials published in 1990 ( Jaeschke 1990 ). In addition, a large randomized controlled trial has been published showing that digitalis, while improving hospitalization rates, has no significant effect on long\u2010term mortality ( DIG study 1997 ). The current review updates the 1990 meta\u2010analysis and provides a summary statement about the current status of digitalis in treating HF. Although the utility of digitalis glycosides for controlling heart rate in patients with rapid atrial fibrillation is well recognized, the current review is restricted to the use of digitalis in patients who are in normal sinus rhythm.",
        "summary": "Orally administered digoxin may be beneficial in terms of reducing the need for hospitalization, improving patients\u2019 symptoms, and slowing clinical deterioration in adults with symptomatic heart failure in sinus rhythm; several trials informing these outcomes were at an unknown risk of bias, making the reliability of their results uncertain. There was no discernible reduction in mortality. The review assessed the effects of orally administered digoxin (for at least seven weeks, most commonly at 0.25 mg/day) in adults with symptomatic heart failure (primarily Class II or III heart failure). At least 90% of the participants were in sinus rhythm, the average age across studies ranged from 58 to 69 years, and the proportion of men ranged from 38% to 90%. Digoxin had no detectable impact on mortality (eight RCTs, 7755 participants) when compared with placebo. This analysis was dominated by a single large trial in patients with reduced left ventricular ejection fraction (45% or less). This trial was not subject to any major bias that would adversely affect this outcome. The other studies in the analysis were very small in comparison and had a lot of uncertainty surrounding their results, but overall were consistent with the larger study. Digoxin reduced the need for hospitalization (four RCTs, 7262 participants; analysis dominated by the same large trial), improved patients\u2019 symptoms (eight RCTs, 681 participants), and slowed clinical deterioration (12 RCTs, 1234 participants). Four out of ten trials (1049 participants) reported improvements in functional capacity with digoxin during exercise testing, whilst only one out of five studies (over 347 participants, exact number unclear) reported any improvement in quality of life with digoxin; all the trials were underpowered as the largest trial did not report on these outcomes. The risk of bias that could impact on the more subjective outcomes (changes in symptoms and heart failure classification, clinical deterioration and quality of life) was unknown for several of the studies, which makes it difficult to assess the reliability of the results of these trials. The review did not evaluate adverse events; given the frequency and seriousness of such events (particularly gastrointestinal and neurological), these would need to be weighed against any potential benefit when considering the use of digoxin in this population."
    },
    "CD006127": {
        "query": "In people with diabetes, what are the effects of blood pressure control on diabetic retinopathy?",
        "document": "Background Description of the condition Diabetic retinopathy is a complex disorder of the retinal vasculature that is characterized by increased vascular permeability, retinal ischemia and edema, and new blood vessel formation. The National Eye Institute reports age\u2010related macular degeneration, cataracts, glaucoma, and diabetic retinopathy to be the leading causes of visual impairment and blindness among Americans older than 40 years ( EDPRG 2004a ). Similar findings have been reported for older Americans over the age of 75 years ( Desai 2001 ) and from other epidemiologic studies from Western Europe ( Buch 2004 ; Grey 1989 ; Krumpaszky 1999 ; Rosenberg 1996 ). Globally, diabetes mellitus is a significant public health problem. Some estimates predict that the worldwide prevalence of diabetes will exceed 366 million people by 2030 ( Wild 2004 ). Diabetic retinopathy is a common complication among individuals with diabetes and an important cause of loss of vision ( Sivaprasad 2012 ). A diabetic individual has a three\u2010fold increased risk of blindness compared with the general population ( Hayward 2002 ). The US Centers for Disease Control and Prevention estimates that 25.8 million people in the US were living with diabetes in 2010 ( CDC 2011 ). In the US alone, it is estimated that 4.1 million adults over the age of 40 have diabetic retinopathy (any level of severity) and that 899,000 adults have vision\u2010threatening diabetic retinopathy ( EDPRG 2004b ). Among Americans with type 1 diabetes, the prevalence of diabetic retinopathy of any severity is 74.9% and 82.3% in black and white persons respectively; the prevalence of vision\u2010threatening (severe non\u2010proliferative and proliferative) retinopathy is 30% and 32.2% ( EDPRG 2004c ). The prevalence of diabetic retinopathy among type 1 and type 2 diabetics in Wales recently was reported to be 56% and 30.3%, respectively ( Thomas 2014 ). People with impaired visual acuity or legal blindness secondary to diabetic retinopathy face enormous challenges in pursuing activities of daily life. Visual impairment is defined as best\u2010corrected visual acuity worse than 20/40 in the better\u2010seeing eye; blindness is defined as best\u2010corrected visual acuity of 20/200 or worse in the better\u2010seeing eye as measured on the original Bailey\u2010Lovie or modified Bailey\u2010Lovie (also known as the Early Treatment Diabetic Retinopathy Study (ETDRS)) visual acuity chart or other charts that use a logMAR scale. The duration of diabetes and the severity of hyperglycemia are major risk factors associated with the development (incidence) and progression of diabetic retinopathy ( DCCT 1993 ; DRS10 1985 ; ETDRS18 1998 ; Harris 1998 ; Klein 1984a ; Klein 1984b ; Klein 1988 ; Krakoff 2003 ; Kullberg 2002 ; Leske 2003 ; Porta 2001 ; UKPDS33 1998 ; van Leiden 2003 ; Zhang 2001 ). After retinopathy develops, persistent hyperglycemia has been reported to be a more important factor than duration of diabetes for progression of the disease ( ETDRS18 1998 ; Giuffre 2004 ). The Diabetes Control and Complications Trial ( DCCT ) and the United Kingdom Prospective Diabetes Study (UKPDS) demonstrated lower incidence and slower progression of diabetic retinopathy with tight blood glucose control ( DCCT 1993 ; UKPDS33 1998 ). The Diabetic Retinopathy Study ( DRS2 1978 ; DRS5 1981 ; DRS8 1981 ) and the ETDRS ( ETDRS1 1985 ; ETDRS9 1991 ) demonstrated a decrease in the progression of proliferative diabetic retinopathy and diabetic macular edema with more than 1200 applications of 'pan\u2010retinal' (for proliferative retinopathy) or with 'focal' (for macular edema) laser photocoagulation. However, the prevalence of diabetic retinopathy observed in recent epidemiologic studies ( EDPRG 2004b ; EDPRG 2004c ) conducted after the DCCT and UKPDS continued to be high. Recently, investigators participating in trials conducted by the Diabetic Retinopathy Clinical Research Network (DRCRnet) have reported that other treatments, either alone or combined with laser treatment, can slow progression of diabetic retinopathy ( DRCR.net 2010 ). These studies strongly support treatment of diabetic retinopathy to reduce loss of vision. Nevertheless, findings from all studies reinforce the need to evaluate the role of other risk factors and to intervene on those that are modifiable in order to decrease the prevalence of diabetic retinopathy. Risk factors for diabetic retinopathy include hypertension ( Klein 1989a ; Klein 1989b ; Leske 2003 ; Tapp 2003 ; UKPDS38 1998 ), hypercholesterolemia ( Chew 1996 ; Klein 2002b ; van Leiden 2002 ), abdominal obesity and elevated body mass index ( van Leiden 2002 ; van Leiden 2003 ; Zhang 2001 ), alcohol intake ( Giuffre 2004 ), younger age at onset ( Krakoff 2003 ; Kullberg 2002 ; Porta 2001 ), smoking, and ancestry ( Keen 2001 ; Moss 1996 ). Age and ancestry are not modifiable, but other risk factors suggest possible interventions. Diabetic retinopathy progresses sequentially from a mild non\u2010proliferative stage to a severe proliferative disorder. Increased retinal vascular permeability occurs early, at the stage of mild non\u2010proliferative diabetic retinopathy (NPDR). Moderate and severe NPDR are characterized by vascular closure, which results in impaired retinal perfusion (ischemia). Diabetic retinopathy typically is diagnosed during ophthalmoscopy. Fundus photographs and fluorescein angiograms may be used to monitor progression. Signs of NPDR include microaneurysms, intraretinal hemorrhages, and occasional 'cotton wool spots' caused by closure of small retinal arterioles, resulting in localized ischemia and edema, with consequent damage to nerve fibers leading to reduced axonal transport. Signs of increasing ischemia include extensive intraretinal hemorrhages, venous abnormalities such as wide variations in caliber ('beading') and looping ('reduplication'), capillary non\u2010perfusion, and intraretinal microvascular abnormalities. Severe NPDR, also known as pre\u2010proliferative retinopathy, is diagnosed when these changes progress to pre\u2010defined thresholds. Proliferative diabetic retinopathy (PDR) is characterized by neovascularization, which is the growth of abnormal blood vessels in response to severe ischemia. The new vessels grow into the vitreous and often are seen at the optic disc (NVD) and elsewhere in the retina (NVE); they are prone to bleeding, which results in vitreous hemorrhage and vision loss. Furthermore, these vessels may undergo fibrosis and contraction and, along with other fibrous proliferation, may lead to epiretinal membrane formation, vitreoretinal traction bands, retinal tears, and either tractional or rhegmatogenous retinal detachments (that is, those due to a retinal hole or tear). It is said that proliferative diabetic retinopathy is at the high\u2010risk stage when NVE that occupy a total area of 0.5 optic disc area or more in size throughout the retina are accompanied by pre\u2010retinal or vitreous hemorrhage, or when NVD occupy an area greater than or equal to about one\u2010third disc area, even in the absence of vitreous hemorrhage, or when NVD of any size are accompanied by vitreous hemorrhage. People in the 'high risk' stage of PDR who do not receive prompt pan\u2010retinal laser treatment have a 30% to 50% probability of progressing to severe visual acuity loss and blindness (less than 5/200 best\u2010corrected visual acuity) in three years ( DRS8 1981 ; ETDRS10 1991 ; ETDRS12 1991 ). Increased retinal vascular permeability, which can occur at any stage of diabetic retinopathy, may result in retinal thickening (edema) and lipid deposits (hard exudates). Retinal thickening, hard exudates, or both that occur at or within 500 microns (approximately one\u2010third an optic disc diameter) of the center of the macula, and which therefore threaten, or actually cause, loss of central visual acuity, are referred to as clinically significant macular edema. The major reasons for vision loss in diabetic retinopathy include macular edema, macular capillary non\u2010perfusion (which can be demonstrated by fluorescein angiography), vitreous hemorrhage, distortion or tractional detachment of the retina ( PPP 2012 ), and neovascular glaucoma (new blood vessels in the iris), which usually is associated with very late\u2010stage PDR ( Fong 2004 ). Several biochemical pathways have been investigated for the pathogenesis of diabetic retinopathy. Apart from the well\u2010documented role of chronic hyperglycemia, none of the other biochemical pathways has been shown conclusively to be relevant ( Frank 2004 ). Although the exact mechanism for the pathogenesis of hypertensive damage in eyes with diabetic retinopathy is unknown, scientists have hypothesized that an increase in blood pressure damages the retinal capillary endothelial cells ( Klein 2002a ). In eyes with diabetic retinopathy, chronic hyperglycemia leads to endothelial cell damage, pericyte loss, and breakdown of the blood\u2010retinal barrier. Such changes to the structure of the microvasculature lead to dysregulation of retinal perfusion, thereby making eyes with diabetic retinopathy more susceptible to hyperperfusion damage from hypertension ( Gillow 1999 ). Description of the intervention The current standard of care for the prevention and treatment of diabetic retinopathy consists of strict glycemic control and regular ophthalmologic screening for diabetic retinopathy among diabetics, the use of focal laser treatment or intravitreal anti\u2010vascular endothelial growth factor injections for diabetic macular edema, and the use of pan\u2010retinal scatter laser photocoagulation for proliferative diabetic retinopathy ( Smith 2011 ; Virgili 2014 ). Strict blood pressure control has been recommended as part of the standard of care for diabetics, primarily because of its known beneficial effect on the prevention of cardiovascular events, stroke, and nephropathy, rather than for its effect on diabetic retinopathy ( Hansson 1998 ; HOPESI 2000 ). How the intervention might work Blood pressure control may be beneficial in preventing the development or slowing the progression of diabetic retinopathy by reducing the damage to endothelial cells, blood vessels, and surrounding tissues from hyperperfusion. Diabetic retinopathy leads to endothelial cell dysfunction, loss of pericytes, and breakdown of the blood\u2010retinal barrier. Hypertension may cause additional vascular damage because of shearing that occurs with hyperperfusion. Blood pressure control may prevent hyperperfusion and decrease the likelihood of shearing damage to the blood vessels from hypertension. Why it is important to do this review Diabetic retinopathy remains an important cause of vision loss even with good blood glucose control ( ADA 1998 ; Ferris 1993 ). At the end of the DCCT , with participant follow\u2010up of 6.5 \u00b1 1.6 years (mean \u00b1 standard error), 10% of type 1 diabetic patients in the intensive glycemic control group had developed diabetic retinopathy despite strict glycemic control ( Zhang 2001 ). Similarly, in the UKPDS, tight blood glucose control decreased but did not eliminate the risk of diabetic retinopathy ( UKPDS33 1998 ). Diabetic retinopathy is a substantial public health problem ( Zhang 2010 ). Because studies of retinal physiology suggest a role for blood pressure in pathological changes in diabetic retinopathy ( Sj\u00f8lie 2011 ), a systematic review of the effectiveness of blood pressure control with respect to diabetic retinopathy is warranted ( Sleilati 2009 ).",
        "summary": "Looking at studies comparing tight blood pressure control in people with diabetes (type 1 or 2) with loose/no hypertensive control, there is moderate quality evidence to suggest that tight blood pressure control may reduce the prevalence of diabetic retinopathy in 4\u20105 years (on average 228 versus 285 per 1000 people). Moderate\u2010quality evidence suggests that tight blood pressure control does not prevent the progression of retinopathy, the onset of proliferative diabetic retinopathy/clinically significant macular edema, or the loss of vision (3 lines or more) in the same time period. However, more people experienced hypotension with tight control (on average 137 vs. 66 per 1000 people). A clinically important subgroup is people with diabetic retinopathy that is likely to progress, such as severe non\u2010proliferative retinopathy or those with macular disease. These people are more likely to lose vision, therefore, it is possible that the benefit of treatment could be greater, making the risk of adverse effects more acceptable. These were not analyzed separately in the review, however, where reported, the number of participants with moderate to severe non\u2010proliferative (Early Treatment Diabetic Retinopathy Study (ETDRS) score 41 to 53) was often low (<10% in 4 trials, 17% in 1 trial, 36% 1 trial)."
    },
    "CD011749": {
        "query": "What are the benefits and harms of pharmacological interventions for treating critically ill adults with delirium?",
        "document": "Background Description of the condition Delirium is a reversible, non\u2010specific syndrome of cognitive impairment commonly associated with surgery, infection, or critical illness ( APA 2013 ). In the intensive care unit (ICU), this acute brain dysfunction is reported in 40% to 60% of non\u2010ventilated patients, and in 50% to 80% of mechanically ventilated patients ( Ely 2001a ; Ely 2001b ; Ely 2007 ; Hipp 2012 ; Inouye 2014 ). Delirium is challenging to detect, as symptoms are highly variable, with either hyperactivity or hypoactivity, or even a mixed picture, and symptoms fluctuate with periods of lucidity ( Inouye 2014 ). Delirium may be detected by psychiatric assessment based on the Diagnostic and Statistical Manual (DSM) criteria ( APA 2013 ), or by use of a validated screening tool ( Bergeron 2001 ; Ely 2001a ; Neelon 1996 ); however, assessment in the ICU is predicated on the patient being awake and able to communicate, and delirium is said to be \"unable to be assessed\" when the patient does not respond to verbal communication. In the ICU, commonly used sedatives and opioids impair consciousness, thereby making identification of delirium challenging ( Patel 2014 ). Drug exposure should be considered when ICU delirium is assessed, and if possible, assessments should be co\u2010ordinated with periods of wakefulness or should be conducted during a sedation interruption ( Patel 2014 ). Over the past decade, we have acquired a greater understanding of the effects of delirium on patients, their families, and the healthcare system. Clinically important outcomes of delirious critically ill patients include prolonged duration of mechanical ventilation and ICU and hospital stay, as well as long\u2010term cognitive impairment, increased likelihood of transfer to long\u2010term care facilities, and mortality ( Black 2011 ; Ely 2001b ; Ely 2004 ; Girard 2010b ; Jackson 2004 ; Lin 2004 ; Milbrant 2004 ; Pisani 2009 ; Van den Boogaard 2012 ). The odds of a poor outcome with delirium are increased by patient frailty, advanced age (> 75 years), pre\u2010existing cognitive impairment, and visual or hearing impairment ( Andrew 2006 ; Inouye 2006a ). Precipitating factors are numerous and include sleep deprivation, pain, environmental insults (e.g. noise, physical restraint use, catheters), and psychoactive drug exposure (e.g. sedatives) ( Burry 2017 ; Fraser 2013 ; Inouye 2006a ; Rose 2016 ; Zaal 2015 ). Description of the intervention Pharmacological interventions for delirium treatment have focused on alterations in neurotransmitter pathways, in particular dopaminergic and cholinergic pathways. At present, the pathophysiology of delirium is not fully understood ( Gunther 2008 ; Reade 2014 ). Hypotheses currently include abnormalities in cerebral oxidative metabolism, direct neurotoxic effects of inflammatory cytokines, such as those released during sepsis and septic shock, and alterations in neurotransmitters that modulate cognition, behaviour, and mood (e.g. cholinergic, dopaminergic, serotonergic, gamma\u2010aminobutyric acid (GABA) pathways) ( Cerejeira 2011 ; de Rooji 2007 ; Ebersoldt 2007 ; Flacker 1999 ; Gunther 2008 ; Inouye 2006b ; Rudolph 2008 ; White 2002 ). These pathophysiological mechanisms are not thought to be mutually exclusive and are likely to act together. In the light of these different proposed mechanisms, it is not surprising that numerous pharmacological strategies for delirium have been investigated, including alpha 2 agonists, antidepressants, antipsychotic drugs (either typical or atypical agents), benzodiazepines, cholinesterase inhibitors, melatonin and melatonin agonists, and opioids ( Devlin 2010 ; Girard 2010a ; Maldonado 2009 ; Ohta 2013 ; Reade 2009 ; Rubino 2010 ; van Eijk 2010 ). In considering these agents, it is important to note that critical care guidelines first recommend the use of non\u2010pharmacological strategies in both prevention and management of delirium ( Barr 2013 ). These non\u2010pharmacological strategies include early mobilization and re\u2010orientation, risk factor assessment and modification (e.g. drugs, medical devices), and normalization of the sleep\u2010wake cycle (e.g. noise reduction, use of ear plugs) ( Inouye 2006a ; Schweickert 2009 ). Guidelines suggest that when delirium is suspected or identified, patients should be closely evaluated for identification of underlying cause(s), allowing for exposure to be removed or corrected whenever possible; pharmacological interventions are to be used only when non\u2010pharmacological methods have failed to control symptoms ( Barr 2013 ). How the intervention might work Given the multiple neurotransmitters linked to development of delirium, pharmacological strategies have investigated target suspected neurotransmitter imbalances or attempts to control distressing cognitive (e.g. hallucinations) or dangerous behaviours (e.g. agitation, interference with medical devices). Pharmacological strategies may target pain control (e.g. opioids) or the dopaminergic (e.g. antipsychotics), cholinergic (e.g. cholinesterase inhibitors), GABA (e.g. benzodiazepines), N\u2010methyl\u2010D\u2010aspartate (NMDA) (e.g. ketamine), serotonergic (e.g. antidepressants, antinauseants, melatonin), and alpha 2 (e.g. clonidine, dexmedetomidine) pathways ( Devlin 2010 ; Girard 2010a ; Maldonado 2009 ; Ohta 2013 ; Reade 2009 ; Rubino 2010 ; van Eijk 2010 ). The specific therapeutic effects of such agents are unknown, but effects may be mediated through their ability to affect sedation and behavioural symptoms. Despite conflicting evidence for the benefits of various pharmacological interventions, many of these agents are routinely used to treat ICU delirium, or to at least manage symptoms (e.g. agitation), and they are often continued after hospital discharge ( Bell 2007 ; MacSweeney 2009 ). Of the available pharmacological strategies, antipsychotics represent the most common treatment for ICU delirium, despite limited evidence regarding their benefit and studies in non\u2010critically ill patients identifying significant adverse effects, including sudden death ( Barr 2013 ; Briskman 2010 ; Burry 2014 ; Gill 2007 ; MacSweeney 2009 ; Tropea 2009 ; Wang 2005 ). Why it is important to do this review ICU delirium is associated with prolonged duration of mechanical ventilation and ICU and hospital stay, as well as increased mortality ( Ely 2001b ; Ely 2004 ; Girard 2010b ; Jackson 2004 ; Lin 2004 ; Milbrant 2004 ; Pisani 2009 ; Van den Boogaard 2012 ). ICU delirium initiates a cascade of events that can include functional decline and long\u2010term cognitive impairment, with resultant caregiver burden ( Girard 2010b ; Jackson 2004 ; Van den Boogaard 2012 ). The geriatric and oncological literature shows that delirium is traumatic for both patients and family members, and it can lead to long\u2010term psychological sequelae ( Bruera 2009 ; Morita 2004 ; Partridge 2013 ; Rosenbloom\u2010Brunton 2010 ). The economic burden of delirium is also significant; each additional day spent in a delirious state is associated with a 20% increased risk of prolonged hospitalization, translating to an average of more than 10 additional hospital days per patient. The annual cost of delirium is estimated to be greater than USD 164 billion in the USA, and greater than EUR 182 billion as estimated across 18 European countries ( Leslie 2008 ; OECD 2012 ; WHO Regional Office 2012 ). Furthermore, delirium is considered a substantial public health concern that has garnered the attention of patient safety institutes; it is now included as an indicator of quality care for the elderly ( IHI 2015 ). Advances in detection of ICU delirium and improved understanding of its impact on patient outcomes have prompted trials comparing different treatment options (both pharmacological and non\u2010pharmacological), either against each other or versus placebo. However, there remains considerable uncertainty regarding the relative benefits and safety of pharmacological interventions for the ICU population, and trials have shown benefit ( Devlin 2010 ; Pandharipande 2007 ; Reade 2009 ), indeterminate outcomes ( Girard 2010a ; Page 2013 ), or harm ( van Eijk 2010 ). A previous Cochrane Review on antipsychotics for delirium did not specifically address the ICU population ( Lonergan 2007 ); numerous ICU\u2010specific trials have been published since this review was completed. A recent systematic review of ICU delirium included both prevention and treatment studies ( Al\u2010Qadheeb 2014 ), as well as randomized controlled trials (RCTs) evaluating sedation strategies, in which delirium was evaluated as a secondary endpoint when the study population considered was not restricted to patients with confirmed delirium. As a Cochrane Review protocol by Herling and colleagues will provide data on delirium prevention trials in critically ill adult patients ( Herling 2018 ), our review focuses on delirium treatment trials in critically ill adult patients. Given the availability of numerous strategies to treat ICU delirium in clinical practice, and the existence of many trials yielding conflicting results, we planned this systematic review to include a network meta\u2010analysis (NMA) to determine the comparative benefits and harms of all published pharmacological interventions for treatment of delirium based on available direct and indirect evidence of relevance. An NMA, also known as a multiple treatment comparison meta\u2010analysis, is a statistical method used to assess the comparative effectiveness of multiple different interventions among similar patient populations that have not been compared directly in an RCT. In contrast to conventional pairwise meta\u2010analysis (e.g. RCTs comparing treatment A vs treatment B), NMAs can provide estimates of relative efficacy between all interventions, even though some have never been compared head\u2010to\u2010head via indirect evidence (i.e. comparing results from two or more studies that have one treatment in common).",
        "summary": "Most pharmacological interventions are no more effective than placebo for treating critically ill adults with delirium. High\u2010 to low\u2010certainty evidence suggests that both atypical and typical antipsychotics had little or no effect on delirium, coma, duration of mechanical ventilation, or length of intensive care unit (ICU) stay in critically ill adults with delirium during hospital stay compared with placebo. Rates of adverse events, when reported, seem no higher than with placebo but the analyses were underpowered. Reviewers found similar effects in both groups for all outcomes when typical and atypical antipsychotics were compared, and a single small RCT provided insufficient evidence to allow conclusions as to how typical antipsychotics compare with opioids. For the alpha 2 agonist dexmedetomidine, moderate\u2010certainty evidence shows a shorter delirium duration of 0.55 days and a shorter duration of mechanical ventilation of 0.59 days compared with placebo, but these results are based on a single small RCT (71 participants). For cholinesterase inhibitors, moderate\u2010certainty evidence shows a longer delirium duration of 0.61 days and a longer duration of ICU stay of 0.78 days compared with placebo, but again these results are based on a single small RCT (104 participants)."
    },
    "CD009900": {
        "query": "What are the effects of pharmacological interventions for preventing epilepsy following traumatic head injury?",
        "document": "Background Description of the condition Head injury is a common event and can cause a spectrum of motor and cognition disabilities. A frequent complication is seizures. While 'early seizures' are frequently considered to be nonspecific diffuse reactions as a result of an acute encephalopathy and are self limited, seizures following several weeks or months after head trauma seem to reflect an underlying process of post\u2010traumatic scar formation and epileptogenesis. However, there is evidence from epidemiologic studies that early seizures can be predictors for late seizures ( Wyllie 2010 ). This suggests that these definitions reflect simplifications of the underlying ongoing tissue transformation over time. The risk for late 'unprovoked' seizure recurrence increases with the severity of the injury, involvement of the cerebral cortex, presence of dura penetration, skull fracture and intracerebral hematoma, and the occurrence of early seizures ( Jennett 1981 ; Annegers 1998 ). Timing and the interplay of potentially involved factors in the development of this epileptogenic process are unclear. Description of the intervention Behind the concept of preventing post\u2010traumatic epilepsy stands the hope that the silent period of weeks and months after the trauma, before seizure occurrence, is a window of opportunity to stop the process using appropriate interventional treatment strategies ( Temkin 2009 ). Antiepileptic drugs (AEDs) can suppress seizures; however, it is the subject of a controversial debate if they are also able to interfere positively with the process leading to epilepsy. Experimental studies looking at neuroprotective agents, such as antioxidants and free radicals, have also been promising but historically have not translated well into the clinical environment ( Slemmer 2008 ). Therefore, this Cochrane review will carefully evaluate the impact of either early or late use of AEDs and neuroprotective agents on the occurrence of unprovoked seizures following the trauma. How the intervention might work Current experimental epilepsy research using animal models, such as kindling and post\u2010status epileptic condition, suggests that some new AEDs may have the potential to alter the underlying epileptogenic process and act as disease\u2010modifying agents ( L\u00f6scher 2002 ; Brandt 2006 ). There is also some evidence that neuroprotective agents may alter the epileptogenic process. For example, antioxidants may be able to suppress this process by interfering with free radical reactions initiated by hemorrhage associated with brain injuries ( Willmore 2009 ). Why it is important to do this review Post\u2010traumatic seizures are quite prevalent. Most of these people undergo a careful functional and structural diagnostic algorithm including electroencephalography (EEG) and magnetic resonance imaging (MRI) or at least computed tomography (CT). Therefore, post\u2010traumatic seizures can be considered an ideal model to study tissue changes and regional hyperexcitability as part of the evolving epileptogenic scar. It is not yet known whether immediate medical intervention following head trauma with either AEDs or neuroprotective drugs can alter the process of epileptogenesis and lead to a more favorable outcome. There are limited data on traditional AEDs such as phenytoin, phenobarbital, valproate and carbamazepine. With the advent since the mid\u20102000s of many new AEDs and research into alternative treatments such as neuroprotective agents, it seems critical and timely to review the human experience carefully and evaluate how these experimental findings might translate into the prevention of post\u2010traumatic epilepsy in clinical practice. Therefore, this review will conduct a systematic, up\u2010to\u2010date review of randomized controlled trials (RCTs) examining the effectiveness and safety of both AEDs and neuroprotective agents with special focus on recently licensed products.",
        "summary": "Phenytoin may confer benefit in terms of post\u2010trauma seizures in the short term after a moderate to severe traumatic brain injury (TBI), and may be associated with fewer deaths than other antiepileptic drugs, but most of the evidence was of low quality and no firm conclusions could be. When people admitted to an emergency department or an acute trauma unit after a moderate to severe TBI were treated with an antiepileptic drug (most commonly, phenytoin) compared with placebo or usual care, low\u2010quality evidence suggested a decrease in the incidence of seizures within one week of trauma (on average, 59 versus 139 per 1000 people). Study results showed no such benefit for seizure incidence between 2 and 24 months, mortality, or adverse events, but all evidence was of very low to low quality. Moderate\u2010quality evidence showed that fewer people died with phenytoin compared with levetiracetam or valproate (on average, 87 versus 164 per 1000 people). Low\u2010 to moderate\u2010quality evidence detected no difference between different antiepileptic drugs given for post\u2010trauma seizures. In comparing the neuroprotective agent magnesium sulfate versus placebo, one further trial showed no benefit of magnesium sulfate in terms of post\u2010trauma seizures (low\u2010 to high\u2010quality evidence) or mortality (high\u2010quality evidence). The trials did not assess adverse event profiles for either of these comparisons."
    },
    "CD004759-0": {
        "query": "What are the effects of using an antibody preparation during induction therapy (alone or with other immunosuppressive agents) for kidney transplant recipients?",
        "document": "Background Description of the condition Kidney transplantation is the treatment of choice for many patients with end\u2010stage kidney disease (ESKD) but demand exceeds supply from organ donors. Increasing this supply and prolonging kidney transplant survival are therefore important for patients and health systems ( Tonelli 2011 ). Description of the intervention Immunosuppressive therapy consists of initial induction and maintenance regimens to prevent rejection. Induction may be defined as treatment with a biologic agent either before, at the time of, or immediately after transplantation to deplete or modulate T cell responses at the time of antigen presentation. Maintenance immunosuppression protocols usually involve three drugs acting on different parts of the T\u2010cell activation or proliferation cascade: calcineurin inhibitors (CNI) (e.g. cyclosporin (CSA), tacrolimus), antiproliferative agents (e.g. azathioprine, mycophenolate mofetil) and corticosteroids (e.g. prednisolone) ( Denton 1999 ; Hong 2000 ). Induction immunosuppression with antibody therapy is now recommended at the time of transplantation for all patients ( KDIGO 2009 ). Antibody therapies are monoclonal or polyclonal, and depleting or non\u2010depleting of lymphocytes. Non\u2010depleting interleukin\u20102 receptor monoclonal antibodies (IL2Ra) are considered first line but it is suggested that recipients at high risk of rejection (e.g. children, subsequent transplants, certain racial groups such as African\u2010Americans, and other sensitised patients) should receive lymphocyte\u2010depleting antibodies. Depleting antibodies are also used for those at risk of delayed graft function to delay the introduction of full dose CNI, which can prolong the duration of acute tubular necrosis ( Denton 1999 ). Depleting antibodies include polyclonal antibodies against the human lymphocyte (antilymphocyte globulin (ALG); antithymocyte globulin (ATG)). How the intervention might work Depleting antibodies bind to target immune effector cells leading to complement mediated destruction. Non\u2010depleting antibodies bind to targets on effector cells preventing their interaction with other cells rendering them ineffective, but do not lead to cell destruction. Most antibodies used in transplantation have been directed at T cells. Significant reduction in circulating T\u2010effector cells is rapidly observed, leading to impaired cell mediated immunity (the desired effect to prevent kidney transplant rejection). A number of different preparations of ATG have been produced over the last few decades. These can be broadly divided into horse ATG (hATG), derived from horse serum after immunisation of horses with human thymocytes, and rabbit ATG (rATG), derived from rabbit serum. There are currently two or three standardised preparations available globally. Historical ATG preparations used in early studies were less standardised compared to the preparations currently available. Even though both hATG and rATG contain antibodies to a wide variety of T\u2010cell antigens and MHC antigens, it is likely that the effects are not equal given that the two types are prepared differently. One study assessing both efficacy and safety clearly showed differences between these two preparations ( Brennan 1999 ). Monomurab\u2010CD3 is a murine monoclonal antibody against the CD3 receptor on activated T cells (Orthoclone OKT3) which became available in the late 1980s. OKT3 removes the functional T\u2010cell population from circulation, producing immunosuppression useful for both induction therapy and the management of acute rejection. However, this profound immunosuppression is associated with immediate toxicity (cytokine release syndrome) and higher rates of infection and malignancy than standard triple therapy ( Soulillou 2001 ). Use of these preparations may also be limited by the development of neutralising antibodies to their xenogeneic components ( Kreis 1992 ). Use of OKT3 for both induction and treatment of acute rejection has declined in many countries over recent years due to the side effect profile. Janssen\u2010Cilag discontinued the manufacture of OKT3 in 2010 due to a combination of declining sales and evidence from a Cochrane review on treatment of acute rejection confirming that OKT3 was associated with increased side effects compared to newer biologic agents ( Webster 2006 ). More recently, the IL2Ra basiliximab and daclizumab have been used in the induction phase. IL2Ra are IgG monoclonal antibodies to the interleukin\u20102 receptor found only on activated T cells. IL2Ra are more specific immunosuppressants, with no immediate toxicity, and are increasingly used as induction agents, but not for treating acute rejection ( Cibrik 2001 ). These agents are investigated in a separate Cochrane review ( Webster 2010 ) and so will not be considered here. Other antibodies have also been introduced for kidney transplantation induction such as alemtuzumab. This humanised CD\u201052 specific complement fixing monoclonal antibody was first used for induction by Calne 1999 . Alemtuzumab causes profound depletion of T\u2010cells from peripheral blood and also less marked depletion of other mononuclear cells. Although the majority of current anti\u2010rejection therapies are targeted at T\u2010cell mechanisms, there is increasing evidence that B\u2010cells may have a role due to their ability to act as antigen presenting cells and T\u2010cell activators ( Zand 2007 ). For this reason the B\u2010cell depleting anti\u2010CD20 antibody, rituximab is also being used in kidney transplantation. Initially this was used in studies for ABO\u2010incompatible kidney transplants at induction ( Tyden 2003 ) but is now being considered for selected patients in some centres. Why it is important to do this review Favoured antibody preparations and rates of use differ from country to country and among transplant units. In 2007 in the USA, 78% of recipients received an antibody preparation as part of induction immunosuppression. Forty five per cent of kidney recipients received ATG, 1% OKT3, 27% IL2Ra and 10% received alemtuzumab ( UNOS 2011 ). In Australia, 93% of patients received an IL2Ra in 2008 and 5% to 10% received an additional or alternative antibody preparation ( ANZDATA 2009 ). There has clearly been an increase in use of antibody induction therapy over the last decade ( ANZDATA 2009 ; UNOS 2011 ) but there is still a large amount of variability in the type of antibody preparation used. This reflects local policies to some extent but there is also uncertainty, in particular in patients at high risk of rejection, as to whether one agent is superior to another. In patients at higher risk of rejection, increased risk of side effects may be acceptable if a treatment is more effective at reducing the risk of acute rejection, leading to improved rates of allograft and patient survival. The aim of this systematic review is to summarise the relative short and long\u2010term beneficial and adverse effects of different antibody preparations (except IL2Ra) used as induction in kidney transplant recipients. A previous Cochrane review looks at the use of antibodies for treatment of acute rejection episodes ( Webster 2006 ).",
        "summary": "Some antibody preparations used during induction therapy for kidney transplant recipients may decrease graft loss, delayed graft function and acute rejection, but produce a concomitant increase in adverse events such as infection, leukopenia, and thrombocytopenia. None of the available evidence was of high quality, making it difficult to draw firm conclusions. Randomized controlled trials reported that when compared with placebo/no induction, antibody preparations used during induction therapy for kidney transplant recipients (antithymocyte globulin [ATG], alemtuzumab, rituximab, and antilymphocyte globulin [ALG]) had no apparent impact on mortality; monomurab\u2010CD3 (OKT3) reduced mortality, but only over the short term (one to two years; on average, 25 vs 61 per 1000 people died). Only ATG reduced the number of people with graft loss (including loss due to death) at one to two years (on average, 201 vs 286 with graft loss per 1000 people), and only ALG reduced the number of people with delayed graft function (on average, 107 vs 195 per 1000 people), whereas ATG reduced acute rejection (on average, 195 vs 311 per 1000 people), as did OKT3 (on average, 361 vs 606 per 1000 people) and ALG (on average, 370 vs 533 per 1000 people). Data on adverse events show that infections were more common with antibody preparations, particularly cytomegalovirus infection, which occurred on average in 25 to 155 more people per 1000, depending on the preparation used. Leukopenia and thrombocytopenia were not consistently reported across comparisons; more people developed leukopenia with rituximab (on average, 115 vs 14 per 1000) or ATG (on average, 334 vs 86 per 1000 people), and more people developed thrombocytopenia with ALG (on average, 677 vs 56 per 1000 people) or ATG (on average, 78 vs 32 per 1000 people). Assessment of malignancy, new\u2010onset diabetes, and post\u2010transplant lymphoproliferative disorders revealed no differences between placebo and any of the antibody preparations, but event rates reported were very low."
    },
    "CD003861": {
        "query": "Is tap water sufficient for wound cleansing?",
        "document": "Background Management of chronic and acute wounds has changed significantly in the last decade; however, minimal attention has been focused on the types of solutions used for wound cleansing. The process of wound cleansing involves the application of a non\u2010toxic fluid to remove debris, wound exudate and metabolic wastes to create an optimal environment for wound healing ( Murphy 1995 ; Waspe 1996 ; Rodeheaver 1999 ). Clinicians and manufacturers have recommended various cleansing agents for their supposed therapeutic value. Preparations with antiseptic properties have been traditionally used, but published research using animal models has suggested that antiseptic solutions may hinder the healing process ( Brennan 1985 ; Thomlinson 1987 ; Glide 1992 ; Bergstrom 1994 ; Hellewell 1997 ). The controversy surrounding the use of antiseptics prompted the development of guidelines for the use of antiseptics by wound care experts. These guidelines have resulted in changes in hospital practice. Normal saline (0.9%) is the favoured wound cleansing solution because it is an isotonic solution and does not interfere with the normal healing process, damage tissue, cause sensitisation or allergies or alter the normal bacterial flora of the skin (which would allow the growth of more virulent organisms) ( Huxtable 1993 ; Lawrence 1997 ; Philips 1997 ; Joanna Briggs 1998 ). Tap water is also recommended and has the advantages of being efficient, cost\u2010effective and accessible ( Fowler 1985 ; Angeras 1992 ; Murphy 1995 ; Thompson 1999 ). However, clinicians have been cautioned against using tap water to cleanse wounds that have exposed bone or tendon, in which case normal saline is recommended ( Lindholm 1999 ). There has been much debate in clinical circles about the potential advantages and disadvantages of cleaning exudate from the wound, as the exudate itself may contain growth factors and chemokines which contribute to wound healing ( Thomson 1998 ). However, the literature also suggests that large amounts of bacteria may inhibit wound healing because of the proteases secreted by the organisms ( Robson 1988 ). Until further research has established its demerits, cleansing will continue to remain an integral part of the wound management process ( Hellewell 1997 ). Wounds cause considerable cost to individuals in terms of morbidity, and to the health services in terms of the personnel and consumables to perform wound care ( Johnson 1997 ). The purpose of this systematic review was to investigate the effectiveness of water for cleansing wounds in clinical practice.",
        "summary": "There were no apparent differences in terms of wound infection or proportion of wounds healed when wound cleansing with tap water (during showering) was compared with no wound cleansing (analyses in around 800 people), or cleansing with normal saline (analyses in around 1400 people, almost all adults). Most trials were conducted in the emergency department or hospital wards. Patients seemed to prefer showering to no cleansing or irrigation with normal saline.The trials included a wide range of wounds, including lacerations, surgical incisions, and chronic wounds; these would vary in their risk for infection and ability to heal, limiting the generalizability of the results to any particular type of wound. In addition, the methodology of the included trials was either unclear or poor, which would affect the confidence that could be placed in the results."
    },
    "CD008580": {
        "query": "In women undergoing amniocentesis or chorionic villus sampling, is there randomized controlled trial evidence to support the use of analgesia?",
        "document": "Background Description of the condition On average, between 5% and 10% of pregnant women have invasive tests for reassurance that their baby is well ( Eddleman 2006 ; Nicolaides 2005 ). Before the procedure, women want to be informed about the associated risk of amniocentesis or chorionic villous sampling (CVS) ( Mujezinovic 2007 ). Pain during the procedure is also an area of concern and women are often unclear about the amount of pain associated with the procedure. They also like to know whether maternal movements caused by such discomfort may complicate the procedure and increase the risk for the fetus ( Gordon 2007 ). It is often assumed that local anaesthesia is used during the procedure, or that drugs for pain relief will be offered ( Ferber 2002 ; Harris 2004 ; Van Schoubroeck 2000 ). Pain is a complex sensation that is difficult to measure and compare between individuals. Degree of pain is not directly related to the extent of tissue damage. There are different approaches available for quantifying pain, such as asking a series of questions regarding different aspects of pain ( Lowe 1987 ), or simple categorisation of pain experience ( Downie 1978 ). The visual analogue scale (VAS) is by far the most commonly used method for pain evaluation, because of its reliability and validity as a measurement tool ( DeLoach 1998 ; Mottola 1993 ; Price 1994 ). VAS can detect smaller differences in pain relative to other scales and it is easy to administer ( Banos 1989 ; Liu 1991 ; Niven 1996 ). In addition, VAS was proven to be a useful and valid measure of pre\u2010operative anxiety. It correlates with the classic measurement of anxiety by the State Anxiety Score of Spielberger ( Kindler 2000 ). Perception of pain differs among populations. Variables such as ethnicity and education could influence experience of pain ( Bates 1993 ; Lipton 1984 ; Streltzer 1981 ). Genetics and development, family, psychology, social status and culture also have a major part in the construction of pain perception and anxiety ( McGrath 1994 ; Turk 1999 ). Few studies have evaluated the intensity of the pain during amniocentesis. Severity of pain associated with amniocentesis was reported by Harris 2004 , with 19% of women reporting no pain, but 7% of women describing pain as distressing or horrible, 43% as mild and 31% as discomforting. The mean intensity of pain was 1.6+/\u20101.3 (on a scale of zero to seven) ( Harris 2004 ). Moderate or severe pain was experienced by 2% of the interviewees undergoing amniocentesis in De Crespigny 1990 . Ferber 2002 observed that the levels of pain and anxiety expected to be present during the procedure were higher than actual level of pain and anxiety experienced. The authors recommended that pre\u2010amniocentesis counselling should emphasise the fact that, for most women, the actual pain and anxiety experienced during the procedure is significantly lower than expected. Results of the study also demonstrated that for women who had had prior amniocentesis, the expectation and experience of pain were significantly reduced. The technical degree of difficulty was the only significant variable impacting on the actual pain and anxiety ( Ferber 2002 ). In contrast, women had higher pain than expected in the study by Harris 2004 . The same was true when the needle had to be inserted in the lower third of the uterus. Maternal weight, parity, previous surgery, fibroid tumours, and depth of needle insertion were not correlated with perceived pain. Presence or absence of an accompanying person was not associated with pain intensity. Anxiety and pain were increased when abnormal serum screen was the indication compared with women with advanced maternal age as the main indication ( Harris 2004 ). Beeson 1979 reported higher levels of anxiety in women undergoing amniocentesis after having delivered an anomalous fetus relative to women whose procedures were performed due to advanced maternal age. Description of the intervention Currently, different types of local analgesia are applied before a wide range of procedures in medicine and surgery. The primary aim is to decrease the pain and suffering of the patients. Local analgesia could be categorised in two groups \u2010 non\u2010pharmacological and pharmacological agents. Acupuncture is an example of non\u2010pharmacological analgesia where acupuncture needles are inserted at certain acupuncture points on the body. This activates small myelinated nerve fibres in the muscle which transmit impulses to the spinal cord and then activate three centres in order to produce analgesia: the spinal cord, midbrain and pituitary/hypothalamus. Transcutaneous electrical nerve stimulation (TENS) is another a non\u2010pharmacological agent, based on delivering low voltage electrical currents to the skin. TENS is used for the treatment of a variety of pain conditions. In a Cochrane review about the effects of non\u2010pharmacological methods of treating dysmenorrhoea, Proctor 2002 found that high\u2010frequency TENS was effective for the treatment of dysmenorrhoea by a number of small trials. Walsh 2009 analysed effectiveness of TENS as an isolated treatment for acute pain in adults such as cervical laser treatment, venipuncture, screening flexible sigmoidoscopy and non\u2010procedural pain, e.g. postpartum uterine contractions and rib fractures. Despite 12 included randomised controlled trials (RCTs) with 919 participants, the authors were unable to draw any definitive conclusions about the effectiveness of TENS as an isolated treatment for acute pain in adults. East 2007 evaluated local cooling treatments (ice packs, cold gel pads or cold/iced baths), hamamelis water (witch hazel), pulsed electromagnetic energy, hydrocortisone/pramoxine foam (Epifoam) or warm baths for perineal pain. Authors concluded that there is only limited evidence to support the effectiveness of local cooling treatments (ice packs, cold gel pads, cold/iced baths) applied to the perineum following childbirth to relieve pain. Two other Cochrane reviews evaluated topical agents for pain relief in acute otitis media ( Foxlee 2006 ) and venous leg ulcers ( Biggs 2003 ) without conclusive evidence of benefit. Uman 2006 evaluated psychological interventions for needle\u2010related procedural pain and distress in children and adolescents in 28 trials with 1951 participants. The most commonly studied needle\u2010procedures were immunisations and injections. The largest effect sizes where seen for distraction, combined cognitive\u2010behavioral and hypnosis with promising but limited evidence for the efficacy of numerous other psychological interventions, such as information/preparation, nurse coaching plus distraction, parent positioning plus distraction, and distraction plus suggestion. Another useful approach to pain reduction could be the change of the needle temperature. Denkler 2001 concluded that needles stored at \u20107 C significantly reduced pain during facial intramuscular botulinum toxin injections. How the intervention might work Local anaesthetics cause reversible local anaesthesia and a loss of nociception. They can be divided into two classes: amino amide and amino ester local anaesthetics. Both alter the resting potential of the nerve, acting as a membrane stabilising drugs ( Yanagidate 2007 ). Cryoneurolysis involves using cold to induce nerve injury, thus providing analgesia. A second\u2010degree injury involves degeneration of axon and myelin sheets from the site of freezing distally to the nerve's termination. The return of normal sensory and motor activity depends primarily of two factors: the rate of axonal regrowth and the distance of a cryolesion from the end organ. Cryoneurolysis is best suited for conditions in which the nerve is small and well localised ( Warfield 2004 ). One of the prevailing theories behind massage is that musculofascial pain is caused by a self\u2010perpetuating neuromuscular feedback circuit in which the stimulation of touch interferes, thus restoring normal function. The intervention may take the form of ischaemic compression, passive stretching, passive shortening or any simultaneous or sequential combination of these ( Clay 2006 ). Reflexology is another method of alternative medicine which involves practice of massaging, squeezing or pushing on parts of feet or sometimes the hands and ears, with the goal of encouraging a beneficial effect or relieving pain on other parts of the body, or to improve general health ( Norman 1989 ). Why it is important to do this review Amniocentesis is a rather frequent procedure in clinical obstetrics practice. It is associated with anxiety and fear of pain that can be expected during the procedure. It is therefore important to determine whether different approaches to pain and anxiety reduction work and which of the various methods is most effective. Women often expect that some kind of analgesia will be offered; this expectation is based on an assumption that analgesia must relieve some of the discomfort associated with a procedure.",
        "summary": "There is insufficient randomized controlled trial evidence from single studies assessing a range of topical interventions (lidocaine injection or cream, light leg rubbing or subfreezing needles) to draw conclusions on the benefits and harms of analgesia during amniocentesis . None of the studies found evidence of a difference between intervention and placebo or no intervention groups with regard to anxiety score and pain, although all were limited by lack of power. None of the studies assessed adverse effects. There were no studies assessing analgesia during chorionic villus sampling."
    },
    "CD006812": {
        "query": "How do different adjuvant radiotherapy and/or chemotherapy regimens compare in women who have received surgery for uterine carcinosarcoma?",
        "document": "Background Description of the condition Uterine carcinosarcomas are uncommon accounting for 4.3% of all cancers of the uterine corpus in Western Populations ( El Nashar 2011 ; Young 1981 ). The worldwide annual incidence is between 0.5 and 3.3 cases per 100,000 women ( Harlow 1986 ; Brooks 2004 ). In the UK the incidence of sarcoma is quoted to be 1 per 100,000 women and of these, 87% are carcinosarcomas ( Olah 1992 ). Uterine carcinosarcomas, also called malignant mixed mesodermal tumours (MMT) or malignant mixed mullerian tumours (MMMT) are rare tumours with both malignant epithelial and malignant mesenchymal components. Surveillance Epidemiology and End Results (SEER) programme data also demonstrated that carcinosarcoma was the predominant uterine sarcoma (0.82/100,000) followed by leiomyosarcoma (0.64/100,000) and endometrial stromal sarcoma (0.19/100,000) ( Harlow 1986 ). Uterine carcinosarcomas tend to be aggressive with poor prognosis in comparison to uterine adenocarcinomas ( Barwick 1979 ; Gadducci 2002 ; Toyoshima 2004 ). About 35% of carcinosarcomas are not confined to the uterus at diagnosis, and in most reports the median overall survival was about 21 months ( Gadducci 2002 ). The most important prognostic factor is the extent of the tumour at the time of diagnosis; the prognosis being very poor when the tumour has extended beyond the uterus ( Sartori 1997 ). There has been no significant improvement in survival suggested by some reports ( Callister 2004 ; Chi 1997 ; Le 2001 ; Sutton 2000 ). Carcinosarcomas can be subdivided histologically into homologous and heterologous types and it is important to differentiate the tumours that are monoclonal, that is those derived from a single stem cell, from true mixed cell tumours ( Zelmanowicz 1998 ). This histological distinction ( McCluggage 2002 ) is significant as the natural history of true mixed carcinosarcomas appear to be more aggressive. There is convincing recent evidence that most cases of uterine carcinosarcoma are monoclonal in origin ( Szukala 1999 ; Toyoshima 2004 ). These data indicate that uterine carcinosarcoma may be metaplastic, with the implication that the sarcomatous components are derived from the carcinomatous elements ( McCluggage 2002 ). Histological diagnosis and clinical staging (based on findings at surgery) usually follows primary treatment which is surgical. In a prospective multi\u2010centre Gynecologic Oncology Group (GOG) study of carcinosarcomas 61 of the 301 patients (20%) with clinical Stage I and II disease were reassigned to pathological Stages III and IV on the basis of lymph node metastases. The study also revealed a recurrence rate of 53% for all carcinosarcomas, with 44% for homologous and 63% for heterologous tumours ( Major 1993 ). Description of the intervention As with uterine adenocarcinomas, the mainstay of treatment is surgical removal of the tumour ( Menczer 2005 ), however, the high rates of both local and distant relapse after surgery suggests a need for effective adjuvant therapies ( Galaal 2009 ; Sutton 2000 ). The survival of patients with advanced uterine carcinosarcoma is poor with a pattern of failure indicating a higher likelihood of upper abdominal and distant metastatic recurrence ( Galaal 2009 ; Spanos 1986 ). These patients are less likely to benefit from local adjuvant therapy and therefore consideration for systemic adjuvant chemotherapy as well as whole body irradiation has been considered in several studies ( Chi 1997 ; Menczer 2005 ; Ramondetta 2003 ; Sutton 1989 ). How the intervention might work Several chemotherapeutic agents have been examined as single agent therapy in uterine carcinosarcoma with response rates as follows: 16% to19% with adriamycin ( Omura 1983 ), 32% to 36% with ifosfamide ( Sutton 1989 ; Sutton 2000 ), 19% with cisplatin ( Thigpen 2004 ), and 18% with paclitaxel ( Curtin 2001 ). Doxorubicin, despite being established in the treatment of uterine carcinoma, does not seem to be highly active in uterine carcinosarcoma ( Omura 1983 ). Combination chemotherapeutic agents have been used in uterine carcinosarcoma with combination therapy appearing to be superior to single\u2010agent treatment in terms of improvement in progression\u2010free and overall survival. However, these combination therapies may be associated with increased toxicity ( Homesley 2007 ; Sutton 2000 ; Van Rijswijk 1994 ). Whole abdominal irradiation has been investigated in a retrospective study on early staged uterine carcinosarcoma in the adjuvant setting. This study suggested that the addition of whole abdominal irradiation did not improve survival ( Chi 1997 ). Why it is important to do this review Carcinosarcoma is a disease with a high recurrence rate (40% to 60%), and a tendency to distant metastasis, therefore, an effective systemic therapy may improve the outcomes of this disease. Several chemotherapeutic agents have been shown to produce objective response rates in patients with advanced carcinosarcoma. In addition, whole abdominal irradiation has been used in the adjuvant setting ( Chi 1997 ; Ramondetta 2003 ). These treatment modalities may be associated with some costs in terms of toxicity and quality of life (QoL). Therefore, there is a need to assess the effectiveness and safety rigorously.",
        "summary": "Combination chemotherapy may improve the survival of women who have undergone surgery for advanced/metastatic uterine carcinosarcoma. In particular, two RCTs (including around 350 women) showed that mortality and disease progression were lower in women who were administered combination chemotherapy (including ifosfamide) compared with ifosfamide alone. In addition, combination chemotherapy was not associated with higher toxicity rates, except for nausea and vomiting (on average, 140 vs 40 per 1000 women). As regards radiation therapy, a single RCT with around 200 participants found little or no difference between combination chemotherapy and whole body irradiation in overall or progression\u2010free survival but noted that irradiation caused lower rates of toxicity."
    },
    "CD004896": {
        "query": "What are the effects of antifibrinolytic drugs on outcomes in people with acute traumatic injury?",
        "document": "Background Description of the condition For people aged five to 45 years, trauma is second only to HIV/AIDS as a cause of death. Each year, worldwide, about four million people die as a result of traumatic injuries and violence ( GBD 2013 ). Approximately 1.6 million of these deaths occur in hospital and about one third of these deaths (480,000) are from haemorrhage ( Ker 2012 ). Among trauma patients who do survive to reach hospital, exsanguination is a common cause of death, accounting for nearly half of in\u2010hospital trauma deaths in some settings ( Sauaia 1995 ). Central nervous system injury and multi\u2010organ failure account for most of the remainder, both of which can be exacerbated by severe bleeding ( BTF 2000 ). Clotting helps to maintain the integrity of the circulatory system after vascular injury, whether traumatic or surgical in origin ( Lawson 2004 ). Major surgery and trauma trigger similar haemostatic responses and the consequent massive blood loss presents an extreme challenge to the coagulation system. Part of the response to surgery and trauma in any patient, is stimulation of clot breakdown (fibrinolysis) which may become pathological (hyper\u2010fibrinolysis) in some cases. Antifibrinolytic agents have been shown to reduce blood loss in patients with both normal and exaggerated fibrinolytic responses to surgery, without apparently increasing the risk of post\u2010operative complications. Description of the intervention Antifibrinolytic agents are widely used in major surgery to prevent fibrinolysis (lysis of a blood clot or thrombus) and reduce surgical blood loss. Antifibrinolytic agents considered within this review include aprotinin, tranexamic acid (TXA), epsilon\u2010aminocaproic acid and aminomethylbenzoic acid. How the intervention might work Antifibrinolytic agents work by preventing blood clots from breaking down. The blood clots help to reduce excessive bleeding. Fewer people die from blood loss, or from there being too little blood in the circulatory system to keep the heart functioning normally. Because the coagulation abnormalities that occur after injury are similar to those after surgery, it is possible that antifibrinolytic agents might also reduce blood loss and mortality following trauma. Why it is important to do this review A simple and widely practicable intervention that reduced blood loss following trauma might prevent tens of thousands of premature deaths. A reduction in the need for transfusion would also have important public health implications. Blood is a scarce and expensive resource and major concerns remain about the risk of transfusion\u2010transmitted infection. Trauma is particularly common in parts of the world where the safety of blood transfusion cannot be assured. A study in Uganda estimated the population\u2010attributable fraction of HIV acquisition as a result of blood transfusion to be around two per cent ( Kiwanuka 2004 ), although some estimates are much higher ( Heymann 1992 ). A systematic review ( Henry 2011 ) of randomised controlled trials of antifibrinolytics (mainly aprotinin or TXA) in elective surgical patients showed that antifibrinolytics reduced the numbers receiving transfusion by one third, reduced the volume needed per transfusion by one unit, and halved the need for further surgery to control bleeding. These differences were all statistically significant at the P < 0.01 level. Specifically, aprotinin reduced the rate of blood transfusion by 34% (relative risk [RR] = 0.66; 95% confidence interval [CI] 0.60 to 0.72) and TXA by 39% (RR = 0.61; 95% CI 0.53 to 0.70). Aprotinin use saved 1.02 units of red blood cells (RBCs) (95% CI 0.79 to 1.26) in those requiring transfusion, and TXA use saved 0.87 units (95% CI 0.53 to 1.20). There was a non\u2010significant reduction in mortality with both aprotinin (RR = 0.81; 95% CI 0.63 to 1.06) and TXA (RR = 0.60; 95% CI 0.33 to 1.10). This review is an update of a Cochrane review first published in 2004 ( Coats 2004 ; Roberts 2004 ) and was updated in 2010 ( Roberts 2012 ). The review considers a different population group (trauma patients only) than the review conducted by Henry et al described above. In the 2012 update, we concluded that TXA safely reduces mortality in bleeding trauma patients without increasing the risk of adverse events; and that it should be given as early as possible and within three hours of injury, as treatment later than this is unlikely to be effective. Trauma is one of the leading causes of injury and death worldwide. This review will continue to be updated since antifibrinolytic agents are being given to patients and it is important that patients are given treatments based on current research evidence, and to respond to methodological advances in the analysis of evidence identified previously. The review will be updated again in the future as new research is published.",
        "summary": "Antifibrinolytic agents are widely used in major surgery to prevent fibrinolysis and reduce surgical blood loss in trauma patients with a risk of serious bleeding. Tranexamic acid is more effective than placebo for reducing mortality. High\u2010quality evidence suggests that mortality due to any cause, bleeding or myocardial infarction were reduced with tranexamic acid when compared with placebo in adults who suffered trauma and had, or were at risk of, serious bleeding. A single trial with 20,211 participants reported similar incidences for the two groups for deaths caused by vascular occlusion, stroke, pulmonary embolism, multi\u2010organ failure, or head injury, as well as the number of people requiring surgical intervention or blood transfusion. There is some indication that the timing of tranexamic acid administration may be important; subgroup analyses showed there to be fewer deaths due to bleeding when tranexamic acid was administered within the first three hours after injury, but perhaps no benefit when administered after this time."
    },
    "CD005122": {
        "query": "Does evidence from randomized controlled trials support the use of cardiotocography in pregnant women instead of intermittent auscultation of fetal heart rate?",
        "document": "Background Assessment of fetal wellbeing throughout pregnancy, labour and birth is widely regarded as a fundamental component of maternity care and essential for optimising fetal outcomes. Although a variety of methods are used to assess fetal wellbeing, including fetal movement counting and biophysical tests such as Doppler ultrasound, monitoring of the fetal heart rate (FHR) remains the most common method for the assessment of fetal wellbeing ( Alfirevic 2013 ; NCCWCH 2007 ). The FHR undergoes constant changes in response to changes in the intrauterine environment and to other stimuli such as uterine contractions. These changes in the FHR can be monitored to assess the wellbeing of the fetus during pregnancy and labour. Description of the condition Two common methods of monitoring the FHR are by intermittent auscultation and by an electronic fetal monitoring (EFM) machine that produces a printout called a cardiotocograph (CTG) ( Ayres\u2010de\u2010Campos 2015 ). Intermittent auscultation involves listening to the fetal heart at predetermined intervals using either a Pinard stethoscope or a hand\u2010held Doppler ultrasound device. The CTG is a graphical printout of the FHR and uterine contractions. The FHR recorded on a CTG may be captured externally via an ultrasound transducer attached to the mother's abdomen, or internally via a fetal scalp electrode placed directly on the baby's head. Uterine contractions are recorded via a pressure transducer attached to the mother's abdomen or, less commonly, by an intrauterine pressure device placed in the uterine cavity ( Ayres\u2010de\u2010Campos 2015 ). Description of the intervention The admission CTG is a commonly\u2010used screening test consisting of a short, usually 20 minute, recording of the FHR and uterine activity performed on the mother's admission to the labour ward with signs of labour ( Cheyne 2003 ; Impey 2003 ; Mires 2001 ). Anecdotally, some women will have an admission CTG performed before assessments aimed at diagnosing the onset of labour, while others will not have the admission CTG until a diagnosis of labour has been established. The implications of this are that some women will have an admission CTG performed on admission to the labour ward or labour assessment room where, on subsequent assessment, a diagnosis of not being in labour is made. Differences in timing of the admission CTG with respect to the onset of labour may result in differences in outcomes assessed. We planned to explore this through subgroup analysis (see Subgroup analysis and investigation of heterogeneity ). How the intervention might work Pioneered in the 1950s and 1960s as an alternative to intermittent auscultation of the FHR by stethoscope or Pinard ( Caldeyro\u2010Barcia 1966 ; Hammacher 1968 ; Hon 1958 ), EFM was introduced into widespread clinical practice in the 1970s to 1980s on the premise that it would facilitate early detection of abnormal FHR patterns thought to be associated with hypoxia (lack of oxygen), to enable earlier intervention to prevent fetal neurological damage and death or both ( Nelson 1996 ). However, because antenatal risk factors do not identify all fetuses who will subsequently experience morbidity, mortality, or both, the admission CTG was introduced as a means of attempting to identify those fetuses of low\u2010risk mothers at greatest risk of intrapartum hypoxia ( Arulkumaran 2000 ; RCOG 2001 ) who might benefit from more intensive monitoring by continuous EFM and fetal scalp blood gas analysis or both, or from immediate intervention (e.g. expedited birth). Current prevalence rates of perinatal mortality, neonatal encephalopathy and cerebral palsy are relatively low and, of those, only a small proportion are thought to be attributable directly to intrapartum causes ( RCOG 2001 ). Changes in FHR patterns are neither sensitive (the ability of a test to identify those who have the disease or condition) nor specific (the ability of the test to correctly identify those without the disease or condition) to any particular cause ( MacLennan 1999 ). Multiple late decelerations and decreased FHR variability have been shown to be associated with an increased risk of cerebral palsy ( Nelson 1996 ). However, the associated false positive rate is reported as high as 99.8% in the presence of tracings displaying these abnormalities in the FHR pattern ( Nelson 1996 ). This poor positive predictive value implies that to identify the fetus who may be compromised, EFM identifies abnormal FHR patterns in many healthy fetuses who are not truly compromised. Why it is important to do this review There is a lack of evidence of benefit supporting the use of the admission CTG in low\u2010risk pregnancy. Despite recommendations that it should not be recommended for this group of women ( Liston 2007 ; NCCWCH 2007 ; RCOG 2001 ), the admission CTG was used by approximately 79% of maternity units in the UK in 2000 ( CESDI 2001 ), by 96% of units in Ireland in 2004 ( Devane 2007 ) and by approximately 76% of Canadian hospitals ( Kaczorowski 1998 ). More recently, the admission CTG was used in all (100%, n = 42) labour units in Sweden in 2008 ( Holzmann 2010 ). Although the admission CTG remains in widespread use, several issues remain controversial. These include whether the admission CTG (a) should be offered routinely to all women without risk factors for intrapartum hypoxia; (b) whether the admission CTG is effective at predicting those fetuses who will subsequently develop intrapartum hypoxia; and (c) the effect of the admission CTG on neonatal mortality and on maternal and neonatal morbidity. It was important to undertake this systematic review to explore these issues and to evaluate the efficacy of admission CTG compared to intermittent auscultation as a method of assessing fetal wellbeing in women on admission to the labour ward, or labour assessment room, with signs of possible labour. This review complements other Cochrane systematic reviews evaluating the effectiveness of other interventions for the assessment of fetal wellbeing including the following. Amniotic fluid index versus single deepest vertical pocket as a screening test for predicting adverse pregnancy outcomes ( Nabhan 2008 ). Antenatal cardiotocography for fetal assessment ( Grivell 2015 ). Biochemical tests for placental function for assessment in pregnancy ( Neilson 2012 ). Biophysical profile for fetal assessment in high\u2010risk pregnancies ( Lalor 2008 ). Fetal and umbilical Doppler ultrasound in high\u2010risk pregnancies ( Alfirevic 2013 ). Fetal manipulation for facilitating tests of fetal wellbeing ( Tan 2013a ). Fetal movement counting for assessment of fetal wellbeing ( Mangesi 2015 ). Fetal vibroacoustic stimulation for facilitation of tests of fetal wellbeing ( Tan 2013b ). Maternal glucose administration for facilitating tests of fetal wellbeing ( Tan 2012 ). Regimens of fetal surveillance for impaired fetal growth ( Grivell 2012 ). Utero\u2010placental Doppler ultrasound for improving pregnancy outcome ( Stampalija 2010 ). Vibroacoustic stimulation for fetal assessment in labour in the presence of a non\u2010reassuring FHR trace ( East 2013 ). Amniotic fluid index versus single deepest vertical pocket as a screening test for predicting adverse pregnancy outcomes ( Nabhan 2008 ). Antenatal cardiotocography for fetal assessment ( Grivell 2015 ). Biochemical tests for placental function for assessment in pregnancy ( Neilson 2012 ). Biophysical profile for fetal assessment in high\u2010risk pregnancies ( Lalor 2008 ). Fetal and umbilical Doppler ultrasound in high\u2010risk pregnancies ( Alfirevic 2013 ). Fetal manipulation for facilitating tests of fetal wellbeing ( Tan 2013a ). Fetal movement counting for assessment of fetal wellbeing ( Mangesi 2015 ). Fetal vibroacoustic stimulation for facilitation of tests of fetal wellbeing ( Tan 2013b ). Maternal glucose administration for facilitating tests of fetal wellbeing ( Tan 2012 ). Regimens of fetal surveillance for impaired fetal growth ( Grivell 2012 ). Utero\u2010placental Doppler ultrasound for improving pregnancy outcome ( Stampalija 2010 ). Vibroacoustic stimulation for fetal assessment in labour in the presence of a non\u2010reassuring FHR trace ( East 2013 ).",
        "summary": "In low\u2010risk pregnancy, cardiotocography as a labor admission test does not appear to offer any advantage over intermittent fetal auscultation for reliably predicting fetal resilience in labor. Its use increases cesarean section rates without seeming to improve neonatal outcomes. Moderate\u2010quality evidence showed higher rates of cesarean delivery among women receiving cardiotocography compared with intermittent auscultation (on average, 44 per 1000 women with admission cardiotocography vs 36 per 1000 women with intermittent auscultation) and similar rates of fetal and neonatal death between women receiving one intervention or the other. Low\u2010quality evidence suggested a similar number of instrumental vaginal births in both groups, and four trials including 11,000 participants found no evidence of a difference between interventions in admission to the neonatal intensive care unit, infants with Apgar score less than 7 at or after five minutes, or neonatal hypoxic ischemic encephalopathy (very low\u2010quality evidence). No trials assessed severe neurodevelopmental disability at 12 months of age or older, nor did investigators assess the incidence of serious maternal complications."
    },
    "CD011850": {
        "query": "For pregnant women, what are the effects of fluoride supplementation to prevent dental caries in the primary teeth of their children?",
        "document": "Background Description of the condition Dental caries (tooth decay) is one of the most common chronic childhood diseases, and can have a negative impact on a child's growth, speech, self\u2010confidence, general health and quality of life ( Anderson 2004 ; Casamassimo 2009 ; Jankauskiene 2010 ). Dental caries occurs because of long\u2010term exposure to a mixture of acid\u2010producing bacteria and fermentable carbohydrates, and many other factors that include saliva secretion rate and buffering capacity ( Rozier 2010 ; Selwitz 2007 ). In particular, acid production from bacteria, especially mutans Streptococci and Lactobacilli , and the subsequent decrease in local pH, cause the demineralization of tooth tissue ( Featherstone 2004 ). If this process is not reversed, carious lesion progresses. Dissolved calcium and phosphate mineral ions can be redeposited on the tooth surface even though they are provided from saliva. This demineralization/remineralization process occurs continuously in oral fluids. Dental caries is generated when the demineralization/remineralization process lose the balance. Dental caries is widespread in all countries ( WHO 2009 ). Dental caries affects 30% to 50% of children aged 5 to 6 years ( Armfield 2009 ; CDC 2007 ; Public Health England 2012 ), 60% to 90% of school\u2010aged children and a large majority of adults ( Petersen 2005 ). An increasing number of decayed, missing or filled teeth (dmft) is reported in some low\u2010income and middle\u2010income countries ( Bagramian 2009 ). Although the mean dmft has declined in many high\u2010income countries, this disparity remains and some population groups have high dmft ( Bagramian 2009 ; Dawkins 2013 ; Jones 2017 ). Prevalence of dental caries is associated with increased consumption of sugar in the diet including high\u2010sugar confectionery and sweet carbonated beverages ( Ismail 1997 ), socioeconomic status, dental insurance coverage and residential locations ( Campus 2009 ; Dawkins 2013 ; Hobdell 2003 ; Kolker 2007 ). Several studies have reported that caries in primary teeth are correlated with caries in permanent teeth ( Helfenstein 1991 ; Li 2002 ; Seppa 1989 ). 'Early childhood caries', which is the presence of at least one carious lesion on a primary tooth in a child under the age of 6 years, is a serious problem in the world ( Anil 2017 ). If the fluoride supplementation in pregnant woman is effective to prevent dental caries in the primary teeth of their children, the prevalence of dental caries can be prevented. Description of the intervention The benefits of topical fluorides, such as toothpastes, gels, varnishes and mouthrinses, for preventing dental caries in children and adolescents are well established ( Marinho 2003a ; Marinho 2003b ; Marinho 2013 ; Marinho 2015 ; Marinho 2016 ). Topical fluoride plays an important role in preventing dental caries ( Marinho 2003b ) including the inhibition of demineralization of the crystal surfaces, the enhancement of remineralization in demineralized lesions ( Featherstone 1990 ) and inhibition of bacterial metabolism ( Featherstone 1999 ; Featherstone 2000 ; Fejerskov 2004 ). The American Dental Association (ADA) and Centers for Disease Control and Prevention (CDC) state that an appropriate concentration of fluoride, which varies depending on age, is effective to prevent dental caries ( CDC 2001 ; Rozier 2010 ). The upper limit of fluoride intake from all sources (fluoridated water, food, beverages, fluoride dental products and dietary fluoride supplements) is set at 0.10 mg/kg/day for infants, toddlers, and children through to 8 years old. For older children and adults, who are no longer at risk for dental fluorosis, the upper limit of fluoride is set at 10 mg/day regardless of weight ( Levy 1999 ). CDC reported water fluoridation is especially beneficial for communities of low socioeconomic status ( CDC 1999 ). It should be noted that separation of pre\u2010eruptive, post\u2010eruptive, systemic or topical effects of fluoride is impossible. Fluoridated water may have all effects. Topically applied fluorides are not intended for ingestion but can be swallowed unintentionally. Fluoride drops, lozenges or chewing gums are sucked or chewed to provide topical fluoride and ingested to provide systemic fluoride. Fluoride supplements taken during pregnancy have the effect of prenatal fluoride and possibly reduce dental caries in offspring ( Stephen 1993 ). However, risks associated with exposure to fluoride during pregnancy including miscarriage, premature delivery and premature birth were also reported ( Diouf 2012 ; Sastry 2011 ). How the intervention might work The enamel formation process is composed of two principal stages: secretory stage and maturation stage. In the former stage, ameloblasts produce protein matrix (predominantly amelogenins) and crystallites are deposited in the protein matrix. In the latter stage, ameloblasts transport the substances used in enamel formation out of the enamel. Excess water and organic materials are removed and mineral is transported into the tissue in order to achieve full mineralization of enamel ( Hiller 1975 ; Termine 1980 ). Fluoride can be transported from maternal serum to the fetus and prenatal deciduous enamel through the placenta ( Toyama 2001 ). The mechanism of fluoride placental transfer is controversial; there are some hypotheses that the placenta allows passive diffusion of fluoride from mother to fetus, while others suggest that the placenta acts as a barrier. It has been reported that the placenta allows passive diffusion of fluoride from mother to fetus when fluoride intake is low and that the placenta acts as a selective barrier when fluoride intake exceeds a particular level ( Gupta 1993 ; Toyama 2001 ). In the development of enamel formation, fluoride is incorporated into the crystal lattice and binds to calcium, which is contained within the protein matrix. As a result, fluoride alters crystal formation in the enamel matrix and strengthens the properties of the enamel crystals ( Tanimoto 2008 ). All primary tooth enamel and some permanent tooth enamel start to grow and continue to develop in utero ( Kraus 1965 ). Most of the permanent tooth enamel start to grow after birth. It has been reported that while the effect of fluoride supplements after birth was unclear on deciduous teeth, it was associated with a reduction in caries increment in permanent teeth ( Tubert\u2010Jeannin 2011 ). Namely, it might be suggested that postnatal systemic fluoride did not attribute to caries prevention in primary teeth. In regions without water fluoridation, fluoride supplementation for pregnant women may be an effective way to increase fluoride intake during pregnancy. If fluoride supplements taken by pregnant women improve neonatal outcomes, pregnant women with no access to a fluoridated drinking water supply can obtain the benefits of systemic fluoridation. Why it is important to do this review Recent systematic reviews and Cochrane Reviews have evaluated evidence regarding the effectiveness and safety of fluoride treatment and the adverse effects of high fluoride exposure in children and adults ( Choi 2012 ; Iheozor\u2010Ejiofor 2015 ; Ismail 2008 ; Marinho 2003a ; Marinho 2003b ; Marinho 2013 ; Marinho 2015 ; Marinho 2016 ; Tubert\u2010Jeannin 2011 ; Wong 2010 ). However, it remains uncertain whether fluoride supplementation in pregnant women is effective in preventing dental caries in their offspring. Currently, no systematic reviews have investigated the effectiveness and safety of this intervention. This Cochrane Review aims to address this gap in the literature and assess the current available evidence on fluoride supplementation during pregnancy.",
        "summary": "Data do not suggest that fluoride supplements for pregnant women can prevent dental caries and fluorosis in their children. However, these findings are based on only one randomized controlled trial (RCT), and the quality of evidence is very low. One RCT showed no clear differences between children whose mothers received fluoride supplements or placebo during pregnancy, in terms of caries in the primary teeth at three years (very low\u2010quality evidence) or at five years (one RCT with 798 participants). Similarly, fluoride supplements had no apparent impact on dental caries in children, as measured by the number of decayed or filled primary tooth surfaces (separately or combined) at three years (one RCT with 938 participants) and at five years (one RCT with 798 participants). Reviewers found no sign that fluoride supplements can increase fluorosis in mandibular teeth (one RCT with 798 participants) or in maxillary teeth (very low\u2010quality evidence) at five years; trialists did not report on any other adverse effects."
    },
    "CD011066": {
        "query": "Is there randomized controlled trial evidence to support the use of dehydroepiandrosterone in women in the postmenopausal phase?",
        "document": "Background Description of the condition The menopause is said to have occurred once there is permanent cessation of menstruation. Prior to the final menstruation there is usually a gradual reduction in the frequency of menstrual periods, known as the perimenopause. Postmenopause is defined as the period of time following on from 12 months after a woman has experienced her last menstruation. During perimenopause there is a fluctuation in estrogen levels due to decreasing ovarian follicular response ( Hoffman 2012 ; Rosen 2011 ; Speroff 2005 ). Most women become menopausal between 45 and 55 years of age, however there are also women who reach menopause at an earlier age for various reasons (for example premature ovarian insufficiency (sometimes due to chemotherapy) or bilateral oophorectomy). Menopausal women can develop a range of symptoms including vasomotor symptoms such as hot flushes and night sweats,and vaginal atrophy leading to vaginal dryness and dyspareunia, which can impact on sexual function ( Genazzani 2002 ; Speroff 2005 ). Hormone therapy (HT) (estrogen alone or in combination with a progestin) is currently indicated for the treatment of menopausal symptoms. However, HT has been associated with a significant increase in the risk of various conditions including breast cancer, venous thromboembolism and stroke ( Cameron 2005 ; Manson 2013 ; Marjoribanks 2012 ; Rossouw 2002 ; State\u2010of\u2010the\u2010Science\u2010Panel 2005 ; Taylor 2011 ). Description of the intervention Dehydroepiandrosterone (DHEA) is one of the main precursor sex steroids. DHEA is synthesized from cholesterol in the zone reticularis of the adrenal gland. It is converted to estrogens and testosterone by steroidogenic enzymes expressed in peripheral tissues such as the skeleton, breasts and ovary ( Figure 1 ). DHEA peaks at the age of 25 years and then slowly declines to approximately 30% of the initial levels at postmenopause ( Genazzani 2002 ; Rosen 2011 ; Speroff 2005 ). In the United States of America (USA) DHEA can be purchased without prescription but in most countries it is only available by prescription. DHEA may be administered orally, intravaginally or by alternative routes of administration (for example transdermal patches). Biosynthesis of DHEA and estrogens. As DHEA and eventually estrogen can be synthesized from cholesterol, levels of circulating estrogen and DHEA may differ in overweight women (body mass index (BMI) > 25 kg/m 2 ) compared to women with a normal weight (BMI 18 to 25 kg/m 2 ) or underweight women (BMI < 18 kg/m 2 ) ( Buster 2000 ; Miller 2002 ). Therefore, administered DHEA may have differing effects on women who are over\u2010 or underweight compared to those with a normal BMI. Some inflammatory diseases (for example systemic lupus erythematosus, Sj\u00f6gren syndrome) are associated with low circulating DHEA levels ( Sawalha 2008 ). DHEA levels may correlate with disease activity, therefore DHEA supplementation may have a different effect in women with and without inflammatory disease. How the intervention might work Due to a fluctuation and eventual decrease in estrogen levels (estradiol in particular), women can develop various perimenopausal and postmenopausal symptoms. While the body is adjusting to these hormonal changes women may experience symptoms such as hot flushes and night sweats. Low estradiol levels can cause vaginal dryness, which may lead to diminished sexual function ( Hoffman 2012 ; Speroff 2005 ). All of these symptoms may cause a decrease in the general wellbeing of peri\u2010 and postmenopausal women. It is suggested that testosterone increases sexual desire and sexual function ( Davis 2008 ; Pluchino 2013 ). As DHEA is one of the main precursors of androgens, which in turn are converted to testosterone and estrogens ( Figure 1 ) ( Simpson 2003 ; Vanson 1996 ), it is possible that the administration of DHEA may increase estrogen and testosterone levels in peri\u2010 and postmenopausal women to alleviate their symptoms and improve general wellbeing ( Buster 2000 ; Davis 2008 ; Mayo 2002 ; Pluchino 2013 ; Raven 2007 ). The effects of DHEA on menopausal women may differ from HT because of the additional androgenic effect of DHEA ( Dobs 2002 ; Labrie 2005 ). In postmenopausal women DHEA has been hypothesized to increase the incidence of breast cancer and caution is advised. However, there have been limited studies on this subject and the results of these studies are inconsistent ( Morris 2001 ; Schwartz 2006 ; Shilkaitis 2005 ; Stoll 1999 ). No other serious adverse effects of DHEA have been described in the published literature. Doses of 50 mg and above have shown androgenic side effects (for example acne and increased hair growth) ( Kroboth 1999 ; Panjari 2010 ). Different effects of DHEA have been described using different routes of administration ( Casson 1996 ). Intravaginal administration of DHEA has been reported to have a better effect on alleviating the symptom of vaginal atrophy and improving sexual function than oral administration of DHEA ( Goel 2011 ). Why it is important to do this review Treatment with DHEA is controversial as there is uncertainty about its effectiveness and safety ( Buster 2000 ; Panjari 2010 ; Raven 2007 ). Inconsistent findings have been published on the effects of DHEA in menopausal women and much of the data from clinical trials are limited by small sample sizes and short duration of treatment ( Cameron 2005 ; Panjari 2010 ). This review should clearly outline the evidence for DHEA in the treatment of menopausal symptoms and evaluate its effectiveness and safety by combining the results of randomised controlled trials.",
        "summary": "Not enough reliable randomized controlled trial evidence exists to either support or refute the use of dehydroepiandrosterone (DHEA) in women in the postmenopausal phase. When compared with placebo, DHEA did not show any improvement in quality of life; the trials were underpowered and at high risk of bias. There was no evidence of a difference in sexual function between groups but when one large trial at high risk of bias was excluded from the analysis, a small improvement was noted. A single underpowered trial showed inconsistent effects when assessing a range of menopausal symptoms, and low\u2010quality evidence suggested higher rates of androgenic effects. When DHEA was compared with estrogen or tibolone, very low\u2010quality evidence showed no obvious difference in quality of life and adverse effects like acne, breast tenderness, nausea and weight gain, but the trials were underpowered. As a result of this, menopausal symptoms and sexual function could not be properly assessed either. There were no trials comparing DHEA with any other medical or non\u2010medical treatment."
    },
    "CD011205": {
        "query": "How does riociguat compare with placebo for adults with pulmonary hypertension?",
        "document": "Background Description of the condition Pulmonary hypertension (PH) is an important cause of morbidity and mortality, affecting up to 100 million people worldwide ( Schermuly 2011 ). It leads to a substantial loss of exercise capacity, as well as causing right ventricular overload, resulting in heart failure and early mortality. Current three year survival ranges from 58.2% to 73.3% ( Ling 2012 ). PH is diagnosed when resting mean pulmonary arterial pressure (pPA) is 25 mmHg or more at cardiac catheterization. The clinical phenotype includes dyspnoea ( Table 1 ), syncope ( Table 1 ), chest pain and fatigue, whilst examination may reveal cyanosis ( Table 1 ), right ventricular heave, a loud pulmonary second heart sound, and in late disease, signs of right heart failure. The degree of physical limitation can be categorised by the 6\u2010minute walking distance (6MWD) or the results at cardiopulmonary exercise testing ( Miyamoto 2000 ; Abumehdi 2015 ). PH symptoms are classified clinically according to the World Health Organization (WHO) functional class system (1 = no limitation, 4 = symptomatic at rest; Gali\u00e8 2013 ). Term Explanation Cyanosis A blue discolouration of the skin secondary to poor circulation or blood oxygenation Dyspnoea Difficulty in breathing Syncope Transient loss of consciousness and posture due to inadequate perfusion of the brain PH is divided into five aetiological categories according to the WHO criteria ( Simonneau 2013b ), of which three categories are relevant to our review. Group 1 is pulmonary arterial hypertension (PAH). This is subdivided further into groups including PH associated with congenital heart disease (CHD), connective tissue disease, heritable PH and, the most common group, idiopathic PH. PAH treatment options include calcium channel blockers, phosphodiesterase\u2010V inhibitors (PDE5is), dual\u2010endothelin receptor antagonists and prostanoids. More recently soluble guanylate cyclase (sGC) stimulators have gained approval. Despite this, annual mortality remains 15% ( Thenappan 2010 ). Group 2 is PH associated with left heart disease, the most common form of PH worldwide, affecting up to 30% of people with heart failure. There is no proven treatment that specifically targets PH in this condition ( Damy 2010 ). The other major class relevant to this review is group 4, chronic thromboembolic PH (CTEPH). This entails vessel occlusion secondary to thromboses or emboli, alongside a vasoconstrictive component, which leads to significant increases in vascular resistance. PH is associated with poorer outcomes in pulmonary embolism unless early diagnosis is made ( Riedel 1982 ). Pulmonary endartectomy (PEA) is the gold\u2010standard management for CTEPH. However, up to 63% of candidates are ineligible for surgery due to co\u2010morbidities, unsuitable morphology, surgical centre access or declining consent ( Mayer 2011 ). Balloon angioplasty has also been used with some effect for inoperable cases, although there is currently no randomised data underlying this ( Tatebe 2016 ). Furthermore, the condition either recurs or is refractory to surgery in 5% to 35% of cases ( Thistlesthwaite 2006 ; Bonderman 2007 ). This leaves substantial unmet medical need. Description of the intervention As our understanding of PH has evolved, numerous medications have been developed with the hope of improving clinical outcomes ( Gali\u00e8 2013 ). Historically calcium channel blockers are the first line treatment but only for those responding positively at vaso reactivity tests. Prostanoids were the first real breakthrough, and are still widely considered the gold standard due to their superior efficacy but have a significant side effect profile. However, the most common form requires continuous intravenous infusion, with associated complications limiting utility. More recent developments include both selective and dual endothelin receptor antagonists (e.g. ambrisentan, bosentan, macitentan) and phosphodiesterase\u2010V inhibitors (e.g. sildenafil, tadalafil) ( Wardle 2013b ). The decision of which PAH regimen to use is determined by WHO functional class amongst other factors ( Gali\u00e8 2013 ). Studies have suggested and it is widely accepted that 6MWD greater than 400 m is associated with improved survival; however, this is yet to be proved. There is a lack of randomised controlled trial (RCT) data in paediatric groups overall, meaning drug treatment is based largely upon adult evidence ( Ivy 2013 ). In 2013, the Food and Drug Administration (FDA) approved riociguat, a sGC stimulator, the first in a novel class of PH treatments ( Guha 2013 ), which now also has European approval ( Dowdall 2014 ). This license pertained not only to PAH, but to CTEPH also. This makes sGC stimulators the first specific treatment for inoperable or recurrent CTEPH. Inoperable CTEPH is that determined to be unsuitable for PEA by a multi\u2010disciplinary surgical team, whilst recurrent CTEPH is that occurring after PEA has been performed. Medical therapy is not intended as a replacement for surgery in people suitable for PEA ( Archer 2013 ) and any attempt at this should be discouraged. Furthermore, surgery should not be delayed for trials of medical therapy ( Keogh 2009 ). When considering the clinical use of riociguat, the price implications are still poorly defined, but should be in line with rival PAH agents at GBP 25,931 per year ( Bayer 2014 ). Riociguat comes in the form of an oral tablet (up to 2.5 mg as tolerated), taken three times daily to aid patient compliance. How the intervention might work sGC stimulators are a novel class of PH medication that manipulate endogenous mechanisms controlling pulmonary pressures. In normal physiology, the body regulates pulmonary flow, and therefore resistance, through a unique set of endocrine and paracrine effectors. One of these is the ventilation\u2010induced release of endothelial nitric oxide (NO) to stimulate vasodilatation via smooth muscle cells. NO does this by stimulating sGC to produce cyclic guanosine monophosphate (cGMP). cGMP activates kinases that lead to vasodilatation and the inhibition of inflammation and thrombosis ( Denninger 1999 ; Ghofrani 2004 ). In addition, the potential for increasing cGMP as a therapeutic target for pulmonary vasodilatation is demonstrated by PDE5is ( Wardle 2013a ; Wardle 2013b ). However, it is hoped that sGC stimulators may display greater efficacy than PDE5is due to their independence from NO bioavailability, relative lack of adverse effects, and actions as a NO sGC sensitiser and stimulator ( Stasch 2011 ). Why it is important to do this review There is already significant controversy over the most effective medication for PAH and this will deepen with the introduction of sGC stimulators. It also remains to be seen how sGC stimulators should be used in the context of CTEPH \u2014 whether they are a replacement for PEA in people at high surgical risk, an adjunct, or a measure of last resort. By collating the evidence of these agents in PH it is hoped that we will address these issues whilst also increasing the evidence base for adverse effects \u2014 an area still lacking sufficient data. Finally, this work will also act as a protocol capable of being repeated in the future as the evidence base for these agents continues to evolve, and therefore begin to act as basis for comparison between different available treatments.",
        "summary": "For adults with pulmonary hypertension, riociguat (0.5 to 2.5 mg three times/d for 12 to 16 weeks) increased exercise capacity (on average, by 30 meters on the six\u2010minute walking distance test; 659 participants) and slightly reduced mean pulmonary artery pressure at 16 weeks (on average, by 2.8 mmHg; 744 participants) compared with placebo. Overall, randomized controlled trials (RCTs) showed no clear differences between riociguat and placebo in terms of improvement in World Health Organization (WHO) functional class, clinical worsening, mortality, or serious adverse events (nature of serious adverse events not described). Although two subgroup analyses showed benefit of riociguat in terms of WHO functional class or clinical worsening, the former was noted in people with chronic thromboembolic pulmonary hypertension, and the latter in people with pulmonary artery hypertension, so results were not consistent. The quality of the evidence was not assessed, but no trial reported blinding of outcome assessors; this could have had a detrimental impact on the reliability of results for subjective outcomes. It is worth noting that the RCTs included people with different functional status and specific populations that may not fully represent people with pulmonary hypertension seen in clinical practice."
    },
    "CD011715": {
        "query": "How do remote check\u2010ups compare with face\u2010to\u2010face check\u2010ups in people with asthma?",
        "document": "Background Description of the condition Asthma is a chronic disease of the airways, which causes reversible inflammation and narrowing of the airways and mucus production ( GINA 2014 ). It commonly causes symptoms of wheezing, breathlessness, chest tightness and cough, although these vary between people and over time in their presence, frequency and severity ( GINA 2014 ). Despite the emergence and update of several national and international management guidelines which recommend a range of cost\u2010effective treatments based on frequency and severity of symptoms and exacerbations (e.g. BTS/SIGN 2014 ; GINA 2014 ), the disease remains a significant cause of avoidable morbidity and mortality worldwide ( BTS/SIGN 2014 ; Global Asthma Report 2014 ; NRAD 2014 ). A national review of the 195 asthma deaths that occurred between February 2012 and January 2013 in the UK revealed that, in the year preceding their death, nearly one\u2010third had no record of seeing a general practitioner (GP) and nearly two\u2010thirds had not had an asthma check\u2010up in secondary care ( NRAD 2014 ). The importance of self\u2010monitoring and regular check\u2010ups with a healthcare professional to monitor symptoms, and encourage adherence to preventer inhalers, is now well accepted ( Gibson 2002 ; NRAD 2014 ), especially for people at high risk of severe asthma attacks. Description of the intervention Communication technologies, such as telephones and video conferencing, have been proposed as a way to conduct asthma check\u2010ups remotely. Conducting check\u2010ups in this way is a form of 'telehealth', otherwise referred to as 'telecare', 'digital health', 'telemedicine' or 'e\u2010health'. McLean 2013 described this field as \"the use of information and communication technologies to deliver healthcare at a distance and to support patient self\u2010management through remote monitoring and personalised feedback\". It may also be conceptualised as \"an emerging field in the intersection of medical informatics, public health and business, referring to health services and information delivered or enhanced through the internet and related technologies ( Eysenback 2001 ). Health services around the world are considering remote check\u2010ups as a way to manage the rising number of people with long\u2010term health conditions, to improve health outcomes and reduce the burden on emergency and inpatient services ( Department of Health 2012 ; Steventon 2012 ). The UK government outlined its aims for the widespread use of technology in health in their 2013 mandate, including wide availability of 'e\u2010consultations' by GPs, and significant progress towards home 'telemonitoring' of three million people with long\u2010term conditions by 2017 ( Department of Health 2013 ). Researchers have studied the role of a range of technology\u2010based check\u2010ups and monitoring in asthma and other health conditions, including the use of telephone calls, email contact, text\u2010messaging and video\u2010conferencing ( Laver 2013 ; McLean 2010 ; McLean 2011 ). How the intervention might work In the context of asthma, a condition that affects around 334 million people worldwide ( Global Asthma Report 2014 ) and places a significant burden on healthcare systems, remote check\u2010ups may represent an unobtrusive and efficient way of maintaining contact with patients. Regular monitoring with communication technologies that does not disrupt a patient's life in the way that regular clinic visits might, may serve to enhance self management behaviours that have known benefits on morbidity and mortality, such as keeping personalised action plans up\u2010to\u2010date, and adherence to maintenance medications ( NRAD 2014 ). However, while governments and health services have highlighted the potential for cost savings and improved clinical outcomes of using remote check\u2010ups instead of face\u2010to\u2010face consultations, its use to monitor patients with potentially serious or life\u2010threatening conditions may not be without hazard. Focus groups have suggested that telehealth may be acceptable to patients and clinicians, but they have also raised concerns that it could actually discourage self management, or increase the likelihood of serious outcomes, by instilling a false sense of security ( Pinnock 2007b ). The feasibility of using communication technologies in different situations and populations may be hampered by barriers, including insufficient healthcare infrastructure and funding ( Lustig 2012 ). However, it may be a way to reduce inequality in health care related to socioeconomic status and rural living by improving access to services ( Jannett 2003 ; Lustig 2012 ). Why it is important to do this review The release of the UK National Health Service (NHS) mandate in 2013 has seen a push to advance the use of telehealth for economic and clinical benefit. A recent overview of systematic reviews suggested that these benefits should not be assumed and that people at highest risk of serious health outcomes are likely to show the biggest gains ( McLean 2013 ). For asthma, existing reviews have noted a large degree of variation in the way telehealth is defined and delivered in studies, to whom and to what it is compared ( Jaana 2009 ; McLean 2010 ), and have been limited for this reason in the conclusions that could be drawn. This Cochrane review will focus on conducting asthma check\u2010ups remotely as a form of telehealth compared with usual face\u2010to\u2010face consultations in a hospital or clinic. A related Cochrane review will consider the evidence for remote monitoring of asthma control between visits with ongoing personalised feedback from a health professional ( Kew 2015a ).",
        "summary": "When remote check\u2010ups/asthma reviews were compared with face\u2010to\u2010face check\u2010ups/asthma reviews in adults and children with asthma, the only outcome that showed a difference between these methods was lung function measured as trough FEV1; this was better with remote check\u2010ups/asthma reviews (moderate\u2010quality evidence. No differences were detected between groups for any other outcome reported (exacerbations requiring oral corticosteroids, asthma control, asthma\u2010related quality of life, unscheduled healthcare visits, exacerbations requiring emergency department visit or admission). However, the evidence for several outcomes was low\u2010quality and the numbers of participants was often small. Therefore, no firm conclusion that these types of check\u2010ups/asthma reviews are equivalent for these outcomes can be drawn."
    },
    "CD004024": {
        "query": "What are the effects of high\u2010dose chemotherapy with autologous stem cell transplantation for the treatment of aggressive non\u2010Hodgkin lymphoma?",
        "document": "Background Since the standard chemotherapy regimen CHOP (Cyclophosphamide, Doxorubicin, Vincristine, Prednisone) was established in 1976 ( Gottlieb 1973 , McKelvey 1976 ), there has been no substantial improvement in the prognosis of patients with aggressive non\u2010Hodgkin lymphoma (NHL). Complete remission rates and long\u2010term survival rates with CHOP are about 55% and 30% respectively ( Agthoven 2003 , Project 1993 , Fisher 1994 ). The role of Rituximab, a monoclonal antibody against surface antigen CD20, either combined with conventional chemotherapy or with high\u2010dose chemotherapy is still under investigation. Several so called third\u2010generation regimens include new agents and different doses and are based on the CHOP backbone. These regimens showed encouraging results in pilot studies. However, three large randomised studies failed to show an improvement in disease\u2010free or overall survival when compared with the CHOP\u2010regimen ( Cooper 1994 , Gordon 1992 , Fisher 1993 ). According to the Norton\u2010Simon\u2010hypothesis, relapses are not only caused by resistant tumour cells (genetic resistance), but also by tumour cells with reduced sensitivity to chemotherapeutic agents (kinetic resistance) ( Norton 1982 ). According to this hypothesis, an intensification of chemotherapy with non\u2010cross reacting agents applied after the induction therapy should lead to higher cure rates. With the development of high\u2010dose chemotherapy (HDT) followed by autologous stem\u2010cell transplantation (ASCT) and the increasing feasibility of this approach by improved supportive care, this hypothesis was tested in clinical trials. For patients who relapsed after conventional chemotherapy a significant benefit could be demonstrated when receiving HDT and ASCT as salvage therapy ( Philip 1995 ). Therefore eradicating the disease at an earlier time point or during first\u2010line therapy might be promising approach sparing a later salvage treatment. Although the initial results of uncontrolled trials using HDT as first\u2010line treatment in aggressive NHL were encouraging ( Freedman 1993 , Nademanee 1992 , Nademanee 1997 , Vitolo 1997 ), several randomised controlled studies showed no significant survival benefit of this approach for the majority of patients ( Haioun , Martelli , Kluin\u2010Nelemans , Verdonck ). However, a few studies indicated a trend towards a better survival for patients with a poor prognosis according to the International Prognostic Index score (IPI) that was established in 1993 ( Project 1993 ). This score was designed to further clarify lymphoma staging. Based on the number of negative prognostic factors present at the time of diagnosis (age > 60 years, stage III/IV disease, elevated lactate dehydrogenase [LDH] level, Eastern Cooperative Oncology Group [ECOG] performance status > 2, more than one extranodal site of disease, four discrete outcome groups were identified with a 5\u2010year overall survival ranging from 26% to 73%. The modified age\u2010adjusted IPI score was defined for patients less than 60 years and comprises three factors (performance status, stage and LDH) and also four risk groups. As the feasibility of the HDT approach for older patients was quite unknown, most conducted trials only included younger patients and subsequently applied the age\u2010adjusted score. The randomised LNH87\u20102 trial demonstrated no difference in terms of overall survival and disease\u2010free survival between patients in complete remission who received conventional therapy or HDT ( Haioun ). But in a retrospective analysis, the same investigators suggested an improvement for the subgroup of patients with high\u2010intermediate and high risk IPI ( Haioun (subgroup) ). The best strategy for the application of HDT is still unclear and subsequently varied among trials. Some trials applied HDT after a reduced number of standard\u2010dose chemotherapy cycles. Others after a full standard\u2010dose regimen or employed a sequence of two or three single drugs in a high but non\u2010myeloablative dose, followed by myeloablative polychemotherapy, named high\u2010dose sequential chemotherapy. Additionally, investigated patient populations differed between trials. Some investigators assumed best efficacy for patients who do respond only slowly to standard chemotherapy, others for patients who achieved a complete remission. Subsequently these investigators randomised patients in complete or partial remission only, respectively. Single studies are often underpowered. In addition the randomly assigned patients often do not receive the intended therapy, which further diminishes the ability of a single trial to show differences between treatment strategies when using an intention to treat analysis. By systematically reviewing the literature, identifying randomised controlled trials (RCTs) and pooling individual patient data where possible, this meta\u2010analysis tries to summarize the current evidence for the role of HDT in first\u2010line therapy for patients with aggressive NHL. This also includes the assessment of the impact of different subgroups derived from the IPI score, different HDT strategies, and different remission status of patients receiving HDT. For this individual patient data (IPD) concerning age adjusted outcomes (aaIPI) were requested from the authors of the included trials and a limited IPD analysis was performed next to the conventional meta\u2010analysis. Two meta\u2010analyses addressing the same question have been published previously ( Simnett 2000 , Strehl 2003 ). Neither found clear evidence for the usage of HDT. These analyses did not include all RCTs and neither used hazard ratios or individual patient data to analyse time to event data. The odds ratios used in the previous analyses are based on cumulative death rates at one particular time point and are considered to favour the risk of biased results ( Duchateau 2001 ). We therefore decided to systematically review the literature, to identify randomised controlled trials (RCTs) and to pool the results of individual studies and in case of availability individual patient data to summarize the current evidence for the role of HDT in first\u2010line therapy for patients with aggressive NHL. Therefore we contacted authors to provide us with individual patient data for evaluation of overall survival according to the aaIPI.",
        "summary": "In people receiving first\u2010line therapy for aggressive non\u2010Hodgkin lymphoma, high\u2010dose chemotherapy with autologous stem cell does not seem to provide a meaningful benefit compared with conventional chemotherapy, with increased rates of leukopenia/neutropenia, thrombocytopenia and infection. Randomized controlled trials including 2400 participants show no benefit of high\u2010dose chemotherapy compared with conventional chemotherapy in terms of overall survival, event\u2010free survival, and incidence of secondary neoplasm. Although more people achieved a complete response with high\u2010dose chemotherapy compared with conventional chemotherapy, rates of leukopenia/neutropenia, thrombocytopenia and infection were higher with high\u2010dose chemotherapy. These data are limited due to the fact that subsets of patients (e.g. double\u2010hit, triple\u2010hit lymphoma or patients who responded after chemotherapy or salvage) were not considered"
    },
    "CD000535-0": {
        "query": "Do aspirin or aspirin/dipyridamole help prevent thrombosis after peripheral arterial bypass surgery?",
        "document": "Background Description of the condition Symptomatic, chronic peripheral arterial disease (PAD) of the lower extremities may present either as intermittent claudication (IC), pain on walking, or as critical limb ischaemia (CLI), which is a more progressive stage with pain at rest, ulceration and gangrene ( Becker 2011 ). The implantation of a femoropopliteal or femorodistal bypass graft (a makeshift blood vessel used to bypass a blockage in the main artery of the thigh) is one treatment option for people who are at risk of losing a limb or whose walking ability is greatly impaired because of the disease. By placing a graft in the groin area (an infrainguinal graft) the blocked arterial segment is bypassed, thereby improving blood flow in the limb. This then relieves the symptoms of claudication or rest pain and decreases the potential for amputation due to ulceration and gangrene, termed limb salvage. The patency rate for femoropopliteal and femorodistal grafts, or the number of bypasses remaining open after a certain period of time, depends on several risk factors. These include whether or not the patient has acute lower limb ischaemia ( Baril 2013 ), graft material, length of the bypass, site of the distal anastomosis (surgical connection of the graft to the existing artery), outflow conditions in the calf (flow of blood out of the graft) and female gender ( Cooper 1990 ; Tangelder 2000 ). Autologous saphenous vein (using a vein from the patient's calf or thigh or both) is superior to prosthetic (artificial) materials such as Dacron or polytetrafluoroethylene (PTFE) ( Rutherford 1988 ). Where the distal anastomosis is above the knee there is a lower risk of graft failure, and patients with two or three patent calf arteries have a better outcome than those with only one patent artery. Graft failure most frequently occurs at the site of either the distal or proximal anastomosis, when smooth muscle cells of the medial (middle) layer of the vessel wall grow into the intimal (inner) layer, which is known as intimal hyperplasia. This in turn causes the diameter of the perfused graft to become smaller, known as stenosis. When blood flow through the vessel is significantly reduced intermittent claudication may be experienced. Graft occlusion (closure) can occur by formation of a thrombosis (clot) at the stenotic site. If blood flow in the failed graft cannot be restored and further bypass surgery is not possible, then blood flow may be so poor that the limb cannot remain viable and amputation is required. Successful prevention of graft failure and thus the need for surgical re\u2010intervention is of major clinical and economic importance. Occlusion rates one year after the operation vary between 15% and 75% depending on the various risk factors described above ( Abbott 1997 ; Consensus 1991 ). In addition, patients with lower limb atherosclerosis (progressive hardening of the arteries) frequently experience increased platelet aggregation, leading to clot formation. Moreover, the body's physiological stress response to surgery is a prothrombotic state (where the body is preparing to form a blood clot). Description of the intervention Antiplatelet treatment is used to prevent formation of blood clots by preventing platelet aggregation ( NICE 2013 ). Antiplatelet agents work by improving blood rheology and also by suppressing the advancement of in\u2010 and outflow atherosclerosis (reducing the rate of development of stenosis) as well as coronary artery atherosclerosis, indirectly optimising cardiac function and circulation. For patients receiving bypass surgery, the intensity of platelet uptake by the graft material has been shown to be inversely related to graft patency at one year, indicating the need for antiplatelet intervention. In animal experiments, antiplatelet drug administration started before bypass surgery has been shown to increase patency in prosthetic grafts when compared to no treatment ( Fujitani 1988 ). To prevent graft occlusion patients are usually treated with either an antiplatelet or an anticoagulant drug, or a combination of both. It is not known which regimen is best to prevent infrainguinal graft occlusion. The aim of this review was to evaluate whether antiplatelet treatment improves graft patency, limb salvage and survival in patients with chronic PAD who are undergoing infrainguinal bypass surgery. Why it is important to do this review Another Cochrane review was recently undertaken to also evaluate the prevention of thrombosis after infrainguinal arterial bypass, but this review focused on the use of antithrombotic therapies ( Geraghty 2011 ). Overall their findings were inconclusive, with a possible benefit of antithrombotics for autologous vein grafts. Also, there may be higher economic costs and adverse haemorrhagic events associated with the use of anticoagulants compared with antiplatelet therapy. If the efficacy of antiplatelets is acceptable then it may be a more economical alternative to anticoagulant therapy following bypass graft surgery in the legs.",
        "summary": "Aspirin, with or without dipyridamole, seems to reduce the number of graft occlusions post\u2010peripheral arterial bypass surgery. However, small participant numbers and varying medication regimens make it difficult to draw firm conclusions. In people post\u2010peripheral arterial bypass surgery, primary graft patency seems to be aided by aspirin or aspirin/dipyridamole (various regimens used), reducing the number of people with primary occlusion at 12 months [142 versus 281 with occlusion per 1000; 6 RCTs, 952 participants]). This apparent benefit was also observed when patients with prosthetic grafts were analyzed separately at all time points up to 12 months (160 versus 500 with occlusion per 1000 people with prosthetic grafts at 12 months). The results for people with venous grafts were inconsistent across time points up to 24 months. A single, underpowered, trial (148 participants) did not detect a reduction in limb amputations with aspirin/dipyridamole, but may have been too small to do so if one was present. There is no suggestion of a reduction in cardiovascular events (4 RCTs, 811 participants) or mortality (4 RCTs, 799 participants). There appear to be increases in gastrointestinal complications and rates of major bleeding with aspirin or aspirin/dipyridamole. However, the small numbers of participants means none of the analyses reached statistical significance, therefore, the safety of these interventions remains uncertain. There were no apparent differences detected between groups in infection rates. No trials assessed quality of life, assisted graft patency, or lower limb blood flow. The ability of the analyses to reliably detect differences between groups would have been affected by a lack of power due to the small numbers of participants, varying medication regimens, and the lack of blinding of outcome assessors and allocation concealment."
    },
    "CD007223": {
        "query": "How does misoprostol compare with surgery among women with incomplete miscarriage?",
        "document": "Background Description of the condition Miscarriage is generally defined as the spontaneous loss of a pregnancy prior to 24 weeks' gestation, that is, before the fetus is usually viable outside the uterus ( Shiers 2003 ). The clinical signs of miscarriage are vaginal bleeding, usually with abdominal pain and cramping. If the pregnancy has been expelled, the miscarriage is termed 'complete' or 'incomplete' depending on whether or not tissues are retained in the uterus. If a woman bleeds but her cervix is closed, this is described as a 'threatened miscarriage' as it is often possible for the pregnancy to continue and not to miscarry ( RCOG 2006 ; Shiers 2003 ); if the pregnancy is in the uterus but the cervix is open, this is described as an 'inevitable miscarriage', i.e. it will not usually be possible to save the pregnancy and fetus. The now widespread use of ultrasound in early pregnancy, either for specific reasons (e.g. bleeding) or as a routine procedure, reveals pregnancies that are destined to inevitably miscarry, because they are 'non\u2010viable' ( Sawyer 2007 ; Weeks 2001 ). Non\u2010viable pregnancies are either a 'missed miscarriage' if an embryo or fetus is present but is dead, or an 'anembryonic pregnancy' if no embryo has developed within the gestation sac. Regardless of the type of miscarriage, the overall incidence is considered to be between 10% and 15%, although the real incidence may be greater ( Shiers 2003 ). Most miscarriages occur within the first 12 weeks of pregnancy and are called 'early miscarriage', with those occurring after 13 weeks being known as 'late miscarriage'. The cause of miscarriage is generally unknown, but most are likely to be due to chromosomal abnormalities. The risk of miscarriage has been reported to be higher in older women, and where there are structural abnormalities of the genital tract, infection, and maternal complications such as diabetes, renal disease, and thyroid dysfunction. Also, some environmental factors have been linked with miscarriage, including alcohol and smoking ( Shiers 2003 ). Miscarriage can sometimes lead to haemorrhage and infection, and it can be an important cause of morbidity, and even mortality, particularly in low\u2010income countries ( Lewis 2007 ). Women experiencing miscarriage may be overwhelmed by the symptoms and also quite distressed ( Shiers 2003 ). Psychological problems can follow a miscarriage, and these can include loss of self\u2010esteem resulting from the woman's feeling of inability to rely on her body to give birth ( Swanson 1999 ). Emotional responses described include those of emptiness, guilt, and failure ( Swanson 1999 ). There can also be depression, anxiety, grief, and anger ( Klier 2002 ; Thapar 1992 ). A number of other consequences, including sleep disturbance, social withdrawal, anger, and marital disturbance, may occur following miscarriage ( Lok 2007 ). Fathers can also be affected emotionally ( Klier 2002 ). Description of the intervention Traditionally, all pregnancies that had miscarried were considered by clinicians as potentially incomplete. Therefore, surgical curettage ('evacuation of the uterus') was performed routinely to remove any retained placental tissue. If no tissue was obtained, then a retrospective diagnosis of complete miscarriage was made. Surgical curettage was the 'gold standard management' for miscarriage for many years because it is quickly performed and it is possible to completely remove any retained products of conception ( Ankum 2001 ). Histological examination of the removed tissues also allowed exclusion of trophoblastic disease, e.g. hydatidiform mole \u2010 although this is quite rare. New clinical approaches have evolved to try to minimise unnecessary surgical interventions whilst aiming to maintain low rates of morbidity and mortality from miscarriage. These approaches have included ultrasound imaging to diagnose complete miscarriage and thus avoid treatment, or more conservative treatments of incomplete miscarriage, such as drug (medical) treatment or no active treatment (expectant management) ( Ankum 2001 ; Luise 2002 ). Various types of medical treatment could be suitable as alternatives to routine surgical treatment for miscarriage and these include the use of prostaglandins, or other uterotonic (uterus\u2010contracting) drugs or anti\u2010hormone therapy. How the intervention might work Misoprostol is a synthetic prostaglandin E1 analogue and is marketed for the prevention and treatment of peptic ulcers. Recognised as a potent method for pregnancy termination ( Costa 1993 ; Norman 1991 ), it is inexpensive, stable at room temperature, and has few systemic effects, although vomiting, diarrhoea, hypertension, and even potential teratogenicity (causing fetal malformation) when misoprostol fails to induce the abortion, have been reported ( Fonseca 1991 ). Misoprostol has been shown to be an effective myometrial stimulant of the pregnant uterus, selectively binding to EP\u20102/EP\u20103 prostanoid receptors and stimulating contractions, which push the products or pregnancy out. It is rapidly absorbed orally and vaginally. Vaginally\u2010absorbed serum levels are more prolonged, and vaginal misoprostol may have locally\u2010mediated effects ( Zieman 1997 ). Misoprostol could be especially useful in low\u2010income countries, where transport and storage facilities are inadequate, and the availability of uterotonic agents and blood is limited. Its use in obstetrics and gynaecology has been explored, especially to induce first and second trimester abortion ( Costa 1993 ; Norman 1991 ), for the induction of labour ( Alfirevic 2014 ; Hofmeyr 2010 ), and for the prevention of postpartum haemorrhage ( Tuncalp 2012 ). The stimulatory actions of misoprostol on the early pregnancy uterus could, in theory, help to expel retained tissue from the uterus after miscarriage, and provide an attractive medical alternative to surgical treatment of incomplete miscarriage ( Chung 1995 ). It is important to distinguish between the use of misoprostol for incomplete miscarriage and its use for termination of viable pregnancies. Ergometrine (extracted from the rye fungus, ergot) will promote contraction of involuntary muscles throughout the body ( Hawk 1985 ; Kawarabayashi 1990 ), and oxytocin promotes strong rhythmic contractions of the uterus ( Arthur 2007 ; Mota\u2010Rojas 2007 ). Both drugs could potentially have a role in expelling tissue after miscarriage. A number of progesterone antagonists are now available, and these drugs will interfere with the production, or functioning, or both, of progesterone. The progesterone antagonist, mifepristone, has an established role in the termination of first and second trimester pregnancy ( Jain 2002 ), and may also be effective in promoting expulsion of retained placental tissues following miscarriage ( Tang 2006b ). Why it is important to do this review Bleeding in early pregnancy is the most common reason for women to present to the gynaecology emergency department, and in many of these women, miscarriage will be diagnosed ( Ramphal 2006 ). It is now clear that routine surgical evacuation of the uterus following miscarriage may not be indicated, and the subsequent risk of infection, haemorrhage, cervical damage, uterine perforation, and risks of anaesthesia may not be justified ( Harris 2007 ). In order to optimise clinical management of this common condition, it is important to establish whether the use of medical treatment (drugs), or expectant management (no routine treatment) may offer a safer alternative for women with incomplete miscarriage, and whether there are specific circumstances where one type of treatment plan is superior to others. We initially aimed to systematically review medical treatments for both non\u2010viable pregnancies and incomplete miscarriages combined. On further reflection, this seemed illogical. Non\u2010viable pregnancies contain viable trophoblast (placental) tissue, which produces hormones, which may in theory make these pregnancies more susceptible to anti\u2010hormone therapy and more resistant to uterotonic (stimulating uterine contractions) therapy than pregnancies in which (incomplete) miscarriage has already taken place. Therefore, this review focuses on the management of incomplete miscarriage. Another Cochrane Review has covered non\u2010viable pregnancies ( Neilson 2006 ). Other relevant Cochrane Reviews on the treatment of miscarriage include: ' Expectant care versus surgical treatment for miscarriage ' ( Nanda 2012 ), ' Surgical procedures for evacuating incomplete miscarriage ' ( Tuncalp 2010 ), ' Anaesthesia for evacuation of incomplete miscarriage ' ( Calvache 2012 ), and ' Follow\u2010up for improving psychological well being for women after a miscarriage ' ( Murphy 2012 ). There is also a series of Cochrane Reviews on the possible prevention of miscarriage ( Aleman 2005 ; Bamigboye 2003 ; Empson 2005 ; Haas 2013 ; de Jong PG 2014 ; Wong 2014 ; Balogun 2016 ). In addition, there are Cochrane Reviews on medical and surgical interventions for induced abortions ( Dodd 2010 ; Kulier 2011 ; Lohr 2008 ; Wildschut 2011 ; Say 2010 ).",
        "summary": "Evidence from randomized clinical trials suggests that surgery is more effective than misoprostol for treating incomplete miscarriage, but the evidence is of low quality and is insufficient to permit full assessment of the benefits and harms of misoprostol versus surgery. Compared with surgery among women with incomplete miscarriage, evidence of very low to low quality suggests that misoprostol results in complete miscarriage in fewer women (on average, 963 vs 992 per 1000 women) and leads to unplanned surgery in more women (on average, 38 vs 8 per 1000 women). Despite these findings, moderate\u2010quality evidence shows that surgery and misoprostol are equally acceptable to women. Only one serious complication was observed across trials, in a woman who was taking oral misoprostol; therefore, despite inclusion of a relatively large number of women, the analysis was underpowered for assessment of this outcome. Other underpowered outcomes with low event rates and small participant numbers included required blood transfusion; anemia; pelvic infection; and cervical damage."
    },
    "CD004028": {
        "query": "What are the benefits and harms of valproate as an adjunct to antipsychotics for people with schizophrenia?",
        "document": "Background Despite the introduction of antipsychotic (neuroleptic) medications in the 1950s, there is still a sizeable minority (at least 30% of people with schizophrenia and related conditions), who do not achieve remission of symptoms ( Schooler 1993 ). Over the last 40 years a variety of adjunctive treatments have been used to treat schizophrenia ( Christison 1991 ). These are often used in addition to antipsychotics, in an attempt to alleviate the symptoms of schizophrenia such as hallucinations and delusional beliefs, although they have been used instead of antipsychotics. Treatments such as lithium ( Leucht 2007b , indicated for bipolar affective disorder), carbamazepine ( Leucht 2007c ), valproate ( Casey 2003 ), benzodiazepine ( Dold 2012 ), beta\u2010blockers ( Cheine 2003 ) and electroconvulsive therapy ( Tharyan 2005 ) have all been used for people whose psychosis did not respond to traditional therapy. The situation has improved somewhat in recent years, with the re\u2010introduction of clozapine, which has proven efficacy for those that have not responded to traditional antipsychotic medications ( Essali 2010 ). Whether the other second\u2010generation ('atypical') antipsychotics are more effective for the treatment of those with treatment\u2010resistant schizophrenia is unclear ( Bagnall 2000 , Gilbody 2000 , Srisurapanont 2004 ). Description of the condition Schizophrenia is often a chronic and disabling psychiatric disorder. It afflicts approximately one per cent of the population world\u2010wide with few gender differences. Its typical manifestations are 'positive' symptoms such as fixed, false beliefs (delusions) and perceptions without stimuli (hallucinations), and 'negative' symptoms such as apathy and lack of drive, disorganisation of behaviour and thought, and catatonic symptoms such as mannerisms and bizarre posturing ( Carpenter 1994 ). The degree of suffering and disability is considerable with 80% to 90% unemployed ( Marvaha 2004 ) and 10% going on to commit suicide ( Tsuang 1978 , Palmer 2005 ). Description of the intervention Valproate (valproic acid) is traditionally used as an anticonvulsant drug and is also used for affective disorders, especially for the treatment of acute mania. Furthermore it is thought to have anti\u2010aggressive effects and it may reduce impulsive behaviour, which might be useful for some people with schizophrenia ( Citrome 2000 ). How the intervention might work It is assumed that GABA\u2010ergic drugs such as valproate have a potential role in the treatment of schizophrenia as they down\u2010regulate dopamine ( Wassef 2000 ). Mesolimbic dopamine hyperactivity is considered one of the main reasons for the development of positive symptoms in schizophrenia. Why it is important to do this review In this review we examined the role of valproate in the treatment of schizophrenia and schizophrenia\u2010like psychoses. The importance of performing such a review is emphasised by the fact that valproate is already frequently used for schizophrenia. For example, between 1994 and 1998 the use of valproate almost tripled among inpatients in the New York State psychiatric hospital system, with 43.4% of 4922 participants with a diagnosis of schizophrenia receiving valproate ( Citrome 2000 ).",
        "summary": "For people with schizophrenia treated with antipsychotic therapy (when reported, olanzapine, risperidone, haloperidol, clozapine, or aripiprazole), adjunct valproate may lead to more people having a clinically significant response (on average, 721 vs 550 per 1000 people; low\u2010quality evidence) and a slightly better mental state (13 RCTs including 1363 participants). People receiving adjunct valproate had slightly lower score on scales assessing positive symptoms (nine RCTs with 1000 participants), negative symptoms (five RCTs with 651 participants), or aggression (three RCTs with 204 participants), although the clinical significance of this difference in scores is unclear. Although researchers found no apparent difference in rates of adverse events between groups, adding valproate led to higher rates of sedation/somnolence/drowsiness (on average, 276 vs 186 per 1000 people; low\u2010quality evidence) and thrombocytopenia, but tardive dyskinesia was less frequent with adjunct valproate."
    },
    "CD011674": {
        "query": "What are the effects of back schools for people with chronic non\u2010specific low back pain?",
        "document": "Background See glossary of terms in Appendix 1 . Description of the condition Low back pain (LBP) is a major problem worldwide, and the associated disability is responsible for a significant personal burden ( van Tulder 2006 ). The Global Burden of Disease Study suggests that LBP is one of the 10 leading causes of disease burden globally ( Murray 2013 ; Vos 2010 ). Many people with LBP become frequent users of healthcare services in their attempt to find treatments that minimise the severity of their symptoms. Exercise therapy is commonly advised for people with LBP, and it is recommended in clinical practice guidelines as an effective treatment for chronic LBP ( European Guidelines 2006 ). A Cochrane systematic review on this topic also concluded that exercise therapy is effective in decreasing pain and improving function in adults with chronic LBP ( Hayden 2005 ). Education has been recommended in clinical practice guidelines for chronic LBP ( European Guidelines 2006 ). Supervised exercise therapy associated with an educational component has been considered to be one of the most effective interventions in reducing pain and disability in people with chronic LBP ( Airaksinen 2006 ; van Tulder 2006 ). Back School is one treatment that provides both exercise and education for the treatment of people with chronic LBP. The original Swedish Back School was introduced by Zachrisson\u2010Forssell in 1969. It was designed to reduce pain and prevent recurrences of LBP episodes ( Forssell 1980 ; Forssell 1981 ). Back School was a therapeutic programme including information on the anatomy of the back, biomechanics, optimal posture, ergonomics, and back exercises. Since the introduction of the Swedish Back School, the content and length of the method have changed and appear to vary widely today. This review is an update of a previously conducted Cochrane review of the effectiveness of Back School for chronic non\u2010specific LBP. The previous Cochrane review was published in 2004 and concluded that Back School seemed to be more effective than other treatments, placebo, or waiting\u2010list controls for improving pain, functional status, and return to work ( Heymans 2004 ). Since the completion of this review, new trials about Back School have been published ( Andrade 2008 ; Cecchi 2010a ; Costantino 2014 ; Devasahayam 2014 ; Donzelli 2006 ; Dufour 2010 ; Durmus 2014 ; Garcia 2013 ; Heymans 2006 ; Jaromi 2012 ; Meng 2009 ; Morone 2011 ; Morone 2012 ; Nentwig 1990 ; Paolucci 2012a ; Paolucci 2012b ; Ribeiro 2008 ; Sahin 2011 ; Tavafian 2007 ). Given this substantial amount of new data, and developments in systematic review methods, a revision of the 2004 Cochrane review was needed to provide clinicians and patients up\u2010to\u2010date information about the effects of this intervention. Our aim was therefore to perform an update on this topic in order to provide accurate and robust information on the effectiveness of the Back School approach for chronic non\u2010specific LBP, as compared to no treatment, medical care, passive physiotherapy, or exercise therapy. Description of the intervention The original Swedish Back School was introduced by Zachrisson\u2010Forssell in 1969. It was meant to reduce pain and prevent recurrences of episodes of LBP ( Forssell 1980 ; Forssell 1981 ). Back School was a therapeutic programme including information on the anatomy of the back, biomechanics, optimal posture, ergonomics, and back exercises and was given to groups of patients. The aim was to reduce back pain and teach people to care for their own backs and back pain in an active way should back pain recur. How the intervention might work Back School is a combination of exercises and education, where lessons are given to groups of patients, supervised by a physical therapist or medical specialist. According to the European guidelines ( Airaksinen 2006 ), the combination of exercise programmes and education seems to be the most promising approach for the management of chronic non\u2010specific LBP. Theoretical information could help patients understand their condition and learn how to modify their behaviour with regard to LBP. People with chronic non\u2010specific LBP often have maladaptive thoughts, feelings, and beliefs, which have an important role in their experience of LBP ( Parsons 2007 ). Exercise therapy is probably the most commonly used intervention for the treatment of people with chronic non\u2010specific LBP. It is reported in the literature as effective in decreasing pain and improving function ( Hayden 2005 ). Treatment that combines both interventions has the potential to improve pain and disability in people with chronic non\u2010specific LBP. Why it is important to do this review This review is an update of a previously conducted Cochrane review of randomised controlled trials on the effectiveness of Back School ( Heymans 2004 ). We split this review into two reviews, one focusing on acute and subacute LBP, and one on chronic LBP. This review evaluated the effectiveness of Back School for chronic non\u2010specific LBP. In previous reviews it was not possible to statistically pool the data because of the heterogeneity of the included studies. Conclusions were generated on the basis of the methodological quality scores of the studies, assessed using a generally accepted criteria list, in combination with a best\u2010evidence synthesis ( van Tulder 2003 ). Since 2011, a number of new RCTs have been published evaluating the effectiveness of Back School. Method guidelines for Cochrane reviews have also been published by The Cochrane Collaboration ( Higgins 2011 ) and in the field of back pain ( Furlan 2015 ). These were also implemented in the current updated review.",
        "summary": "Back schools (which provide single or multiple educational sessions encompassing anatomy, etiology of injuries, advice on physical activity, and self\u2010management techniques with attention to stretching, strengthening, and ergonomics) may lead to minor reductions in pain and disability in the short or intermediate term but no sustained long\u2010term benefits. However, we are uncertain about these effects because the quality of the evidence is very low. Very low\u2010quality evidence suggests that back school was superior to no treatment in decreasing pain for up to three months (difference \u20106 points on a 0 to 100 scale) and was slightly better in reducing disability for up to three months (difference \u20103 points on a 0 to 100 scale). Back school was also more effective in reducing pain up to three months than medical treatment (\u201010 points on a 0 to 100 scale) and reducing disability at three to six months\u2019 follow\u2010up (\u20106 points on a 0 to 100 scale). Compared with those given physiotherapy, back school participants demonstrated no difference in pain but did have worse disability at over six months\u2019 follow\u2010up (difference +10 points on a 0 to 100 scale). Adverse events were poorly reported."
    },
    "CD004246": {
        "query": "How does efavirenz compare with nevirapine when used in three\u2010drug combination therapy for antiretroviral\u2010na\u00efve individuals with HIV infection?",
        "document": "Background Description of the condition A total of 36.7 million people were living with HIV in 2015. This is an increase from previous years, mostly due to the use of antiretroviral therapy (ART) ( UNAIDS 2016 ). In many countries, ART has reduced hospitalization, morbidity, and mortality among people living with HIV ( Gilks 2006 ; Hogg 1997 ; Mocroft 1998 ). Significant public and private resources have been devoted to rapidly scale up efforts in low\u2010 and middle\u2010income countries (LMICs) to provide access to first\u2010line ART. In 2014, only 40% of eligible people in LMICs were receiving ART. These efforts to scale\u2010up access to ART should be accompanied by initiatives to determine the most effective first\u2010line therapy ( UNAIDS 2016 ), which can be used in diverse populations. ART guidelines were first published by the World Health Organization (WHO) in 2002 ( WHO 2002 ), and were updated in 2006, 2010, 2013, 2014, and 2015 ( WHO 2015b ). For countries with limited resources, the WHO recommends a public health approach to ART to improve access, simplify clinical decision making, standardize regimens, and standardize the monitoring and management of toxicity and drug interactions ( Gilks 2006 ). For any initial regimen, the potency, durability of efficacy, ease of administration and storage, tolerability, and toxicity need to be balanced with cost and availability ( Gilks 2006 ). These guidelines provide a framework for choice of medication in most countries. However, when the recommended drugs have different costs and toxicity profiles, head\u2010to head comparisons are necessary to determine which medication should be the choice of preference for clinicians. The more recent guidelines integrate more evidence and are in favour of an earlier start to ART (CD4 cell count of 500 cells/mm 3 or less as opposed to the previous threshold of 350 cells/mm 3 ) in active tuberculosis, hepatitis B co\u2010infection with severe liver disease, pregnant and breastfeeding women, children under five years of age, and sero\u2010discordant couples ( WHO 2014 ). Description of the intervention The WHO Model List of Essential Medicines describes three classes of antiretroviral drugs for treatment and prevention of HIV infection: Nucleoside reverse transcriptase inhibitors (NRTI), non\u2010nucleoside reverse transcriptase inhibitors (NNRTI) and protease inhibitors ( WHO 2015a ). In 2006 the WHO recommended that initial ART should be with one of three regimens: two NRTIs plus efavirenz (EFV), two NRTIs plus nevirapine (NVP), or two NRTIs plus abacavir (ABC) ( Gilks 2006 ; WHO 2006 ). ABC is not a NNRTI and didn't qualify for evaluation in this review. The NRTI combination drugs could either be zidovudine (AZT) plus lamivudine (3TC), or stavudine (d4T) plus 3TC. Stavudine is no longer recommended as a first\u2010line regimen, given its known metabolic toxicities and should be used only when no other drug can be offered ( WHO 2015b ). The current recommendations suggest that the preferred first\u2010line regimen be composed of tenofovir (TDF) and 3TC or emcitrabine (FTC) with EFV. TDF could be replaced with AZT and EFV with NVP in the event that drugs in the preferred regimen are unavailable or contraindicated ( WHO 2015b ). Protease inhibitors can also be used in special circumstances ( WHO 2015b ). How the intervention might work Protease inhibitors cost more, have higher pill burdens, and have dietary constraints associated with their use. Protease inhibitors are also linked to serious long\u2010term metabolic disorders, most notably an increased risk of lipodystrophy and hyperlipidaemia ( Moyle 2000 ; BHIVA 2001 ). Moreover, a meta\u2010analysis of 12 trials revealed that NNRTI\u2010based regimens were better than protease inhibitor\u2010based regimens for virologic suppression ( Chou 2006 ). NNRTIs have a more favourable adverse effect profile than protease inhibitors, are cheaper, and are easier to administer. They are also more cost\u2010effective ( Beck 2008 ). Their main disadvantage is that a single mutation may confer resistance to the entire class of NNRTIs, since cross\u2010resistance among agents of this class is nearly universal ( Deeks 2001 ; Dybul 2002 ). NVP may be responsible for severe or fatal hepatotoxicity, and a rash which may present in severe form as Stevens\u2010Johnson syndrome. Nevertheless, NVP is the NNRTI of choice for pregnant women because EFV may be teratogenic( DHHS 2001a ). EFV may cause a rash and central nervous system symptoms such as dizziness, somnolence, insomnia, drowsiness, nightmares, hallucinations, and poor concentration ( DHHS 2001b ). Why it is important to do this review Providing evidence on the more appropriate choice of NNRTI with respect to efficacy, durability, and tolerability, is important to patients, caregivers, and policymakers worldwide. In the previous version of our review we found that EFV and NVP had similar efficacies, but different toxicity profiles ( Mbuagbaw 2010 ). The current review update represents a collaborative effort between the Cochrane Infectious Diseases Group, the University of California, San Francisco (UCSF), the School of Public Health of the University of Minnesota, the U.S. Centers for Disease Control and Prevention (CDC), the University of Cape Town, and the WHO to address questions through systematic reviews regarding the optimum first\u2010line ART regimen in patients living with HIV in low\u2010 and middle\u2010income countries. The previous review was used in the development of the 2009 WHO ART treatment guidelines ( WHO 2009 ). In the past five years, the body of evidence on NNRTI's has grown, especially among people co\u2010infected with tuberculosis. This Cochrane Review update responds to the need for evidence\u2010based recommendations for managing HIV and tuberculosis co\u2010morbidity.",
        "summary": "Efavirenz given as part of combination therapy for antiretroviral\u2010na\u00efve individuals with HIV infection may confer the favorable effect of reducing antiretroviral therapy (ART) drug resistance, with no apparent increase in the overall frequency of severe adverse events. High\u2010quality evidence shows that, compared with nevirapine, efavirenz given as part of combination therapy for antiretroviral\u2010na\u00efve individuals with HIV infection does not seem to increase virological success (on average, 715 vs 688 per 1000 people with efavirenz vs nevirapine) but could decrease the rate of ART drug resistance (on average, 72 vs 95 per 1000 people with efavirenz vs nevirapine; four trials with 1000 participants). There was no evidence of differences between groups in mortality (on average, 54 vs 64 per 1000 people with efavirenz vs nevirapine), progression to AIDS (on average, 50 vs 41 per 1000 people), discontinuation of therapy (on average, 164 vs 176 per 1000 people), CD4 count change (on average, \u20103.03 cells/mm 3 ), treatment failure (on average, 242 vs 249 per 1000 people), or overall severe adverse events (on average, 216 vs 192 per 1000 people with efavirenz vs nevirapine). When assessing specific severe adverse events, investigators found that efavirenz led to neutropenia and elevated transaminases in fewer people but produced a higher rate of impaired mental function (CNS adverse events). Finally, subgroup analyses on whether concurrent tuberculosis treatment affected outcomes yielded results similar to those of the main analysis, as did subgroup analyses assessing different doses of nevirapine (400 mg in one or divided doses), with the exception of mortality; subgroup analyses revealed that when a single dose of nevirapine was given, mortality was lower with efavirenz (on average, 11 vs 32 per 1000 people with efavirenz vs nevirapine)."
    },
    "CD012416": {
        "query": "How does professional oral care compare with usual oral care for nursing home residents to prevent nursing home\u2013acquired pneumonia?",
        "document": "Background Description of the condition Residents of nursing homes and long\u2010term care facilities are comprised predominantly of a geriatric population. Institutionalised older adults are prone to poor oral health because they have reduced access to professional dental care, and are unable to maintain the practice of good personal oral hygiene ( Berg 2000 ; Gaszynska 2014 ). Many studies have found that older adults require professional oral hygiene care as well as personal oral hygiene instruction (e.g. Frenkel 2000 ; Gaszynska 2014 ; Gluhak 2010 ; Petelin 2012 ). The incidence of community\u2010acquired pneumonia (CAP) requiring hospitalisation is 1.96 to 10 times higher amongst elderly nursing home residents than community\u2010dwelling elderly people ( Marrie 2002 ; Ronald 2008 ; Ticinesi 2016 ), with a 2.29 times higher rate of 30\u2010day mortality ( Liapikou 2014 ). This may be attributable to the particular characteristics of residents of nursing homes and long\u2010term care facilities, as they tend to be older, have greater functional impairment, and to have increased comorbidities, polypharmacy, and dependence upon caregivers ( Dudas 2000 ; Mart\u00ednez\u2010Morag\u00f3n 2004 ). Pneumonia occurring in residents of long\u2010term care facilities and nursing homes can be termed nursing home\u2010acquired pneumonia (NHAP); it closely resembles CAP, and may be caused by multidrug\u2010resistant bacteria ( Craven 2006 ; Mylotte 2002 ). This is suggested by data from the United States and Asia ( Micek 2007 ; Nakagawa 2014 ), but is not confirmed by European data ( Brito 2009 ; Ewig 2010 ). Nursing home\u2010acquired pneumonia is the leading cause of mortality among residents ( Cho 2011 ; Nicolle 1996 ). Its reported mean incidence ranges from 1 to 3.2 per 1000 patient days, with 600,000 emergency department admissions ( El\u2010Solh 2010 ; Medina\u2010Walpole 1999 ; Muder 1998 ). It has been suggested that NHAP may be caused by aspiration of oropharyngeal flora into the lung, and by failure of host defence mechanisms to eliminate aspirated bacteria ( Scannapieco 2014 ; Verghese 1983 ). Comorbidities considered to be risk factors for NHAP include the following ( Klapdor 2012 ; Ticinesi 2016 ): physical impairment; dementia; chronic obstructive pulmonary disease; mechanical ventilation; ageing. physical impairment; dementia; chronic obstructive pulmonary disease; mechanical ventilation; ageing. A growing body of evidence shows that poor oral hygiene and oral hygiene\u2010related factors (e.g. denture use ( O'Donnell 2016 ), being edentulous ( Abe 2008 )) may be additional risk factors for aspiration pneumonia among the elderly, who have an increased rate of dental plaque colonisation as a possible reservoir for pathogenic organisms associated with CAP or NHAP ( Bassim 2008 ; Janssens 2005 ; Scannapieco 2003 ). A systematic review by Azarpazhooh 2006 concluded that there was fair evidence (II\u20102, grade B recommendation) of an association between pneumonia and oral health, and good evidence (I, grade A recommendation) that better oral health and frequent professional oral care reduced the occurrence or progression of respiratory disease among high\u2010risk elderly living in nursing homes, and especially those in intensive care units. However, an RCT by Juthani\u2010Mehta 2015 indicated that advanced oral care measures did not significantly reduce the incidence of radiographically\u2010confirmed pneumonia or lower respiratory tract infection compared with usual care, in residents of nursing homes. Given that NHAP may be linked to oral hygiene, interventions for maintaining good oral hygiene might be of significant interest for this population. Description of the intervention It is widely believed that improved oral hygiene and frequent professional oral health care can be effective in reducing the incidence or progression of respiratory infection in residents of nursing homes and long\u2010term care facilities ( Azarpazhooh 2006 ; Scannapieco 2003 ; Sj\u00f6gren 2008 ; Watando 2004 ). Multiple oral care measures have been reinforced by the National Institute for Health and Care Excellence (NICE) guideline that introduced detailed oral care measures, and recommended that care home managers should ensure care home policies set out plans and actions to promote and protect residents' oral health ( NICE guideline 2016 ). The nature of oral care measures that have been proposed is diverse, but they can be classified broadly as follows. Mechanical aids to remove plaque and debris from the oral cavity, for example: toothbrushing; swabbing with water. Topical (chemical) disinfection to reduce colonisation, for example: mouthrinse; sprays; liquids; gels. Mechanical aids to remove plaque and debris from the oral cavity, for example: toothbrushing; swabbing with water. Topical (chemical) disinfection to reduce colonisation, for example: mouthrinse; sprays; liquids; gels. Antiseptics are broadly defined to include saline, chlorhexidine, povidone\u2010iodine, cetylpyridium, and others, but to exclude antibiotics ( Shi 2013 ). Combination of mechanical plaque removal and topical disinfection, for example: swabbing with antiseptic; toothbrushing with antibacterial toothpaste; daily toothbrushing plus antiseptic rinse. Professional dental care, for example: aided toothbrushing; suction to remove excess fluid. Combination of mechanical plaque removal and topical disinfection, for example: swabbing with antiseptic; toothbrushing with antibacterial toothpaste; daily toothbrushing plus antiseptic rinse. Professional dental care, for example: aided toothbrushing; suction to remove excess fluid. Oral care measures can be delivered at any frequency, by caregivers, nurses, dental care professionals, or dentists ( Ekstrand 2013 ; Zuluaga 2012 ). How the intervention might work Increasing evidence suggests a link between colonisation of bacteria and respiratory infection and pneumonia. Gram\u2010negative bacilli, such as Pseudomonas aeruginosa , Klebsiella pneumoniae, and Enterobacter species, have been suspected as causative pathogens of pneumonia ( Craven 1992 ; Liapikou 2014 ). Dependent and frail elderly patients have a higher detection rate of gram\u2010negative bacilli in their oropharyngeal cavities ( Leibovitz 2003 ; Mylotte 1994 ; Palmer 2001 ). Sumi 2007 showed that in a group of 138 dependent elderly, a potential respiratory pathogen colonised the dental plaques of 89 participants (64.5%). Aspiration of oropharyngeal fluid may cause translocation and colonisation of potential pulmonary pathogens in the lower respiratory tract and lungs ( Gibbons 1989 ; Munro 2004 ; Whittaker 1996 ); the latter may cause aspiration pneumonia ( Van der Maarel\u2010Wierink 2013 ). Therefore, oral care measures that reduce the colonisation of bacteria could result in decreased risk of pneumonia. The risk of NHAP might be reduced by measures that mechanically disrupt the biofilm (such as manual or electric toothbrushing), by the use of oral antiseptics that may remain active on oral tissues for several hours after application, or both, thus reducing the build\u2010up of plaque. For example, chlorhexidine (CHX) gluconate is a broad\u2010spectrum antiseptic agent that reduces both gram\u2010positive and gram\u2010negative bacteria associated with respiratory tract infection; it can remain chemically active on tissue for up to six hours ( Tantipong 2008 ). Oral conditions of elderly people have been shown to be improved by rinsing with 0.12% CHX solution daily or weekly for six weeks ( DeRiso 1996 ; Persseon 1991 ). Similarly, manual oral brushing improves oral hygiene by reducing bacterial pathogen colonisation, and improves the swallowing reflex by stimulating gums ( Yamaya 2001 ; Yoshino 2001 ). With removal or disruption of the oral plaque, pneumonia could be significantly reduced ( Shi 2013 ; Van der Maarel\u2010Wierink 2013 ). Such oral care measures can be used alone, or in combination. For example, Yoshida 2001 found that brushing teeth after each meal and rinsing daily with 1% povidone\u2010iodine, in conjunction with weekly professional dental care, significantly decreased the incidence of pneumonia in nursing homes. Why it is important to do this review Cochrane Oral Health undertook an extensive prioritisation exercise in 2014 to identify a core portfolio of clinically important review titles to be maintained in The Cochrane Library ( Worthington 2015 ). The dental public health expert panel identified this review as a priority title ( ohg.cochrane.org/priority\u2010reviews ). Although good oral hygiene has been shown to play an important role in maintaining the oral health and well\u2010being of institutionalised people, oral care measures have generally been afforded low priority in nursing homes. In some guidelines, such as British Thoracic Society guidance on the prevention of CAP, oral hygiene is not mentioned ( Lim 2009 ). Moreover, nurses have limited knowledge about providing mouth care in general ( Frenkel 2000 ; Jablonski 2005 ; Pyle 2005 ). Chiba 2009 reported that 32.4% of caregivers hesitated to provide oral care measures, which indicated their lack of knowledge about oral hygiene, but bespoke oral health education has been shown to have a positive effect on caregivers' knowledge and attitudes ( Charteris 2001 ; Frenkel 2001 ; Frenkel 2002 ; Sj\u00f6gren 2010 ). A systematic review by Kaneoka 2015 found that mechanical oral cleaning significantly reduced the risk of fatal pneumonia in residents in nursing homes. However, no other kinds of oral care measures were evaluated, and no Cochrane systematic review has focused on this issue. We believe it is important to synthesise the evidence from randomised controlled trials of oral care interventions that have evaluated their effectiveness in reducing NHAP. Identifying effective oral care interventions is also an essential step towards improving oral health and quality of life for nursing home residents.",
        "summary": "Compared with basic oral hygiene for elderly nursing home residents (brushing dentures or teeth daily), use of professional oral care (instruction or assistance of dental practitioners or caregivers with professional oral health knowledge) may reduce the development of nursing home\u2013acquired pneumonia (114 vs 187 per 1000 people; all values on average) and pneumonia\u2010related mortality (68 vs 165 per 1000 people) in the longer term (at 24 months\u2019 follow\u2010up), but the certainty of this evidence is low and reviewers state that results should be treated with caution. One RCT (366 participants) observed fewer people with fever when professional oral care was used (147 vs 297 per 1000 people). Results for the Debris Index and for quality of life assessment based on the Mini\u2010Mental State Examination (MMSE) were inconclusive. Only one RCT that was stopped early reported on adverse events (834 participants); there were 64 protocol\u2010related non\u2010serious adverse events, most commonly oral cavity disturbances such as gum bleeding or mouth sores and dental staining."
    },
    "CD004332": {
        "query": "Can acamprosate (with or without naltrexone) support continued abstinence after detoxification in alcohol\u2010dependent people?",
        "document": "Background Appendix 1 shows the abbreviations used in the text Description of the condition Alcohol dependence is among the main leading health risk factors in most developed and developing countries ( Alonso 2004 ). The one year prevalence of alcohol\u2010use disorders in people aged between 15 to 64 years is estimated at 5.2% in the American Region, 5.5% in European countries and at over 10% in the Eastern European countries ( Rehm 2009 ). According to the World Health Organisation ( WHO 2002 ), the misuse of alcohol belongs to the globally leading health risk factors, causing 20\u201330% of oesophageal cancer, liver disease, epilepsy, motor vehicle accidents, homicide and other intentional injuries. In the year 2004, 3.8% of all global deaths and 4.6% of global disability\u2010adjusted life\u2010years were attributable to alcohol ( Rehm 2009 ). The costs attributable to alcohol consumption are estimated at more than 1% of the gross domestic products in high\u2010income and middle\u2010income countries ( Konnopka 2007 ; Rehm 2009 ). At the same time, alcohol consumption belongs to major potentially avoidable health risk factors, underscoring the need for effective strategies to reduce excessive drinking and to support abstinence in patients who are dependent on alcohol. Description of the intervention Relapse prevention for alcohol dependence was exclusively dominated by psychosocial treatment strategies for many decades. Even though elaborated techniques from different theoretical and therapeutical backgrounds have been developed, treatment effects obtained by an exclusive application of psychosocial treatment are limited: A considerable high proportion of patients does not respond to the interventions at all and of those who respond, only a small portion succeeds in maintaining abstinence in a long\u2010term perspective ( Moos 2006 ). With the investigation of the neurobiological mechanism of alcohol dependence, various pharmacological agents have been examined in their potential to support alcohol dependent patients in achieving abstinence or in cutting down their alcohol consumption. Some of these agents showed promising effects in first small size trials, but could not be confirmed by multicenter trials, while two substances were repeatedly shown to be effective: The opioid antagonist naltrexone and the glutamate antagonist acamprosate. Acamprosate is a synthetic molecule with a chemical structure similar to that of the endogenous amino acid N\u2010acetyl homotaurine ( Zornoza 2003 ), a small, highly flexible molecule with analogy to many amino acids, most notably glutamate, gamma\u2010aminobutyric acid, aspartate, glycine, and taurine ( Spanagel 1997 ; Mann 2008 ). In animal models of alcohol dependence, acamprosate was shown to diminish the temporary increase in voluntary alcohol intake observed during a reinstated access to alcohol after a period of deprivation ( Czachowski 2001 ; Heyser 1998 ; LeMagnen 1987 ; Olive 2002 ; Spanagel 1996 ) \u2010 the so\u2010called \"alcohol deprivation effect\" (ADE), which serves as model of relapse. Besides its effects demonstrated in the limited access paradigm, acamprosate was shown to attenuate self\u2010administration of alcohol under free\u2010choice conditions ( Spanagel 2003 ) and to inhibit the development of the conditioned place preference in rats ( McGeehan 2003 ). It was also shown to selectively reduce alcohol\u2010seeking behavior elicited by environmental stimuli predictive of alcohol availability ( Bachteler 2005 ). Until today, various clinical studies have been conducted, with the majority of trials demonstrating superiority of acamprosate compared to placebo. Indicated for the maintenance of abstinence in alcohol dependent patients, acamprosate was first mainly used in European Countries, but was approved by the US Food and Drug Administration (FDA) in 2004. It is meanwhile prescribed in 40 countries worldwide and in clinical use for more than 20 years. How the intervention might work Acamprosate's precise mechanism of action is still under investigation. Current evidence suggests a multiple mediation of effects, with modulations of the N\u2010methyl\u2010D\u2010aspartic acid (NMDA) receptor, which was early identified as one central mode of operation ( Zeise 1993 ), being still considered as the primary mechanism of action ( Littleton 2003 ). Acamprosate acts as a partial co\u2010agonist with enhanced functioning at low levels of endogenous activators, and inhibitation at high levels ( Lipha 2002 ; Naassila 1998 ). The increased calcium influx through NMDA glutamate receptors during alcohol withdrawal induces a state of neuronal hyperexcitability associated with physical symptoms of withdrawal and an increased desire to start drink again. By inhibiting the calcium influx, acamprosate is restoring the balance between inhibitory and excitatory neurotransmitters. Besides its effects on acute withdrawal, acamprosate additionally attenuates conditioned reactions (\"pseudo\u2010withdrawal\") and opponent processes associated with drinking related cues ( Cole 2000 ; Littleton 1995 ), the latter explaining the potency of the substance to prevent a relapse after physical symptoms of withdrawal have disappeared. Evidence suggests that besides its effects on withdrawal\u2010related processes, acamprosate's reductive effect on drinking is also attributable, at least in part, to its potential to reduce rewarding effects of alcohol ( Cano\u2010Cebrian 2003 ; McGeehan 2003 ). Why it is important to do this review Acamprosate has already been subject to various meta\u2010analyses, which indicate small to moderate, but significant effects of the substance in maintaining abstinence in alcohol dependent patients ( Berglund 2003 ; Bouza 2004 ; Chick 2003 ; Hopkins 2002 ; Kranzler 2001 ; Mann 2004 ; Rosner 2008 ; Schoechlin 2000 ). Within the last years, the primary database for acamprosate has been extended and some of the newer trials partly differ from the previous research in terms of the study design, the trial setting and the source of financial sponsoring. Thus, an update of the database by integrating newer acamprosate trials is likely to increase the validity of conclusions. Besides the statistical integration of primary effects, a systematic assessment of bias risks allows to discuss the demonstrated effects against the background of methodological considerations. The present review is the first review on acamprosate undertaken within the framework of the Cochrane Collaboration. Available Cochrane reviews on alcoholism treatment evaluate 12\u2010step programmes for alcohol dependence ( Ferri 2006 ), brief interventions for heavy alcohol users admitted to general hospital wards ( McQueen 2009 ) and for primary care populations ( Kaner 2007 ) as well as opioid antagonists for the treatment of alcohol dependence ( Srisurapanont 2005 ).",
        "summary": "Randomized controlled trials suggest that acamprosate may be more effective than placebo for reducing return to drinking but may increase diarrhea. Acamprosate and naltrexone seem to be equally effective but naltrexone seems to cause a wider range of adverse effects. Adding naltrexone to acamprosate did not confer additional benefit and increased adverse effects. Trials including around 6000 people found that acamprosate (with daily doses ranging from 1000 to 3000 mg) reduced return to drinking immediately after detoxification, and at 3 to 12 months after treatment, compared with placebo, as well as increasing cumulative and continuous abstinence. However, based on trials including around 5000 people, acamprosate may be associated with increased rates of diarrhea. There was no clear evidence of any other adverse events attributable to acamprosate compared with placebo but it should be noted that most of the analyses of other adverse events (vertigo, dry mouth, poor concentration) only included a single trial. Trials including around 700 people did not demonstrate any difference in rates of return to drinking, return to heavy drinking, or cumulative abstinence when acamprosate plus naltrexone was compared with placebo, but the combination was associated with increased rates of diarrhea, poor appetite, nausea, and vomiting. Trials including around 800 people did not demonstrate any difference in rates of return to drinking, return to heavy drinking, or cumulative abstinence when acamprosate was compared with naltrexone, but naltrexone was associated with lower rates of diarrhea, while acamprosate was associated with lower rates of nausea, vomiting, somnolence, and fatigue. Most of the trials had weak methods, including inadequate randomization and lack of blinding, which reduces confidence in the reliability of their results and the pooled estimates derived from them."
    },
    "CD012785": {
        "query": "What are the benefits and harms of prostacyclin for people with pulmonary arterial hypertension?",
        "document": "Background Description of the condition Pulmonary hypertension is defined as a mean pulmonary arterial pressure (mPAP) exceeding 25 mmHg measured by right heart catheterisation ( Gali\u00e8 2016 ). More than 50 diseases across five main categories (World Health Organization (WHO) type 1 to 5) are reported as potential aetiologies ( Simonneau 2013 ). Many cause progressive disease, with associated right ventricular strain, hypertrophy, remodelling within the pulmonary vasculature and premature death. In the later stages of the disease, cardiopulmonary dysfunction leads to burdensome symptoms, such as exercise intolerance, syncope, oedema and breathlessness. The development of several specific therapies for WHO Group 1 pulmonary arterial hypertension (PAH) has led to heightened interest in the condition. Unfortunately, most people presenting with PAH have progressed to advanced disease at the time of specialist referral ( Humbert 2006 ; Thenappan 2007 ); and the true prevalence of pulmonary hypertension is likely under\u2010recognised ( Gali\u00e8 2016 ). Modern therapies have reduced morbidity and improved survival ( Thenappan 2007 ); however the risks and side effects warrant their careful selection. Prostaglandins have an unusual spectrum of side effects and almost all patients on an effective dose will have significant prostaglandin\u2010related side effects. The WHO classification system for pulmonary hypertension is widely used, grouping disorders based on underlying mechanisms ( Simonneau 2013 ). This provides a framework for treatment, as pathophysiology varies greatly between groups. Group 1 comprises PAH, formerly termed \"primary pulmonary hypertension\", which refers to precapillary flow obstruction, independent of venous thromboembolism or hypoxaemic lung disease ( Badesch 2009 ). PAH is a rare disease, with an estimated prevalence of 10 to 52 cases per million ( Ling 2012 ; Peacock 2007 ). The gold standard diagnostic tool in pulmonary hypertension is right heart catheterisation, which determines a diagnosis of pulmonary hypertension, and further characterises the aetiology according to the WHO classification ( Gali\u00e8 2016 ). PAH is determined as pulmonary hypertension (mean pulmonary arterial pressure (mPAP) equal to or higher than 25 mmHg) with a normal back pressure from the heart (a pulmonary arterial wedge pressure equal to or less than 15 mmHg) and a pulmonary vascular resistance (PVR) more than 3 Wood units measured during right heart catheterisation. A pulmonary arterial wedge pressure higher than 15 mmHg indicates contributing left heart dysfunction. Other baseline evaluation includes high\u2010resolution computed tomography (HRCT) and ventilation\u2012perfusion (VQ) scanning to rule out other causes (non\u2010WHO Group 1); and exercise testing such as six\u2010minute walk distance (6MWD) ( Gali\u00e8 2016 ) for baseline evaluation and prognostication. Beyond confirmation of the diagnosis, right heart catheterisation and other baseline tests assist to stratify risk of progression which assists in directing treatment. Goals of therapy are relief of symptoms, improved exercise capacity, improved quality of life, arresting progression and reducing mortality. People with PAH often respond to disease\u2010specific modifying therapies, including calcium channel blockers, prostacyclin analogues, endothelin receptor antagonists and phosphodiesterase\u20105 inhibitors. In contrast, indications for advanced therapies in other groups of pulmonary hypertension are less clear cut and treatment of underlying conditions is first line ( Gali\u00e8 2016 ). Description of the intervention Prostacyclin is endogenously synthesised by endothelial cells using the cyclo\u2010oxygenase arachidonic pathway. Prostacyclin exerts vasodilatory, antithrombotic and antiproliferative effects that are essential for endothelial function ( Mitchell 2014 ). The principal target of prostacyclin is the IP G protein\u2010coupled receptor in the smooth muscle of arterioles. Its activation triggers intracellular cyclic adenosine monophosphate formation, activating protein kinase A, which mediates vasodilation of the pulmonary arteries, inhibition of platelet aggregation, and relaxation of the smooth muscle ( Humbert 2015 ). Disequilibrium between vasodilating mediators, such as a reduction in the normal release of prostacyclin, and increased release of vasoconstricting mediators, such as thromboxane A2, plays a causative role in PAH ( Christman 1992 ; Sitbon 2016 ). Currently there are three prostacyclin analogues available \u2010 epoprostenol, iloprost and treprostinil. Selexipag is a selective IP prostacyclin receptor agonist that is structurally distinct from prostacyclin. It is rapidly hydrolysed to a long\u2010acting metabolite that binds to IP receptors, resulting in the same actions as prostacyclin \u2010 vasodilation, inhibition of platelet aggregation, and anti\u2010inflammatory effects ( Noel 2017 ). How the intervention might work Epoprostenol directly vasodilates the pulmonary and systemic arterial vasculature, and has been demonstrated in previous trials to reduce ventricular afterload, pulmonary vascular resistance (PVR) and platelet aggregation, and to increase cardiac output ( Sitbon 2016 ). The key attributes of synthetic prostacyclin agents are prostacyclin's short half\u2010life at room temperature (minutes) and that it mainly only exerts local effects ( Mitchell 2014 ). The first synthetic agent (epoprostenol) demonstrated significant efficacy as a therapeutic agent in the improvement of haemodynamic parameters, exercise capacity, and mortality ( Barst 1996 ). However it is not without drawbacks. Its short half\u2010life requires continuous intravenous infusion, via a central venous catheter and continuous pump, requiring central line placement, and potentially introducing the risk of central line\u2010associated blood stream infection ( Kallen 2008 ). Initial preparations were required to be refrigerated or kept on ice; however newer preparations have a more stable half\u2010life of 24 hours ( Sitbon 2012 ). Iloprost is a prostacyclin analogue that is most frequently used via inhalation. It has a slightly longer half\u2010life of 20 to 30 minutes, but still requires 5 to 10 inhalation doses throughout the day. Treprostinil has a much more stable half\u2010life of four hours, and can be administrated at much lower infusion rates via a subcutaneous or intravenous pump ( Tapson 2006 ). However, treprostinil is metabolised by cytochrome P450 (CYP)2C8 in the liver and its metabolites are renally excreted, so clearance may be affected by hepatic impairment. Cumulative effects of treprostinil can occur if used with antihypertensives or anticoagulants ( Simonneau 2002 ). Beraprost is also available as an orally active prostacyclin analogue, theorised to maintain a stable structure due to its cyclopenta benzofuranyl skeleton. It acts by binding to prostacyclin membrane receptors to inhibit the release of calcium, leading to relaxation of smooth muscle cells and vasodilation, and inhibiting platelet aggregation. Given three times a day, it has previously exhibited improved outcomes in those with intermittent claudication due to peripheral arterial disease ( Melian 2002 ). For all prostacyclin agents, dose titration is individualised according to the individual patient. A characteristic pattern of adverse effects, particularly systemic hypotension, but also including flushing, diarrhoea, and muscle pains ( Barst 1996 ; Sitbon 2016 ), may limit dose escalation. Indeed the dose is often up\u2010titrated until side effects are evident. This makes patient and investigator concealment (blinding) somewhat problematic in clinical trials. The method of delivery and the drug itself are expensive. Furthermore, therapy must be continuous, as abrupt withdrawal may precipitate rebound pulmonary hypertension, which can be fatal. Selexipag is an oral selective prostacyclin receptor (IP receptor) agonist that works similarly to prostacyclin. It is postulated that the density of prostacyclin receptors varies between patients, therefore requiring complex personally tailored dosing of prostacyclin analogues, however, clinical trials in selexipag indicates patients respond similarly to the low\u2010, medium\u2010 and high\u2010dose regiments, therefore it offers a potentially more stable drug, with less complex administration and titration ( GRIPHON ). Why it is important to do this review Evidence in the literature suggests that prostacyclin analogues are efficacious in the treatment of PAH; however the treatment may come with considerable risks and side effects. The purpose of this review is to summarise the available published data regarding the relative efficacy and safety of prostacyclin analogues, in particular on haemodynamic response, and on participant\u2010centred outcomes, such as exercise tolerance, adverse effects, and quality of life. Unfortunately, patients with PAH usually have advanced disease at presentation. Early diagnosis and management of this progressive condition offers a greater scope to delay or prevent onset of end\u2010stage symptoms. Recognising the presence of pulmonary hypertension as well as the underlying cause allows early initiation of appropriate treatment and potentially avoidance of end\u2010stage disease states.",
        "summary": "Prostacyclin analogues and prostacyclin receptor agonists in adults and children with primary or secondary pulmonary hypertension improved six\u2010minute walking distance (6MWD), pulmonary vascular resistance (PVR), cardiac index (CI) , and right arterial pressure (RAP). Only prostacyclin analogues lowered mortality, increased rates of improvement in World Health Organization (WHO) functional status, reduced dyspnea, and improved quality of life. Reported adverse events were more frequent with prostacyclin analogues and prostacyclin receptor agonists than with placebo/conventional treatment; types of adverse events experienced were similar with both drug classes . Moderate\u2010 to low\u2010certainty evidence suggests that, compared with placebo or conventional treatment, prostacyclin analogues reduced mortality (on average, 18 vs 30 per 1000 people); slightly improved 6MWD by around 20 meters; improved mean pulmonary arterial pressure (mPAP), PVR, CI, and RAP; and modestly improved dyspnea and quality of life. In addition, more people showed improvement in WHO functional status (on average, 258 vs 127 per 1000 people). Subgroup analyses by route of administration (intravenous, subcutaneous, oral, and inhaled) for these outcomes were underpowered. Adverse events were more common with prostacyclin analogues than with placebo/conventional treatment, including higher rates of vasodilatation, headache, jaw pain, diarrhea, leg pain, nausea and vomiting, pain in extremity, myalgia, upper respiratory tract infection, and infusion site reaction. Moderate\u2010 to high\u2010certainty evidence shows that, compared with placebo, prostacyclin receptor agonists slightly improved 6MWD (by about 13 meters) and improved PVR, CI, and RAP but did not lower mortality, mPAP, or dyspnea, nor increase rates of improvement in WHO functional class. Some outcomes were assessed only in a single RCT with 43 participants. Adverse events were more frequent with prostacyclin receptor agonists than with placebo and were similar to those experienced with prostacyclin analogues."
    },
    "CD005370": {
        "query": "What are the benefits and harms of antithrombin III in critically ill patients?",
        "document": "Background Description of the condition Despite advances in the medical field, growing numbers of patients are becoming critically ill. Each year, 5,700,000 people in the USA are admitted to intensive care units (ICUs) ( Wunsch 2008 ). Critical illness is characterized by cellular immune dysfunction, vascular damage and uncontrolled hyperinflammation, even when the cause of illness is not infection. In critical illness, a systemic activation of coagulation may occur which, at its worst, results in a fulminant disseminated intravascular coagulation (DIC). DIC is characterized by simultaneous widespread microvascular thrombosis and profuse bleeding from various sites ( Levi 2004 ). Sepsis resulting from a generalized inflammatory and procoagulant response to an infection is associated with a high risk of mortality. Twenty per cent of patients who develop severe sepsis will die during their hospitalization ( Mayr 2014 ). Septic shock is associated with the highest mortality, approaching 50% ( Mayr 2014 ). This rate increases in the presence of circulatory shock despite aggressive antimicrobial therapy, adequate fluid resuscitation, and optimal care ( Periti 2000 ), and may reach as high as 70% in patients with multiple organ dysfunction ( Polderman 2004 ). The inflammation associated with critical illness is characterized by an increase in the number and activity of numerous molecules, such as platelet activating factor, von Willebrand factor, and tumour necrosis factor. There is a simultaneous increase in the activity of pro\u2010inflammatory and pro\u2010coagulant processes, such as thrombin formation, fibrin deposition at the vascular wall, and the formation of aggregates containing platelets and leukocytes. Leukocyte rolling, adhesion, and transmigration are also important parts of the inflammatory reaction. These processes lead to capillary leakage, severe disturbance of the microcirculation, tissue damage, and eventually multiorgan failure and death ( Becker 2000 ). Description of the intervention Any proposed treatment of critical illness should aim to eliminate the underlying disorder or condition and to restore microvascular function, hence reducing organ dysfunction ( Levi 2004 ). Antithrombin III (AT III) is primarily a potent anticoagulant with independent anti\u2010inflammatory properties. AT III irreversibly inhibits serine proteases (for example, activated factor X and thrombin) in a one\u2010to\u2010one ratio, with the generation of protease\u2010AT III complexes. Heparin prevents AT III from interacting with the endothelial cell surface by binding to sites on the AT III molecule, competing for the AT III binding site, and reducing AT III ability to interact with its cellular receptor. AT III's anticoagulant effect is thus greatly accelerated (by a factor of 1000) by heparin; heparin reduces AT III's anti\u2010inflammatory properties, weakens vascular protection, and increases bleeding events ( Diaz 2015 ; Opal 2002 ; Rublee 2003 ). Heparin in patients with sepsis, septic shock, or DIC associated with infection may be associated with decreased mortality ( Zarychanski 2015 ). However, the overall effect is still not clear. Major bleeding events related to heparin administration cannot be excluded ( Zarychanski 2015 ) and safety outcomes have yet to be validated in a multicentre trial setting. How the intervention might work The blood concentration of AT III falls by 20% to 40% in septic patients, and these levels correlate with disease severity and clinical outcome ( Opal 2002 ; Wiedermann 2002 ). This reduction in concentration is due to the combined effect of decreased production of AT III in the liver, inactivation by the enzyme elastase, which is increased during inflammation, and loss of AT III from the circulation into tissues through inflamed and leaking capillary blood vessels. These processes reduce the half\u2010life of AT III from a mean of 55 hours to 20 hours ( Fourrier 2000 ). The main mechanism of AT III depletion in severe sepsis is linked to consumption of the molecule. It is this depletion of AT III that has prompted research into the potential benefits of replenishing AT III levels. Investigators have often tried to increase the antithrombin concentration to supranormal values because the activity of pro\u2010inflammatory and pro\u2010coagulant molecules are increased in critically ill patients. Thus artificially high levels of AT III may be required to overcome the inhibitory effect of thrombin and other such serine proteases. This is because the normal serum concentration of AT III does not necessarily reflect the amount bound to endothelial receptors and appears insufficient ( Fourrier 2000 ). Finally, by blocking the actions of thrombin, AT III may have anti\u2010angiogenic and antitumour properties ( Larsson 2001 ). Why it is important to do this review Although critically ill patients are a heterogeneous population, they are characterized by having systemic inflammation, no matter what the cause of their illness. This inflammation causes further damage to tissues and organs and can result in multiple organ failure and death. The process of inflammation can be modified by AT III, whether or not clotting is abnormal, and it is possible that AT III can reduce the high death rate or permanent damage experienced by critically ill patients. The benefit of AT III supplementation in critically ill patients is still controversial and its efficacy is still debated ( Tagami 2014 ; Tagami 2015 ).",
        "summary": "It seems that antithrombin III in critically ill patients has no impact on mortality, but may increase the risk of bleeding compared with placebo/no antithrombin III. The evidence for other clinically important outcomes is insufficient to give a complete benefit/harm profile, therefore drawing any firm conclusion is difficult. Moderate\u2010quality evidence showed that the use of antithrombin III in critically ill patients had no benefit in terms of mortality when compared with placebo/no antithrombin III. There were also no benefits of antithrombin III detected for other clinically important outcomes (respiratory failure, duration of mechanical ventilation or intensive\u2010care stay), but some of this evidence was very low quality. There was, however, moderate\u2010quality evidence that showed antithrombin III was more commonly associated with bleeding (on average 202 versus 128 per 1000 people). There was some suggestion of a reduction in the risk of sepsis among survivors (measured using the multiple organ failure (MOF) score or the APACHE I or II scores); however, these analyses had very small numbers of participants and the evidence was very low quality. One antithrombin III\u2010related complication was reported (intracranial bleeding), but the incidence was low (2%); therefore, even with 2454 participants, this may not have been enough to detect a difference between groups if one was present. Both the number of events and participants in the analysis for non\u2010antithrombin III\u2010related complications was even lower, making it even less likely to detect a difference between groups. It is worth noting that the trials varied considerably in terms of the populations recruited (from premature infants to the elderly intensive\u2010care with a wide range of conditions), regimens of antithrombin III, and the control used (albumen, frozen plasma, unspecified placebo, no treatment, unclear control). Although subgroup analyses were conducted for all\u2010cause mortality based on population (trauma, obstetrics, pediatrics, severe sepsis, disseminated intravascular coagulation) and the use of adjuvant heparin therapy (complete/partial, none) that had similar result to the main analysis, there was still substantial variation across trials, making applicability of the pooled estimates to any particular population/treatment combination uncertain."
    },
    "CD004625": {
        "query": "Can 6\u2010monthly or 12\u2010monthly routine scale and polish treatment improve periodontal health in healthy dentate adults?",
        "document": "Background Description of the condition 'Periodontal (gum) disease' is a broad term that encompasses a cluster of diseases that result in inflammatory responses and chronic destruction of the tissues that surround and support the teeth, namely the gingiva, periodontal ligament, cementum and alveolar bone (collectively referred to as the 'periodontium'). The diseases can be reversible (gingivitis) or can cause irreversible destruction of tissues (periodontitis). Dental plaque is the principal aetiological factor in the pathogenesis of the most prevalent forms of periodontal disease ( Lang 2009 ). Plaque is necessary but is not sufficient for periodontal disease to occur. The host response, the modifying effect of various risk factors and the bacterial attack from dental plaque can account for a variety of disease patterns, both between different people and between different sites in the mouth within the same person. Calcified plaque (calculus) does not have a major role in the pathogenesis of periodontal disease, although it does act as a 'retention web' for bacteria ( Ismail 1994 ), and reduces the effectiveness of personal oral hygiene control. Gingivitis is a reversible disease and can be defined as the presence of gingival bleeding on probing (where the gum bleeds on touch) without loss of connective tissue attachment. Gingivitis is a precursor to periodontitis in some people, that is, gingivitis does not inevitably progress to periodontitis. Periodontitis can be defined as the presence of gingival inflammation at sites where there has been a pathological loss of attachment and bone ( AAP 2015 ). This loss of attachment contributes to pocket formation and the cementum may become contaminated by micro\u2010organisms and their products ( Jenkins 2003 ). Our improved understanding of the causes and development of periodontal disease led the American Academy of Periodontology (AAP) and the European Federation of Periodontology (EFP) to update classification scheme for periodontal and peri\u2010implant diseases and conditions in 2018 ( Caton 2018 ). This new framework also allows the destructive irreversible periodontal disease to be further classified on stage (severity) and grade (risk of progression). Epidemiological studies of periodontal diseases are difficult to interpret due to the diversity of measures used to describe and quantify disease and the absence of uniform definition and classification. This is reflected in the World Health Organization Global Data Bank estimates ( WHO 2004 ), which state that the prevalence of moderate\u2010severity disease ranges from 2% to 67% and that advanced disease occurs in 1% to 79% of the population. Gingivitis is highly prevalent in most populations and at most ages ( Albandar 2002 ; Corbet 2002 ; Sheiham 1986 ), with global values ranging from 50% to 90%. In the UK, the 2009 Adult Dental Health Survey reported that 45% of dentate adults had some periodontal pocketing of 4 mm or more and 8% had deep pocketing (of 6 mm or more), and that 66% of adults aged 55 years and over had some loss of attachment of 4 mm or more and 21% had loss of attachment of 6 mm or more ( White 2011 ). The prevalence of pocketing and loss of attachment increased with age. For example, the proportion of dentate adults with some loss of attachment increased from 61% among people aged 16 to 24 years to 76% among people aged 75 to 84 years. The Global Burden of Diseases study of 2010 estimated that the global age\u2010standardised prevalence of severe periodontitis was 10.8% or 743 million people worldwide ( Kassebaum 2014 ). The goals of periodontal therapy have been defined in many different ways. Some authors have defined the ultimate aim of periodontal treatment as being to control disease progression or achieve a rate of progression which is compatible with a functional dentition for the person's lifetime ( Pilot 1980 ; Sheiham 2002 ; Wennstrom 1990 ). Other authors have defined the key goals as improving periodontal health and thereby satisfying a person's aesthetic and functional needs or demands. Currently accepted clinical signs of a healthy periodontium include the absence of inflammatory signs of disease such as redness, swelling, suppuration and bleeding on probing; maintenance of a functional periodontal attachment level; minimal or no recession in the absence of interproximal bone loss and, where present, functional dental implants ( AAP 2001 ). A fundamental component of the preventive management of periodontal disease is the control of dental plaque by the patient. Hence, patient education and training in personal oral hygiene should form an integral part of any treatment plan for a person with periodontal disease. Conventional periodontal therapy also includes non\u2010surgical treatment and a variety of surgical approaches ( Mailoa 2015 ; Needleman 2002 ; Tonetti 2014 ). The precise choice of intervention may be influenced by the clinical severity of the disease, with surgery generally reserved for cases of advanced disease to allow for adequate access to, and full debridement of, areas with deep pocketing. Description of the intervention Scaling and polishing of the teeth by a dentist or a dental care professional (DCP) (dental therapist or dental hygienist), also known as prophylaxis, professional mechanical plaque removal or periodontal instrumentation, is a non\u2010surgical intervention that is intended to supplement (and is not a substitute for) the patient's home\u2010care plaque control. This treatment is frequently provided as part of the dental recall appointment ( Riley 2013 ). Scaling is the removal of plaque, mineralised plaque deposits (also referred to as calculus or tartar), debris and staining from the crown and root surfaces of the teeth. Specially designed sharp dental instruments ('hand scalers and curettes') or ultrasonic scalers can be used to perform the scaling procedure. Polishing is the mechanical removal of any residual extrinsic stains and deposits, typically undertaken by using a rubber cup or bristle brush loaded with a prophylaxis paste. Scaling and polishing can be used with or without a variety of adjuncts such as antimicrobial agents (either topical or systemic), gingival crevice irrigation and root planing. Root planing is a procedure for smoothening the root surface of a tooth that involves the \"removal of cementum or surface dentin that is rough or impregnated with calculus, toxins or microorganisms\" ( Greenstein 1992 ). The rationale for root planing is to allow the gingival tissue to heal close to the root, shrinking the tissue and reducing the depth of the pocket that has formed ( Bonito 2004 ). Within the confines of this updated Cochrane Review a 'routine scale and polish' (S&P) is defined as scaling or polishing (or both) of the crown and root surfaces of teeth to remove local irritational factors (plaque, calculus, debris and staining), that does not involve periodontal surgery or any form of adjunctive periodontal therapy such as the use of chemotherapeutic agents or root planing. The definition includes both supragingival and subgingival scaling. The use of the term 'routine' is intended to highlight two important features of the intervention considered in this review. First, 'routine' indicates that the review focuses on S&P treatment as it is routinely delivered in everyday general dental practice settings (i.e. the review is concerned with evaluating the effects of S&P treatments in 'real\u2010world' primary care settings). Second, the term 'routine' indicates that the S&P is \"a regular course or procedure\" ( Oxford Dictionary 1995 ), that is, the S&P is an intervention that is typically provided at 'regular intervals' to patients, but without specifying any one particular frequency (e.g . every six months, every 12 months) at which patients may receive this intervention. In this context, a key objective of this review is to evaluate the effects on periodontal health of providing S&P treatments at different recall intervals in primary care settings. How the intervention might work Scaling and polishing of the teeth removes local irritational factors (plaque, calculus, debris and staining). The removal of these irritants at regular intervals may reduce the occurrence of gingivitis and, over time, may prevent progression to periodontitis or reduce the rate of progression of periodontitis. Why it is important to do this review Cochrane Oral Health undertook an extensive prioritisation exercise in 2014 to identify a core portfolio of titles that were the most clinically important ones to maintain on the Cochrane Library ( Worthington 2015 ). This review was identified as a priority title by the periodontal expert panel ( Cochrane Oral Health priority review portfolio ). Scaling and polishing of the teeth is a commonly provided intervention in general dental practice, with significant cost implications. In the UK, approximately 46% of all adult courses of treatment provided under the National Health Service (NHS) (General Dental Services) regulations \"consist of the patient having nothing more than an examination (and a) scale and polish\" ( DoH 2000 ). In 2016/2017, 2.3 million S&P courses of treatment were provided for NHS patients in Scotland at a gross cost to the NHS of GBP 33.2 million ( Primary Care Dentistry in Scotland 2017 ), and in England, 13.1 million S&P courses of treatment were provided for NHS patients ( NHS Dental Statistics for England 2017 ). Scaling and polishing is also frequently provided for patients irrespective of their risk of developing periodontal disease. In a survey of general dental practitioners' preventive recommendations in western New York State, 86% of respondents stated that they would recommend scaling and polishing every six months for 'low\u2010risk' patients of all ages (a 'low risk' patient was defined as a people having \"adequate brushing and flossing habits\" and \"no history of periodontal disease\") ( Frame 2000 ). There is ongoing debate over the clinical effectiveness and cost effectiveness of routine scaling and polishing of teeth and how often it should be provided. This debate is complicated by the fact that a 'routine S&P' is not a precisely defined intervention in periodontal disease management and there is no universally accepted definition of the term. In the USA, the term 'oral prophylaxis' is most often used and has been defined as \"the removal of plaque, calculus and stain from exposed and unexposed surfaces of the teeth by scaling and polishing as a preventive measure for the control of local irritational factors\" ( AAP 1992 ). We have clearly defined the term routine S&P used within this review in the Description of the intervention section. The role and contribution of DCPs (dental hygienists and dental therapists) in maintaining periodontal health has increased in recent years. Any differences in treatment outcome following intervention by a dentist or DCP are not well understood and require investigation. This review updates previous versions ( Beirne 2005 ; Beirne 2007 ; Worthington 2013 ). Since the first version of this review was published ( Beirne 2005 ), the evidence base has developed considerably. We have made appropriate amendments to the initial review objectives and the eligibility criteria to ensure that the review continues to provide the most relevant evidence for dentists working in primary care and patients attending for treatment in general dental practice. We have provided a detailed justification for these amendments in the Differences between protocol and review . We have also provided an updated explanation of the term 'routine S&P' in the Description of the intervention section.",
        "summary": "Scheduled 6\u2010monthly or 12\u2010monthly scale and polish (S&P) does not have a direct positive effect on periodontal health compared to no treatment. Scheduled 6\u2010monthly or 12\u2010monthly scale and polish (S&P) has no impact on gingivitis or probing depth compared with no treatment in patients followed up for 24 to 36 months (high\u2010certainty evidence). Similarly, low\u2010certainty evidence suggests that 6\u2010monthly or 12\u2010monthly S&P may not improve plaque levels (24\u2010month follow\u2010up). However, 6\u2010monthly or 12\u2010monthly S&P modestly reduces calculus levels compared with no treatment over a follow\u2010up period of 24 to 36 months (high\u2010certainty evidence). It seems that more patients who undergo routine S&P perceive their mouth as very clean compared with those receiving no treatment (523 vs 286 per 1000 people; low\u2010certainty evidence). Despite this perception, scheduled S&P did not result in any improvement in patient quality of life (high\u2010certainty evidence). When 6\u2010monthly was compared with 12\u2010monthly S&P, little or no difference was evident between the two groups with regard to any of the above outcomes except calculus; analysis shows greater improvement in calculus levels associated with 6\u2010monthly than with 12\u2010monthly S&P (high\u2010certainty evidence). None of the identified studies reported on adverse events."
    },
    "CD003414": {
        "query": "In subfertile couples, how does a modified frozen\u2010thawed embryo transfer cycle with chorionic gonadotropin triggering compare with alternative transfer protocols?",
        "document": "Background Description of the condition For subfertile couples undergoing assisted reproductive technology (ART), embryo transfer can be performed with either fresh or frozen\u2010thawed embryos. Pregnancy rates following frozen\u2010thawed embryo transfer (FET) treatment cycles have historically been found to be lower than following fresh embryo transfer. Nevertheless, FET increases the cumulative pregnancy rate, reduces cost and is relatively simple to undertake. Moreover, it can be accomplished in a shorter time period than repeated in vitro fertilisation (IVF) or intracytoplasmic sperm injection (ICSI) cycles accompanied by fresh embryo transfer. Description of the intervention FET has been carried out using a variety of cycle regimens; spontaneous ovulatory cycles (natural cycle), cycles in which the endometrium is artificially prepared by oestrogen and progesterone hormones and is called hormone therapy (HT) cycle, and cycles in which ovulation is induced by drugs (ovulation induction cycle). For FET to be successful the age of the embryos after thawing has to be synchronised with the age of the endometrium on the day of embryo transfer. Thus, endometrial receptivity and synchronisation between embryonic and endometrial development are important factors in the process of embryo implantation. Performing FET in a monitored natural cycle has the advantage that no medications are used, making such cycles preferable to many women. It is only feasible for women with regular ovulatory cycles, however, even in women with regular menstrual cycles, ovulation may not always occur and the timing of FET can also be problematic. Monitoring of the cycle requires several pelvic ultrasound scans to confirm follicular development and to time the commencement of urine testing for detection of the luteinising hormone (LH) surge. In addition, a further scan for ultrasonic evidence of ovulation may be required. Spontaneous endometrial development in the follicular phase is affected by age, and this may be a contributing factor in the lower pregnancy rate in older women ( Sher 1991 ). To avoid some of these problems, ovulation induction agents, such as tamoxifen, clomiphene citrate, gonadotrophins or a combination of clomiphene and gonadotrophins have been employed ( Mandelbaum 1987 ). However, even with their use, cycle cancellation rates can be high. An alternative method for establishing an endometrium receptive to implantation is an artificial, hormonally\u2010controlled cycle (HT) using sequentially administered exogenous oestrogen and progesterone. Such a regimen was first used in women without ovarian function receiving embryos derived from donor oocytes ( Lutjen 1984 ). In women with remaining ovarian function, a gonadotrophin\u2010releasing hormone agonist (GnRHa) was used to temporarily suppress ovarian function and render the woman functionally agonadal prior to inducing an artificial cycle with oestrogen and progesterone. Using such a regimen, implantation and pregnancy rates in donor oocyte recipients with retained ovarian function were similar to those in recipients without ovarian function ( Borini 1995 ; Flamigni 1993 ; Pados 1992 ). However, these cycles are more expensive, and the GnRHa can have adverse effects and delay the resumption of spontaneous ovulation if FET fails. A simplified regimen, retaining the benefits but reducing cost and adverse effects and consisting of exogenous oestrogen and progesterone only (without a GnRHa), has also been used in women with remaining ovarian function ( Jaroudi 1991 ; Lelaidier 1992 ). With this approach, the initiation of orally administered exogenous oestrogen on day one of the cycle prevents follicular recruitment by suppressing follicle\u2010stimulating hormone (FSH) and consequently spontaneous ovulation is avoided. Estradiol implants have also been shown to suppress the hypothalamic pituitary ovarian axis resulting in adequate endometrial preparation in women with retained ovarian function ( Ben\u2010Nun 1997 ). The advantage of hormone regimens is a high level of control and flexibility in the timing of transfer. The length of the follicular phase can be varied without detriment to implantation or pregnancy rates ( Leeton 1991 ; Navot 1989 ), and the cycle cancellation rate is low. However, cost is greater, particularly if a GnRHa is used. Fixed regimens of oestrogen and progesterone may be inadequate for proper endometrial development in certain circumstances. In women with functioning ovaries, the possibility has been expressed that stimulatory factors of embryonic origin (such as human chorionic gonadotrophins) could lead to the production of ovarian substances (e.g. androgens or certain peptides) with an effect on endometrial quality. If a pregnancy occurs, oestrogen and progesterone must be continued until placental autonomy is established to replace the absent corpus luteum. Most artificial cycle protocols mimic the natural cycle, but the active substances used and routes of delivery of oestrogen and progesterone vary. Oestrogen may be administered in the form of oral tablets, transdermal patches, subcutaneous implants and vaginal rings or tablets. Progesterone may be given in the form of oral tablets, intramuscular injections and intravaginal suppositories or rings ( Devroey 1998 ). Theoretically, ovulation induction can be offered to women with irregular or anovulatory cycles undergoing FET. The possible advantage is to induce ovulation with natural LH surge and semi\u2010natural endometrial development. The development of a corpus luteum with production of natural oestrogen and progesterone helps support the luteal phase and early pregnancy without the need of exogenous HT. The use of clomiphene can be associated with an anti\u2010oestrogenic effect on the endometrium. Other ovulation induction agents include oral anti\u2010oestrogens such as tamoxifen and aromatase inhibitors such as letrozole. Drawbacks of ovulation induction cycles include drug exposure and its possible adverse effects, and risks including high cancellation rate due to under or over response, the cost of gonadotrophins, the need for intense monitoring and the limited flexibility in connection with FET timing. For these reasons and in the absence of evidence supporting any benefit compared to other cycle regimens, most centres do not use ovulation induction. How the intervention might work All methods of endometrial preparation aim to achieve a state of the endometrium that will be receptive to replaced frozen\u2010thawed embryos. The natural ovulatory cycle is the most physiological method. Using oestrogen and progesterone preparations as HT aims to prepare the endometrium in two stages. The first phase is using oestrogen preparations mimicking the follicular phase of a natural cycle. Following that progesterone preparations are added to oestrogen to mimic the luteal phase. Another method is the use of clomiphene tablets or human menopausal gonadotrophins (HMG) to induce ovulation and build up the endometrium. Why it is important to do this review Early studies reported pregnancy rates in artificial cycles to be equivalent to those obtained after FET in spontaneous ovulatory cycles, with low cancellation rates ( de Ziegler 1990 ; Frydman 1988 ; Meldrum 1989 ; Schmidt 1989 ; Troup 1991 ). Some subsequent studies (often containing small numbers) reported better outcomes in artificial than in natural cycles ( Davies 1991 ; Mausher 1991 ), while others found no difference ( Irianni 1992 ; Sathanandan 1991 ). However, concern about the adequacy of the various replacement regimens and the possibility of higher rates of early pregnancy loss has also been expressed ( Lelaidier 1992 ). Therefore, uncertainty remains as to which type of cycle regimen is superior and many fertility clinics use a mixture of protocols for FET. The clinical effectiveness of the different approaches can only be determined by randomized controlled trials (RCTs) comparing the three cycle regimens (natural, HT and ovulation induction). This review aimed to compare the outcome of FET in an artificial HT cycle or an ovulation\u2010induction cycle with that of a monitored, natural cycle. The findings may be of interest to subfertile couples embarking on ART treatment, as well as practitioners and healthcare providers offering ART. This is the first update of a review first published in 2008, which included seven RCTs (1120 women) and concluded that there was insufficient evidence to support the use of one intervention in preference to another ( Ghobara 2008 ).",
        "summary": "Available RCT evidence is insufficient to support or refute the use of a modified natural frozen\u2010thawed embryo transfer (FET) cycle with human chorionic gonadotropin (hCG) triggering over alternative transfer regimens in couples undergoing assisted reproduction technology (ART) procedures. One RCT with 959 women found a lower cycle cancellation rate per woman with modified natural cycle than with hormonally controlled FET (on average, 204 vs 267 per 1000 women), but low\u2010certainty evidence suggested no apparent differences between groups in live birth, ongoing pregnancy, or clinical pregnancy. When researchers compared modified natural cycle FET with hCG trigger versus hormonally controlled FET plus gonadotropin\u2010releasing hormone agonist (GnRHa), low\u2010certainty evidence suggested no evidence of a difference in live birth, miscarriage, or clinical pregnancy. When they compared natural cycle FET versus modified natural cycle FET with hCG trigger, very low\u2010certainty evidence suggested a higher ongoing pregnancy rate with natural cycle FET (on average, 226 vs 107 per 1000 women) but no apparent differences in live birth, miscarriage, or clinical pregnancy. Most of the included studies were small and probably were not sufficiently powered to provide clinically significant results. Furthermore, some outcomes such as ectopic pregnancy and multiple pregnancy were not assessed in any of the comparisons."
    },
    "CD007754": {
        "query": "What are the effects of community\u2010based intervention packages for reducing maternal and neonatal morbidity and mortality?",
        "document": "Background Description of the condition The Millennium Development Goal for maternal health (MDG\u20105) calls for a reduction in maternal mortality by two\u2010thirds by the year 2015 ( Sachs 2005 ). The estimates of maternal mortality suggest that 287,000 maternal deaths occurred worldwide in 2010, and that 85% (245,000) of these deaths occurred in sub\u2010Saharan Africa (56%) and Southern Asia (29%) ( WHO 2012 ). Most of these maternal deaths seem to occur between the third trimester and the first week after the end of pregnancy ( Ronsmans 2006 ). Mortality has also been found to be extremely high on the first and second days after birth ( Hurt 2002 ). Almost 80% of maternal deaths are due to direct obstetric causes, including severe bleeding (haemorrhage), infection, complications of unsafe abortion, eclampsia, and obstructed labour; with other causes being related to the unfavourable conditions created by lack of access to health care, illiteracy and factors related to poverty ( Hoj 2003 ). Many women are estimated to suffer pregnancy\u2010related illnesses (9.5 million), near\u2010miss events, which are the life\u2010threatening complications that women survive (1.4 million), and other potentially devastating consequences after birth ( Ashford 2002 ; Say 2004 ; WHO 2000 ). The consequences of near\u2010miss events on women themselves and their families can be substantial, and recovery can be slow, with lasting sequelae. An estimated 10 to 20 million women develop physical or mental disabilities every year as a result of complications or poor management ( Ashford 2002 ; Murray 1998 ). The long\u2010term consequences are not only physical, but are also psychological, social, and economic ( Filippi 2006 ). Pregnancy\u2010related illnesses and complications during pregnancy and delivery are associated with a significant impact on the fetus, resulting in poor pregnancy outcomes ( Campbell 2006 ). Around the world, 50 million births occur at home without the presence of skilled birth attendance ( UNICEF 2008 ). In the 1970s the World Health Organization promoted training of traditional birth attendants (TBAs) as a major public health strategy to reduce the burden of mortality and morbidities related to pregnancy and childbirth. However, the evidence of the impact of this strategy on maternal and neonatal outcomes is still limited ( Sibley 2007 ). Deaths occurring in the neonatal period (aged 0 to 27 days) account for 41% (3.575 million) of all deaths in children younger than five years ( Black 2010 ). In developing countries, most of the maternal, perinatal and neonatal deaths and morbidities occur at home. The reasons are multi\u2010factorial, including poverty; poor health status of women; illiteracy; lack of information regarding the availability of health services/providers; lack of control on household resources and decision\u2010making authority; poor antenatal and obstetric care both within the community and health facilities; absence of a trained attendant at delivery; inadequate referral system for emergency obstetric care; inadequacy/absence of transportation facilities; and absence of/poor linkages of health centres with the communities ( Ensor 2004 ). The majority of maternal and neonatal deaths could be prevented with early recognition and proper implementation of required skills and knowledge ( Campbell 2006 ; Ray 2004 ). Description of the intervention Soon after the Alma\u2010Ata Declaration, arguments for selective rather than comprehensive primary health care dominated and it was then recognised that community participation was important in supporting the provision of local health services and in delivering interventions at the community level ( Rosato 2008 ). Community participation has long been advocated to build links with improving maternal and child health and there are several trials from south Asia which have evaluated the role of women's groups on maternal and neonatal health. The Makwanpur trial, Nepal implemented a participatory learning cycle (in which they identify, prioritise a problem, select and implement relevant interventions and evaluate the results) through developing women\u2019s groups and found a reduction in maternal mortality by 88% and neonatal mortality by 30% but the same strategy in other trials has shown variable non\u2010significant impacts on maternal and neonatal outcomes ( Azad 2010 ; Tripathy 2010 ). Another set of studies in which services were provided to women and children in the community indicated that, at full coverage, 41% to 72% of newborn deaths could be prevented by available interventions such as tetanus toxoid immunisation to mothers; clean and skilled care at delivery; newborn resuscitation; prevention of hypothermia; exclusive breastfeeding; clean umbilical cord care; management of pneumonia and sepsis. Around half of this reduction is possible with community\u2010based Interventions ( Darmstadt 2005 ). It has also been stated that a significant proportion of these mortalities and morbidities could also be potentially addressed by developing community\u2010based intervention packages (package is defined as delivering more than one intervention via different set of strategies such as community support groups/women groups, community mobilisation and home visitation and training TBAs/community health workers (CHWs), which should also be supplemented by developing and strengthening linkages with the local health systems. Some prior reviews have also generated evidences from reviewing community\u2010based maternal and neonatal interventions trials ( Bhutta 2005 ; Haws 2007 ), but those were not subjected to meta\u2010analyses. Therefore, in this review we not only assess the effectiveness of community\u2010based intervention packages in reducing maternal and neonatal morbidities and mortality and improving neonatal outcomes, but also the impact of different strategies (home visitation, home\u2010based care, community\u2010support groups/women's groups etc.) on the reported outcomes. This review did not evaluate the impact of training TBAs alone ( Sibley 2007 ), or the effectiveness of a health education strategy designed for mothers and other family members on newborn survival ( Thaver 2009 ), as these are being evaluated in other reviews.",
        "summary": "Community\u2010based intervention packages have been shown to reduce maternal mortality and morbidity, still birth as well as neonatal mortality. The degree of benefit varied slightly when different packages were assessed, and intervention packages that involve community mobilization and home visitation appear to show the clearest benefits. Community\u2010based intervention packages should be considered in the planning of maternal and newborn care especially in low\u2010income countries, where most of the studies were conducted. 11 RCTs with 167000 participants showed that community\u2010based intervention packages reduced maternal mortality with a relative reduction of 20% (95% CI 0% to 36%) and neonatal mortality with a relative reduction of 25% (95% CI 17 to 33%) (21 RCTs, 300000 participants). Community mobilization with home visitation and home\u2010based neonatal treatment and building community support or women groups appeared to show the clearest improvements in this outcome. Stillbirth rate and maternal morbidity were also reduced by community\u2010based intervention packages, whereas mean birth weights of newborn infants were similar between the group that received community intervention packages and those that received standard care. Lower rate of hemorrhage, higher rate of obstructed labor and similar rates of puerperal sepsis, eclampsia and spontaneous abortion in participants were seen in those who received community\u2010based intervention packages compared with those who received standard care."
    },
    "CD011118": {
        "query": "Can remotely delivered cognitive\u2010behavioral therapy (CBT) help children and adolescents manage chronic pain?",
        "document": "Background This is the first update of a review published in 2015, Issue 1 ( Fisher 2015 ). Description of the condition Episodes of chronic pain are surprisingly common during childhood and adolescence ( Perquin 2000 ). About 5% to 8% of youth with chronic pain experience significant pain\u2010related disability ( Huguet 2008 ). The most commonly reported chronic pain problems are headache, recurrent abdominal pain, musculoskeletal pain, and back pain ( King 2011 ). Epidemiological studies report that girls experience more pain than boys and that pain increases during early adolescence ( King 2011 ). Paediatric chronic pain is also among the most costly chronic health conditions, with an estimated economic cost of USD 19.5 billion annually in the US alone ( Groenewald 2014 ). Chronic pain can interfere with many aspects of daily life for children, and is associated with elevated symptoms of depression and anxiety as well as difficulty participating in school, sports, and activities with friends and family ( Cohen 2011 ; Gauntlett\u2010Gilbert 2007 ; Kaczynski 2011 ). The detrimental effects of chronic pain can also impact parents, who report significant distress and anxiety ( Jordan 2007 ; Maciver 2010 ). Longitudinal studies indicate that children with chronic pain are at risk for pain, psychiatric comorbidities, and pain\u2010related disability in adulthood ( Noel 2016 ; Shelby 2013 ; Walker 2012 ). Appropriate treatment of chronic pain in childhood has the potential to disrupt long\u2010term trajectories of pain and disability in adulthood. Description of the intervention Psychological therapies, delivered individually or in groups to children and families, can reduce pain and disability in children with chronic pain ( Fisher 2018 ). However, most children do not receive psychological treatment for chronic pain due to barriers to access including geographic distance from treatment centres, cost, and stigma against mental health treatment ( Palermo 2013 ; Peng 2007 ). This has led to consideration of innovative methods of remote treatment delivery, such as via the Internet, computer, or smartphone devices ( Palermo 2009 ). For example, the Internet is widely available to a large number of children and adolescents; in the US 95% of teenagers have access to the Internet through smartphones ( Anderson 2018 ). Different terms are used within this growing field, broadly described as e\u2010health, m\u2010health, telemedicine, telecare, minimal therapist contact, and distance treatment. Here, we adopt the term 'remotely\u2010delivered therapies' to refer to psychological therapies delivered via technology, such as the Internet, smart phone applications, or CD\u2010ROMs. In clinical practice, these technology\u2010delivered programmes may replace or supplement face\u2010to\u2010face treatment for the child's pain problem. We distinguish remotely\u2010delivered therapies from those that rely on clinician contact, such as telemedicine and telecare, where the technology is used to bring the clinician to the patient. In contrast, remotely\u2010delivered therapies are flexible, self\u2010guided treatments most typically delivered without contact with a clinician. How the intervention might work Psychological therapies are used in paediatric pain practice to reduce pain symptoms, disability, and negative mood associated with pain conditions, and to modify social\u2010environmental factors to enhance the child's adaptive functioning ( Fisher 2018 ). This field is currently dominated by cognitive behavioural therapies (CBTs) and behavioural therapies that typically include components such as pain education, relaxation training, biofeedback, hypnosis, cognitive coping skills, behavioural activation, healthy lifestyle habits, and parent operant strategies. Recognising the advantages of reaching more children in their homes with remotely\u2010delivered interventions, early studies relied on low levels of technology, including written self\u2010help manuals, portable biofeedback monitors, and relaxation audiotapes (e.g. Burke 1989 ; McGrath 1992 ). As technological advances became available, intervention delivery options expanded to personal computers via CD\u2010ROM applications and then to programmes/applications via the Internet. The delivery of psychological therapies over the Internet is becoming more common ( March 2008 ; Richardson 2010 ; Tait 2010 ). The potential benefits of a successful programme include improved access, improved scale of coverage, and lowered cost ( Marks 2009 ; Palermo 2009 ). However, the change of a delivery mechanism from face\u2010to\u2010face delivery to remote delivery via technology arguably changes the content, intensity, and force of a treatment. The move away from face\u2010to\u2010face delivery is not simply a change in the route of administration. The transformation of a treatment to a reliance on communication technology (instead of face\u2010to\u2010face interaction with a therapist) may involve critical changes in aspects of the treatment thought crucial to its success. For example, treatment where a therapist is not present may influence treatment participation and impact treatment outcomes ( Fry 2009 ). At the same time, technology platforms may offer critical benefits that are not available in face\u2010to\u2010face models of care, such as 24\u20107 access to skills training. There may also be different therapeutic opportunities available using interactive and communication technologies. As described in the behavioural change model for Internet interventions ( Ritterband 2009 ), user characteristics interact with website characteristics to produce behaviour change. For example, Internet\u2010delivered therapies may work by better matching and designing technology to maximise the therapeutic benefits (e.g. 24\u2010hour access to skills training), or there may be a blend to these solutions that function differently dependent upon user characteristics. Why it is important to do this review Psychological therapies delivered remotely (principally but not exclusively via the Internet) have now developed into stand\u2010alone treatments, and are investigated as stand\u2010alone treatments. A Cochrane Review has previously summarised the evidence of psychological therapies for the management of chronic pain in children and adolescents ( Fisher 2018 ). This was first authored in 2003, and updated in 2009, 2012, 2014, and most recently in 2018. Earlier updates combined remote and face\u2010to\u2010face treatment delivery. However, we believe it is important to separate them so that the evidence can be separately evaluated. This review should be considered a sister review to the Fisher 2018 update, which now excludes treatments delivered via technology. A similar distinction has also been made in the Cochrane Reviews on psychological therapies for the management of chronic pain in adults: face\u2010to\u2010face in Williams 2012 and Internet delivered in Eccleston 2014 .",
        "summary": "Very low\u2010certainty evidence suggests that for children and adolescents (mean age 13 years) with chronic headache or non\u2010headache pain, there was a possible \u2265 50% reduction in headache pain at post treatment (on average, 288 vs 143 per 1000 people) with CBT delivered remotely versus control but little to no difference in other outcomes (disability, depression, anxiety, adverse events) at post treatment and at follow\u2010up. However, patients and parents were generally satisfied with CBT."
    },
    "CD008205": {
        "query": "What hepatic late adverse effects are associated with antineoplastic treatment in children with cancer?",
        "document": "Background Survival rates have greatly improved as a result of more effective treatments for childhood cancer. Today, most children diagnosed with cancer are expected to become long\u2010term cancer survivors ( Curry 2006 ). Five\u2010year disease\u2010free survival now reaches 80% in Europe ( Gatta 2009 ). Unfortunately, the improved prognosis has been accompanied by the occurrence of late, treatment\u2010related complications. In two large cohort studies of childhood cancer survivors, nearly 75% experienced one or more late adverse effects ( Geenen 2007 ; Oeffinger 2006 ). Liver complications are common during and soon after treatment for childhood cancer ( Field 2008 ). However, among long\u2010term childhood cancer survivors the prevalence of chronic liver disease, like fibrosis, cirrhosis and consequently an increased risk of decompensated cirrhosis, malignancies and liver failure, is largely unknown. It has been suggested that survivors of childhood cancer who received chemotherapy, particularly methotrexate, 6\u2010mercaptopurine, 6\u2010thioguanine, busulphan and dactinomycin; bone marrow transplantation (BMT); radiotherapy involving the liver, including total body irradiation (TBI); or hepatectomy ((partial) removal of the liver) are at risk for developing hepatic late adverse effects ( Bresters 2008 ; Castellino 2010 ; Dawson 2005 ; King 2001 ). However, the evidence has been inconclusive. The aetiology (set of causes) of chronic liver disease following treatment for childhood cancer is complex as often more than one aetiologic factor is present. In addition to cancer treatment, other causes of chronic liver disease have been suggested, such as chronic viral hepatitis, iron overload, and potentially sinusoidal obstruction syndrome (SOS, previously termed veno\u2010occlusive disease (VOD)) and graft\u2010versus\u2010host disease (GVHD) ( Locasciulli 1997 ; Rizzo 2006 ; Strasser 1999 ). Regarding chronic viral hepatitis, patients who were treated for childhood cancer before effective hepatitis C virus (HCV) donor screening was implemented are especially at risk for transfusion\u2010acquired HCV infection. Childhood cancer survivors differ from other groups with chronic viral hepatitis in that they acquired the infection at a young age and were likely to have received immunosuppressive or hepatotoxic therapy ( Fink 1993 ; Strickland 2000 ). For better development of primary and secondary hepatic protective strategies in childhood cancer, more insight into the association between cancer treatment and hepatic late adverse effects is essential. Furthermore, for the follow\u2010up of childhood cancer survivors, it is crucial to know the risk and associated risk factors so that patients at greatest risk can be identified and adequate follow\u2010up protocols established to reduce the consequences of hepatic late adverse effects. With increased survival duration after cancer, survivors are at risk for second malignancies and normal diseases of aging which will require additional pharmacotherapy. This additional morbidity risk also underscores the need for understanding the state of liver health in the long\u2010term survivor of a childhood cancer, as long\u2010term impaired liver function may limit treatment of other late effects, like second malignancies. This is an update of the first systematic review evaluating the state of evidence on hepatic late adverse effects after antineoplastic (acting against cancer) treatment for childhood cancer ( Mulder 2011 ).",
        "summary": "Elevated measures of liver and biliary tract injury in children with cancer treated with antineoplastic therapies may suggest treatment\u2010related hepatic late adverse effects. However, few instances were confirmed by liver biopsy, results for other measures of liver synthetic function were not reported, and factors other than the antineoplastic treatment may be responsible, making it difficult to draw firm conclusions. Reviewers identified cohort studies that assessed cellular liver injury (defined as elevations in alanine aminotransferase [ALT] and aspartate aminotransferase [AST] levels), biliary tract injury (defined as elevations in gamma\u2010glutamyltransferase [\u03b3GT] and alkaline phosphatase [ALP] levels), and disturbance in biliary tract function (defined as elevations in bilirubin levels or results from liver biopsy) associated with antineoplastic treatment in children with cancer. Prevalence for levels above the upper limit of normal was as follows: 5.8% to 52.8% for ALT; 5.3% for \u03b3GT; 4.3% to 11.1% for ALP; 8.7% for unconjugated bilirubin; 0% for conjugated bilirubin; and 1.1% for total bilirubin. Prevalence for levels greater than two times the upper limit of normal was 0.9% to 44.8% for ALT and 0.9% for \u03b3GT (not reported for other measures). Prevalence varies considerably for all measures, which may be indicative of the wide variety of cancers included in the studies or may reflect elevation due to factors other than antineoplastic treatment; reviewers stated that radiotherapy involving the liver, higher body mass index, and chronic viral hepatitis increase the risk of late hepatic adverse effects. Some studies reported results of liver biopsies, but these were conducted in only 19 children, and it is unclear whether the pathologies diagnosed are directly related to the antineoplastic treatment. Some included studies seem to have assessed other measures of liver synthetic function (coagulation times or albumin), but reviewers did not report the results for these outcomes. Elevation of enzyme levels in isolation in a heterogeneous population makes it difficult to assess the cause and extent of liver and biliary tract injury, and to determine how these results can be generalized to children with cancer in clinical practice. However, reviewers state that serum ALT may be a helpful screening tool for early detection of hepatic late adverse effects."
    },
    "CD003659": {
        "query": "Does face/eye washing promotion help prevent active trachoma?",
        "document": "Background Description of the condition Trachoma is an infective eye disease caused by the bacterial microorganism Chlamydia trachomatis . Trachoma remains a major cause of avoidable blindness among underprivileged populations in many areas of Africa, Asia and the Middle East, where poverty, overcrowding, poor personal and environmental hygiene favor transmission of the disease. It is estimated that about 146 million people have active trachoma and nearly six million people are blind due to complications associated with repeat infections ( WHO 1997a ). C. trachomatis bacteria is spread from person to person by close contact in overcrowded living conditions, or through contaminated fingers or cloths used by mothers to wipe away discharges on the faces of children ( ICEH 1999 ). Flies, which are attracted to eye and nasal secretions on the faces of infected children, also are believed to be vectors responsible for transmission of the organism ( ICEH 1999 ; West 1991 ). In communities where trachoma is endemic, infection usually begins in childhood and repeat episodes of infection cause distortion of the eyelids (entropion), in\u2010turned eyelashes (trichiasis), corneal abrasion and ultimately blindness due to corneal opacity. Active trachoma is more commonly observed in children ( Taylor 1985 ; West 1991 ). It is characterized by redness and discharge associated with inflammatory thickening of the upper tarsal conjunctiva (mucous membrane lining the inner surface of the upper eyelids) and follicles (whitish elevations within the conjunctiva). A simplified grading system for the assessment of trachoma and its complications in endemic communities has been published ( Thylefors 1987 ) and discussed in a Cochrane review of antibiotics for trachoma ( Evans 2011 ). Description of the intervention Face washing is promoted by the World Health Organization (WHO) program for the global elimination of trachoma as part of the 'SAFE' strategy ( WHO 1997b ; WHO 2011 ). The 'SAFE' strategy consists of s urgery for trichiasis; a ntibiotics for infectious trachoma; f acial cleanliness to reduce transmission; and e nvironmental improvements (household sanitation and provision of clean water). How the intervention might work The face washing component of the SAFE strategy aims to maintain clean faces in the community in order to reduce eye\u2010seeking flies and person\u2010to\u2010person transmission of C. trachomatis . Face washing promotion as a community intervention can be combined with mass treatment of people with antibiotics in areas with high trachoma endemicity. Mass treatment with antibiotics aims to reduce the reservoir of C. trachomatis in the community, while face washing aims to interrupt the cycle of infection and re\u2010infection in the long term. The antibiotic and environmental arms of the SAFE strategy have been examined in other published Cochrane reviews ( Evans 2011 ; Rabiu 2012 ). Why it is important to do this review The face washing principle appears simple and theoretically sound, but whether this intervention can reduce transmission of trachoma in practice has been the focus of debate ( Bailey 2001 ). Some narrative reviews of the literature have suggested that facial cleanliness may be useful in preventing trachoma ( Emerson 2000 ; Pruss 2000 ). However, most of the data were obtained from observational studies and the methodological quality of the few controlled trials included was not reported. In this Cochrane review we aim to summarize systematically, research evidence from trials of face washing promotion for preventing active trachoma in endemic communities. In communities where water is scarce, the uptake and practice of face washing may not be as good as in communities where water is freely available. We will consider the potential influence of water availability on outcomes in this review.",
        "summary": "Trachoma can lead to blinding, and is most common in areas with poor personal and family hygiene. However, there is no clear randomized controlled trial evidence suggesting that eye/face washing (with or without tetracycline eye drops) reduces the prevalence of trachoma in school\u2010aged children. When eye washing conducted in school children in the Northern Territory of Australia every school day (with or without tetracycline eye drops daily for 1 week every month) for 3 months was compared with tetracycline eye drops alone or with no treatment, there appeared to be similar rates of follicular trachoma in both groups at 84 days follow\u2010up (1 trial, 1143 children). When daily face washing with tetracycline ointment (for 30 days) was followed by an intensive promotion program during neighborhood meetings in villages in Tanzania and reinforced with activities such as school plays, seminars and meetings with other village groups, the number of children with active trachoma was lower in two villages (55% versus 60% and 40% versus 50%), and higher in one village (70% versus 65%), when compared with a paired village using tetracycline ointment alone (1 RCT, 1417 children). The same pattern was observed for severe trachoma; lower in two villages (8% versus 14% and 6% versus 14%) and higher in one village (10% versus 8%) that received combined antibiotic and face washing promotion."
    },
    "CD009755": {
        "query": "How does micronutrient supplementation affect maternal and infant outcomes in pregnant women with HIV infection?",
        "document": "Background Description of the condition Micronutrients are defined as any essential dietary constituents, and include the vitamins and trace minerals required by the body in small quantities ( Churchill 1989 ). Micronutrient malnutrition, caused by deficiencies in vitamins and minerals, can manifest itself through several conditions including fatigue, anaemia (iron deficiency), reduced learning ability (mainly iron and iodine deficiency), goitre (iodine deficiency), reduced immunity, and night blindness (severe vitamin A deficiency) ( UNICEF 2009 ). In pregnant women, inadequate micronutrition is associated with poor outcomes for both the mother and the baby ( Owens 2008 ). Development of the foetus during the peri\u2010conceptional period may also be affected by suboptimal maternal micronutrition ( Owens 2008 ). Micronutrient deficiencies are particularly significant in impoverished conditions common to resource\u2010constrained settings ( Micronutrient Initiative 2009 ). In a recent study to estimate the risk of maternal and child undernutrition globally, Black and colleagues report that maternal undernutrition, including chronic energy and micronutrient deficiencies, is a serious problem and evident in most countries in sub\u2010Saharan Africa, south\u2010central and southeastern Asia, and in Yemen in the Middle East ( Black 2008 ). They estimated that stunting, severe wasting, and intrauterine growth restriction together were responsible for 2\u00b72 million deaths and 21% of disability\u2010adjusted life\u2010years (DALYs) for children younger than 5 years. Deficiencies of vitamin A and zinc were identified to be responsible for 0\u00b76 million and 0\u00b74 million deaths, respectively. Iron and iodine deficiencies resulted in few child deaths, but iron deficiency as a risk factor for maternal mortality added 115 000 deaths and 0\u00b74% of global total DALYs. People living with HIV are at great risk of nutritional disorders. As HIV infection is most prevalent in parts of the world where food security is compromised, key populations at high risk of HIV infection may lack appropriate nourishment prior to infection with HIV. Starvation and undernourishment severely compromises the immune system thereby increasing susceptibility to infection and progression of disease and development of AIDS, once infected. The resultant chronic disease state is associated with inflammatory responses which can lead to further weight loss due to loss of appetite, malabsorption, and development of specific nutrient deficiencies and imbalances ( de Pee 2010 ). With the added nutritional demands of pregnancy and breastfeeding, HIV\u2010infected pregnant and lactating women are especially vulnerable to developing micronutrient deficiencies with the resultant effects passed on in utero and during breastfeeding to their babies. Micronutrient supplements are either single or multiple formulations of vitamins, minerals and trace elements. Following a technical consultation on the nutrient requirements for people living with HIV/AIDS in 2003, the World Health Organization (WHO) recommended that iron\u2010folate supplementation be a standard component of antenatal care for preventing anaemia and improving foetal iron stores ( WHO 2003 ). However, the report recommended that research be conducted on the safety of iron supplementation specifically in pregnant women infected with HIV. Safety concerns were raised given the potential of iron supplementation to accelerate disease progression due to disturbances of iron metabolism observed in chronic diseases. Evidence for multivitamin supplementation and specific micronutrient supplementation (such as zinc) was also lacking at the time of the consultation, and the report recommended that adequate micronutrient intake be best achieved through an adequate diet. Additional data would be required prior to recommending additional micronutrient supplementation for HIV\u2010positive women during pregnancy and lactation. Further to this in 2009 a consortium of organizations including United Nations Children\u2019s Fund (UNICEF) and the World Bank, and funded by the Micronutrient Initiative, produced the report: Investing in the future: a united call to action on vitamin and mineral deficiencies ( Micronutrient Initiative 2009 ) . The report recognises the provision of micronutrients as a cheap intervention with significant potential for investment for development. Although HIV\u2010infected women are not considered as a specific group in the report, the recommendations include the need for further research to explore the feasibility of providing women of a child\u2010bearing age with multiple vitamin and mineral supplements. Why it is important to do this review Current WHO guidelines for HIV\u2010specific populations are based on an appraisal of the evidence conducted in 2003 ( WHO 2003 ). In 2011 the WHO Nutrition for Health and Development Department requested an up\u2010to\u2010date systematic review of the evidence to inform their guideline development process. This review aims to provide a comprehensive summary of the current evidence for micronutrient supplementation in HIV\u2010infected pregnant and lactating women. A review by Irlam et al. , first published in 2005 and updated in 2010 ( Irlam 2010 ), included pregnant and lactating women, children and adolescents, and non\u2010pregnant adults with HIV infection. The review has subseqently been split into three reviews, one per each of the aforementioned populations. This review should therefore be read with the other reviews for a complete picture of the evidence base for micronutrient supplementation in those with HIV infection",
        "summary": "Moderate\u2010quality evidence shows that giving multivitamins at RDA is more effective than placebo at improving maternal and infant outcomes in pregnant women with HIV. However, the administration of zinc or selenium or multivitamins at a higher dose than the RDA confers no benefit over giving the RDA. Moderate\u2010quality evidence shows that multivitamins, compared with single vitamins or placebo, are effective at improving maternal and infant outcomes in pregnant women with HIV, but at the cost of a discrete increment in the frequency of subclinical mastitis. However, results should be taken with caution as they are based on a trial conducted in a population with some degree of anemia and under\u2010nutrition. Moderate\u2010quality evidence suggests that in pregnant women with HIV receiving multivitamins at RDA dose, the administration of zinc or selenium does not improve maternal and infant outcomes. The administration of multivitamins at a higher RDA dose did not improve maternal and infant outcomes, compared with supplementation at RDA dose in pregnant women with HIV. Results should be taken with caution as they are based on only a few, albeit large, trials."
    },
    "CD003949": {
        "query": "How do different preoperative skin antiseptics compare in terms of preventing surgical site infections after clean surgery?",
        "document": "Background Description of the condition Surgical site infections (SSIs) can occur following an invasive surgical procedure ( NICE 2008 ). An SSI can be diagnosed by the presence of clinical signs and symptoms alone, e.g. pus, redness, pain, heat, or based on the presence of one or more clinical symptoms along with a quantitative measurement of more than 10 6 colony forming units per mm\u00b3 tissue ( Mangram 1999 ). Surgical procedures and their resulting surgical wounds are classified as either clean, clean\u2010contaminated, contaminated or dirty\u2010infected, depending upon the area of the body operated upon and the level of infection and inflammation present ( Table 1 ). A surgical wound is less likely to become infected postoperatively if it is classified as clean. Leaper 1995 suggested expected infection rates of less than 2% in clean surgery and less than 10% in contaminated surgery. Classification Description Clean Non\u2010infective surgical wounds in which no inflammation is encountered, and neither the respiratory, alimentary, genitourinary tract nor the oro\u2010pharyngeal cavity is entered. In addition these cases are elective, primarily closed, and drained with closed drainage system when required. Clean/ Contaminated Surgical wounds in which respiratory, alimentary, genital or urinary tract is entered under controlled conditions and without unusual contamination. Specifically, surgeries involving the biliary tract, appendix, vagina and oropharynx are included in this category, provided no evidence of infection or a major break in sterile technique is encountered. Contaminated Fresh, accidental wounds, operations with major breaks in sterile technique or gross spillage from the gastrointestinal tract, and incisions in which acute, non\u2010purulent inflammation is encountered. Dirty Old traumatic wounds with retained devitalised tissue and those that involve existing clinical infection or perforated viscera. This definition suggests that organisms causing postoperative infection were present in the operative field before the operation. In the UK the Health Protection Agency (HPA) collects ongoing SSI data nationally although only data collection following orthopaedic surgery is mandatory. From April 2006 to March 2011, the HPA collected data in 237 NHS hospitals on 438,679 surgical procedures ( Health Protection Agency 2011 ). They report clean SSI rates of: 0.6% for knee prosthesis; 1% for cardiac surgery (non\u2010coronary artery bypass graft); 0.8% for hip prosthesis and 5% for limb amputation. This is in contrast to the HPA\u2010reported incidence (2006 to 2011) of SSI following surgery on the large bowel (contaminated) of 10%. Whilst the incidence of SSI in clean surgery can be low relative to other surgical procedures, there are tens of thousands of clean procedures performed annually world\u2010wide and the frequency of these procedures raises the overall numbers at risk for SSI in this group. Addtionally, since for clean surgery there is, arguably, a lower risk of infection from 'internal' contamination, it may be that skin cleansing plays, relatively, a more important role in terms of SSI prevention compared to non\u2010clean surgeries. The costs incurred when a patient contracts an SSI can be considerable in financial, as well as social, terms. It has been estimated that patients with SSIs require, on average, an additional hospital stay of 6.5 days, and that hospital costs are doubled. When extrapolated to all acute hospitals in England, it is estimated that the annual cost is approximately GBP 1 billion ( Plowman 2000 ). NICE 2006 identified that an SSI increased the costs of surgery by two to five times ( NICE 2008 ). Description of the intervention The removal of transient bacteria and reduction of the number of commensal organisms by an antiseptic is recommended prior to surgery by several organisations including the Royal College of Surgeons of England ( Leaper 2001 ), the Centers for Disease Control and Prevention (CDC) ( Mangram 1999 ), the Association of Perioperative Registered Nurses (AORN) ( AORN 2006 ), and the Association for Perioperative Practice ( AfPP 2007 ). Therefore, it has become routine preoperative practice to cleanse the skin at the operation site with an antiseptic ( McCluskey 1996 ). The effectiveness of preoperative skin preparation is thought to depend on both the antiseptic used and the method of application. CDC guidance states: the size of the area prepared should be sufficient to include any potential incision sites divorced from the main incision site e.g. abdominal preparation for laparoscopic surgery ( Mangram 1999 ); the solution should be applied in concentric circles; a dedicated instrument may be used, e.g. a sponge, or X\u2010ray detectable swab, adapted for the purpose; this applicator should be discarded once the periphery has been reached; time should be allowed for the solution to dry, especially when alcoholic solutions are used, as these are flammable ( MHRA 2000 ). the size of the area prepared should be sufficient to include any potential incision sites divorced from the main incision site e.g. abdominal preparation for laparoscopic surgery ( Mangram 1999 ); the solution should be applied in concentric circles; a dedicated instrument may be used, e.g. a sponge, or X\u2010ray detectable swab, adapted for the purpose; this applicator should be discarded once the periphery has been reached; time should be allowed for the solution to dry, especially when alcoholic solutions are used, as these are flammable ( MHRA 2000 ). AORN guidelines stipulate the following ( AORN 2006 ): that the applicator used should be sterile; the solution should be applied using friction, and extend from the incision site to the periphery. that the applicator used should be sterile; the solution should be applied using friction, and extend from the incision site to the periphery. For the purposes of this review skin preparation antiseptic agents are referred to as \"antiseptics\" and can be applied in the form of liquids, solutions or powders. Leclair 1990 described an antiseptic as \"a chemical agent that reduces the microbial population on the skin\". It is suggested that the ideal agent would: kill all bacteria, fungi, viruses, protozoa, tubercle bacilli and spores; be non toxic; be hypoallergenic; be safe to use in all body regions; not be absorbed; have residual activity; be safe for repetitive use ( Hardin 1997 ). kill all bacteria, fungi, viruses, protozoa, tubercle bacilli and spores; be non toxic; be hypoallergenic; be safe to use in all body regions; not be absorbed; have residual activity; be safe for repetitive use ( Hardin 1997 ). Several antiseptic agents are available for preoperative preparation of skin at the incision site. Iodine/iodophors are iodine solutions which are effective against a wide range of Gram\u2010positive and Gram\u2010negative bacteria, the tubercle bacillus, fungi and viruses. These penetrate cell walls, then oxidise and substitute the microbial contents with free iodine ( Hardin 1997 ; Mangram 1999 ; Warner 1988 ). Iodophors contain a surfactant/stabilising agent that liberates the free iodine ( Wade 1980 ). Iodophor has largely replaced iodine as the active ingredient in antiseptics. Iodophor comprises free iodine molecules bound to a polymer such as polyvinyl pyrrolidine (i.e. povidone), so is often termed povidone iodine (PI) ( Larson 1995 ). Typically, 10% PI formulations contain 1% available iodine ( Larson 1995 ; Reichman 2009 ). PI is soluble in both water and alcohol, and available preparations include: aqueous iodophor scrub and paint, aqueous iodophor one\u2010step preparation with polymer (3M), and alcoholic iodophor with water\u2010insoluble polymer (DuraPrep). Alcohol denatures the cell wall proteins of bacteria ( Hardin 1997 ; Mangram 1999 ; Warner 1988 ). Alcohol is active against Gram\u2010positive and Gram\u2010negative bacteria, the tubercle bacillus and many fungi and viruses. Concentration, rather than type, of alcohol is important in determining its effectiveness ( Larson 1995 ; Leclair 1990 ). Chlorhexidine gluconate (aqueous or alcoholic) is an antiseptic thought to be effective against a wide range of Gram\u2010positive and Gram\u2010negative bacteria, yeasts and some viruses ( Reichman 2009 ) How the intervention might work The aim of preoperative skin antisepsis is to reduce the risk of SSIs by removing soil and transient organisms from the skin ( AORN 2006 ). The skin is not a sterile surface, but is colonised by a large number of bacteria, with up to three million microorganisms on each square centimetre of skin ( Hinchliffe 1988 ). Antiseptics have the ability to bind to the stratum corneum, resulting in persistent chemical activity on the skin ( Larson 1988 ). Primary action of antiseptics includes the mechanical removal, and chemical killing and inhibition, of contaminating and colonising flora ( Larson 1988 ). As micro\u2010organisms tend to colonise the deeper layers of the stratum corneum (the layer of dead cells on the outside of the body), they are not shed with desquamation (loss of dead cells). There are two types of micro\u2010organisms on the skin; commensals, which are normally resident, and transients, which are not consistently present and are easily exchanged between individuals. The transient organisms are easily removed, whereas, it has been suggested, the commensals are difficult to remove completely ( Larson 1988 ). The commensals include Staphylococci, diptheroid organisms, Pseudomonas and Propionibacterium species which can lead to harmful infections if they are allowed to multiply. An SSI occurs when the number of bacteria in the incision overcome the host's defences. Most commonly these bacteria are commensals from the patient's skin ( Malangoni 1997 ). Why it is important to do this review It has become routine preoperative practice to cleanse the surgical site with an antiseptic ( McCluskey 1996 ), however, it is important to assess the comparative effectiveness of alternative antiseptics to inform clinical practice. The current National Institue of Health and Clinical Excellence (NICE) guidelines recommendation regarding skin preparation across surgeries is to: \"Prepare the skin at the surgical site immediately before incision using an antiseptic (aqueous or alcohol\u2010based) preparation: povidone\u2010iodine or chlorhexidine are most suitable\" ( NICE 2008 ). However, a recent trial undertaken in 849 participants undergoing clean\u2010contaminated surgery compared chlorhexidine in alcohol with PI\u2010aqueous and reported that the chlorhexidine solution was more effective in terms of SSI prevention for superficial incisional infection (4.2% developed an SSI in the chlorhexidine group compared to 8.6% in the PI group: p\u2010value 0.08) and deep incisional infection (1.0% developed an SSI in the chlorhexidine group compared with 3.0% in the PI group p\u2010value 0.05) ( Darouiche 2010 ). A further recent systematic review meta\u2010analysed five RCTs that compared chlorhexidine\u2010alcohol with PI\u2010aqueous in skin antiseptics for the prevention of SSI and included Darouiche 2010 , (this was the largest included study in the analysis). The authors report that there was evidence that chlorhexidine\u2010alcohol reduces risk of SSI following surgery compared with PI: risk ratio (RR) of 0.65, 95% CI 0.50 to 0.85 ( Maiwald 2012 ). However, this review goes on to raise the important issue of whether there is potential for the alcohol in the chlorhexidine\u2010alcohol solution to have a role in SSI prevention that is not being acknowledged when PI\u2010aqueous solutions are compared with chlorhexidine\u2010alcohol solutions ( Maiwald 2012 ; Maiwald 2014 ). Given the inclusion of RCTs evaluating clean/contaminated wounds in this review it is not clear how its results relate to clean surgical wounds.",
        "summary": "There is insufficient randomized controlled trial evidence to draw firm conclusions as to which preoperative skin antiseptic is the most effective for prevention of surgical site infection (SSI). When different antiseptic regimens were compared with each other in a network meta\u2010analysis using common comparators, there was no apparent difference between any two interventions included in the network in terms of rates of SSI. There was some suggestion that 4% chlorhexidine scrub in 70% alcohol had the highest probability of being best in terms of preventing SSIs (78%), followed by iodophor in alcohol (16%), but the evidence included in the network was of very low to low quality. Most of the included trials were very underpowered, and most of the network links had only one or two trials. As a result, the probabilities of benefit associated with each intervention should be treated with some caution. In addition, the indication that preparations containing alcohol had the highest ratings for the probability of preventing SSIs was not supported by analysis of direct comparison of alcohol and aqueous\u2010based preparations."
    },
    "CD010432": {
        "query": "In people with newly diagnosed leukemia, what are the effects of idarubicin compared with other anthracyclines?",
        "document": "Background Description of the condition Acute myeloid leukaemia (AML) is a heterogeneous group of clonal malignant myeloid disorders which have clinical similarities but distinct morphologic, immunophenotypic, cytogenetic and molecular features. It is the most common type of myeloid leukaemia with an overall incidence of 3.7 cases per 100,000 persons between 2000 and 2003. The incidence of AML increases with age and the median year at presentation is approximately 65 years ( Deschler 2006 ). AML represents 80% to 90% of acute leukaemia cases in adults but accounts for fewer than 15% of leukaemia cases in children younger than 10 years ( Baer 2009 ). AML is slightly more common among populations of European ethnicity and acute promyelocytic leukaemia (APL), a distinct subtype of AML, has a higher incidence in populations of Latino or Hispanic descent ( Douer 1996 ; Estey 1997 ). AML is characterised by an increased number of immature myeloid cells (blasts) in bone marrow, peripheral blood and other tissues, resulting in impaired haematopoiesis (formation of blood cells) manifested by cytopenias (deficiency of specific blood cells) ( Lowenberg 1999 ). It results from genetic alterations in normal haematopoietic stem cells that induce differentiation arrest or excessive proliferation of the affected cells, or both ( Jabbour 2006 ). Several factors have been implied in causing AML which include exposure to ionising radiation, benzene and cytotoxic chemotherapy ( Estey 2006 ). The World Health Organization (WHO) classifies AML into five major categories: (a) AML with recurrent genetic abnormalities; (b) AML with multi\u2010lineage dysplasia; (c) AML and myelodysplastic syndromes (MDS), therapy\u2010related; (d) AML not otherwise categorised; and (e) acute leukaemia of ambiguous lineage ( Baer 2009 ). Typical clinical presentations of AML are fatigue and weakness, haemorrhage, or infections and fever due to decreases in red blood cells, platelets or white blood cells, respectively. Additionally, leukaemic infiltration of various tissues can produce a variety of corresponding symptoms such as enlarged liver (hepatomegaly), enlarged spleen (splenomegaly), enlarged lymph nodes (lymphadenopathy), leukaemia cutis (the outermost, nonvascular layer of the skin) and so on ( Lowenberg 1999 ). Besides these common clinical presentations of AML, acute promyelocytic leukaemia (APL) possesses additional characteristics. It used to be considered the most fatal subtype of AML because of potential fatal haemorrhage due to consumptive coagulopathy. However, it is now regarded as the most curable subtype due to its high sensitivity to all\u2010trans retinoic acid (ATRA) and arsenic trioxide (ATO) ( Wang 2008 ). Studies on the pathogenesis and prognosis of AML have made revolutionary progress; however, the treatment for AML remains unsatisfactory. Only 40% to 45% of AML patients enjoy long\u2010term disease\u2010free survival (DFS) and most patients still die of their disease, primarily due to persistent or relapsed AML ( Burnett 2011 ). New progress in the treatment of AML is required. Description of the intervention Treatment of AML consists of two phases: remission\u2010induction therapy phase and post\u2010remission therapy phase. The former aims to attain a complete remission (CR), while the latter aims to maintain the CR. Achieving CR by remission\u2010induction therapy is essential for prolonging survival and obtaining a cure for AML patients. For several decades, a combination of an anthracycline (a class of chemotherapy drugs derived from the Streptomyces bacterium Streptomyces peucetius var. caesius ) and cytarabine (Ara\u2010C) has been the standard for remission\u2010induction therapy of AML ( Dohner 2010 ). Therefore, selecting the most effective and tolerable anthracycline is key to maximising treatment outcomes. Daunorubicin (DNR) is the most widely used anthracycline. The standard dose of DNR used in remission\u2010induction therapy is 45 mg/m\u00b2/d for three days ( Fernandez 2009 ). A combination of three days of DNR at a dose of 40 mg/m\u00b2/d to 60 mg/m\u00b2/d and seven days of Ara\u2010C at a dose of 100 mg/m\u00b2/d to 200 mg/m\u00b2/d generally has been used for more than 40 years ( Burnett 2011 ; Lowenberg 1999 ). With this regimen, approximately 60% to 80% of adults with AML achieve CR, whereas only 40% to 45% of patients enjoy long\u2010term DFS ( Burnett 2011 ; Lowenberg 1999 ; Tallman 2005 ; Zittoun 1995 ). Additionally, DNR tends to cause serious cumulative injury to the heart resulting in congestive cardiomyopathy and, ultimately, congestive heart failure, which is usually refractory to medical therapy. Other common side effects include myelosuppression, nausea, vomiting, diarrhoea, alopecia and mucositis ( Hande 2009 ). To improve the efficacy and reduce the side effects of remission\u2010induction therapy, various alternative anthracyclines were developed and introduced into clinics in the 1980s, among which idarubicin (IDA) is a most promising one ( Johnson 1998 ). IDA, also called 4'\u2010demethoxydaunorubicin (4\u2010DMDR), is a DNR derivative synthesised by replacing the C\u20104 methoxyl group with a hydrogen atom ( Arcamone 1976 ). With this minor structural alteration, IDA has several theoretical advantages over the parent compound: (1) IDA has a more effective antileukaemia activity ( Casazza 1980 ); (2) IDA is active by both intravenous and oral routes of administration ( Ganzina 1986 ); (3) IDA has an ability to overcome the multidrug resistant (MDR) phenotype and reduces the development of drug resistance ( Berman 1992 ); (4) IDA is less cardiotoxic and is well tolerated ( Cersosimo 1992 ). IDA was registered and approved by the Food and Drug Administration (FDA) of USA in 1990. The standard dose of IDA used in remission\u2010induction therapy is 12 mg/m\u00b2/d for three days ( Ohtake 2011 ). At present, IDA has been used as the first\u2010line therapy at a dose of 10 mg/m\u00b2/d to 12 mg/m\u00b2/d for three days in younger adult patients (18 to 60 years) with newly diagnosed AML, or relapsed/refractory AML ( Dohner 2010 ). How the intervention might work IDA is an anthracycline antineoplastic agent. It mediates control of AML by two molecular mechanisms. First, IDA inhibits DNA topoisomerase II, which is a nuclear enzyme that modulates DNA topology by passing a double\u2010stranded DNA through a transient break in the DNA backbone. By poisoning the enzyme to prevent it from re\u2010ligating (i.e. binding back together) cleaved DNA, IDA converts topoisomerase II into a toxin, resulting in high levels of transient protein\u2010associated breaks in the genome of treated cells. Second, IDA intercalates into base pairs of DNA and generates free radicals to break the DNA strand. Both eventually lead to the death of leukaemia cells ( Hande 2009 ). Experimental laboratory studies have indicated that IDA and DNR have equal affinity for DNA and comparable inhibitory effects on DNA topoisomerase II ( Ganzina 1986 ). The higher antileukaemia activity of IDA may result from its metabolite idarubicinol, which is more active and has a longer half\u2010life than the metabolite of DNR ( Robert 1992 ). For the ability of overcoming the MDR phenotype, some studies suggest that IDA has a high lipophilic (having an affinity for, tending to combine with, or capable of dissolving in lipids) coefficient and is less of a substrate for P\u2010glycoprotein (P\u2010gp) than DNR, which acts as an active efflux pump, thereby allowing for greater intracellular drug accumulation ( Berman 1992 ; Supino 1977 ). A great number of phase I/II trials support the activity of IDA in AML. In phase I trials, IDA was demonstrated to be less cardiotoxic and the dose\u2010limiting toxicity of the drug was myelosuppression ( Berman 1983; Kaplan 1982 ). In later phase II trials, as a single agent, IDA induced CR in about 20% of adult patients with relapsed or refractory AML ( Carella 1984 ; Hayat 1984 ). Combining IDA with Ara\u2010C increased CR to a range of 24% to 70% in similar groups of heavily pre\u2010treated patients ( Berman 1989 ; Harousseau 1987 ; Lambertenghi\u2010Deliliers 1987 ). In previously untreated AML, more than 80% of patients achieved CR after being treated with a combination of IDA, Ara\u2010C and etoposide (an anti\u2010cancer agent which kills cancer cells by inhibiting their DNA synthesis) ( Carella 1987 ). For newly diagnosed APL, IDA, when combined with ATRA, induced a CR rate higher than 80% either in adults or in elderly patients ( Avvisati 1996 ; Latagliata 1997 ). On the basis of these trials, numerous prospective, randomised controlled trials (RCTs) testing the superiority of IDA versus other anthracyclines including DNR have been conducted in previously untreated AML patients ( Beksac 1998 ; Berman 1991 ; Creutzig 2001 ; Harousseau 1996 ; Indrak 2001 ; Mandelli 1991 ; Mandelli 2009 ; Ohtake 2011 ; Pignon 1996 ; Reiffers 1996 ; Rowe 2004 ; Vogler 1992 ; Wiernik 1992 ). However, the outcomes of these RCTs are inconsistent. Three initial RCTs comparing standard dose IDA (12/13 mg/m\u00b2/d for three days) with standard dose DNR (45/50 mg/m\u00b2/d for three days) reported a superior CR rate for the IDA group ( Berman 1991 ; Vogler 1992 ; Wiernik 1992 ). However, the long\u2010term follow\u2010up of the three RCTs revealed that the IDA group had a better overall survival (OS) than the DNR group in only one of the three RCTs ( Berman 1997 ). A study published by Mandelli in 1991, which compared IDA (12 mg/m\u00b2/d for three days) with DNR (45 mg/m\u00b2/d for three days) in elderly AML patients (age greater than 55 years), failed to demonstrate any significant difference in CR rate, OS and relapse\u2010free survival (RFS) between the two arms ( Mandelli 1991 ). In another study published by Mandelli in 2009, the use of IDA (10 mg/m\u00b2/d for three days) was superior to DNR (50 mg/m\u00b2/d for three days) in terms of DFS, survival from CR and OS, but was similar to mitoxantrone (12 mg/m\u00b2/d for three days) ( Mandelli 2009 ). Moreover, in two recent studies, doubling the dose of DNR from the standard dose (45 mg/m\u00b2/d for three days) to 90 mg/m\u00b2/d for three days significantly improved the CR rate and duration of OS ( Fernandez 2009 ; Lowenberg 2009 ). In a more recent study conducted by Ohtake, DNR at a dose of 50 mg/m\u00b2/d for five days was found to be equivalent to IDA (12 mg/m\u00b2/d for three days) in CR rate, RFS and OS without increasing the risk of infection or cardiomyopathy ( Ohtake 2011 ). Therefore, the superiority of IDA versus other anthracyclines remains a matter of debate. Why it is important to do this review Anthracyclines have been the core treatment for AML for several decades; thus, selecting the most effective and tolerable anthracycline is key to maximising treatment outcomes. In spite of the theoretical advantages of IDA, RCTs comparing induction therapy based on IDA with those based on other anthracyclines have conflicting results. There is no evidence that it would definitively prove the superiority of IDA over other anthracyclines with respect to CR rate, RFS, DFS and OS. We would like to assess which anthracycline is the most effective to be used for induction therapy. Although a meta\u2010analysis for IDA is available ( AML Collaborative Group 1998 ), it only included RCTs published before 1996 and many new RCTs have been published since then ( Beksac 1998 ; Creutzig 2001 ; Indrak 2001 ; Mandelli 2009 ; Morita 2010 ; Ohtake 2011 ; Rowe 2004 ). It is important to update the information by including all new trials. Therefore, we undertook this systematic review to obtain definitive evidence on the role of IDA versus other anthracyclines in the treatment of AML. Our review informed about the current status of clinical practice and provided some guidance for future clinical studies in this area.",
        "summary": "In people with newly diagnosed leukemia (mostly adults), idarubicin is more effective than daunorubicin and as effective as mitoxantrone at improving overall and disease\u2010specific survival and remission rates, although idarubicin increases mortality on induction therapy compared with daunorubicin. High\u2010 to moderate\u2010quality evidence showed that overall survival and disease\u2010free survival at two years were higher with idarubicin compared with daunorubicin. Complete remission rates were also higher and fewer people relapsed with idarubicin but they had higher risk of death on induction therapy, compared with daunoribicin. Rates of adverse effects were similar in both groups although more people receiving idarubicin had mucositis grade 3/4. High\u2010 to low\u2010quality evidence stated that idarubicin was as effective as mitoxantrone at improving outcomes and may be associated with lower grade 3\u20104 toxicity rates, although the latter finding did not reach statistical significance. No firm conclusions can be drawn regarding the comparisons of idarubicin with doxorubicin or zorubicin, as the evidence available is very limited and low quality. Of note, not all combination regimens with anthracyclines used in clinical practice were assessed."
    },
    "CD010292": {
        "query": "What are the benefits and harms of serotonin and noradrenaline reuptake inhibitors (SNRIs) in people with fibromyalgia syndrome?",
        "document": "Background Description of the condition Fibromyalgia is defined by the American College of Rheumatology (ACR) 1990 classification criteria as widespread pain lasting for longer than three months with tenderness on palpation at 11 or more of 18 specified tender points ( Wolfe 1990 ). Chronic widespread pain is frequently associated with other symptoms, such as poor sleep, fatigue, and depression ( Wolfe 2013a ). People with moderate and severe forms of fibromyalgia often report high disability levels and poor quality of life along with extensive use of medical care ( H\u00e4user 2015a ; H\u00e4user 2017 ). Fibromyalgia symptoms can be assessed by patient self\u2010report using the fibromyalgia criteria and severity scales for clinical and epidemiological studies: a modification of the ACR Preliminary Diagnostic Criteria for Fibromyalgia (so\u2010called Fibromyalgia Symptom Questionnaire) ( Wolfe 2011a ). For a clinical diagnosis, the ACR 1990 classification criteria ( Wolfe 1990 ), the ACR 2010 preliminary diagnostic criteria ( Wolfe 2010 ) and the 2016 criteria ( Wolfe 2016 ) can be used. Lacking a specific laboratory test, diagnosis is established by a history of the key symptoms and the exclusion of somatic diseases sufficiently explaining the key symptoms ( Wolfe 2010 ). For epidemiology studies, the modified ACR 2010 preliminary diagnostic criteria (survey criteria) can be used ( Wolfe 2011a ). The indexing of fibromyalgia within the international classification of diseases is under debate. While some rheumatologists have thought of it as a specific pain disorder and central sensitivity syndrome ( Clauw 2014 ; Yunus 2008 ), recent research points at small fibre pathology in a subgroup of people with fibromyalgia that may be of pathophysiological importance ( \u00dcceyler 2017 a ). In psychiatry and psychosomatic medicine, fibromyalgia symptoms are categorized as a functional somatic syndrome, a bodily distress syndrome, a physical symptom disorder, or a somatoform disorder ( H\u00e4user 2009 ; H\u00e4user 2014 ). Fibromyalgia is a heterogeneous condition. The definite etiology (causes) of this syndrome remains unknown. A model of interacting biological and psychosocial variables in the predisposition, triggering and development of the chronicity of fibromyalgia symptoms has been suggested ( \u00dcceyler 2017 a )). Inflammatory rheumatoid arthritis ( Wolfe 2011a ), depression ( Chang 2015 ), genetics ( Arnold 2013 ; Lee 2012 ), obesity combined with physical inactivity ( Mork 2010 ), physical and sexual abuse in childhood ( H\u00e4user 2010a ), sleep problems ( Mork 2012 ), and smoking ( Choi 2010 ) predict future development of fibromyalgia. Psychosocial stress (e.g. working place and family conflicts) and physical stress (e.g. infections, surgery, accidents) might trigger the onset of chronic widespread pain and fatigue ( Clauw 2014 ; \u00dcceyler 2017 a ). Depression and post\u2010traumatic stress disorder worsen fibromyalgia symptoms ( H\u00e4user 2013 a ; Lange 2010 ). Several factors are associated with the pathophysiology (functional changes associated with or resulting from disease) of fibromyalgia, but the relationship is unclear. The functional changes include alteration of sensory processing in the brain (so\u2010called central sensitization), reduced reactivity of the hypothalamus\u2010pituitary\u2010adrenal axis to stress, increased pro\u2010inflammatory and reduced anti\u2010inflammatory cytokine profiles (produced by cells involved in inflammation), disturbances in neurotransmitters such as dopamine and serotonin and small nerve fibre pathology ( \u00dcceyler 2017 a ). Prolonged exposure to stress, as outlined above, may contribute to these functional changes in predisposed individuals ( Bradley 2009 ). Fibromyalgia is common. Numerous studies have investigated its prevalence in different settings and countries. A review gives a global mean prevalence of 2.7% (range 0.4% to 9.3%), and a mean in the Americas of 3.1%, in Europe of 2.5% and in Asia of 1.7%. It is more common in women, with a female to male ratio of 3:1 (4.2%:1.4%) ( Queiroz 2013 ). Estimates of prevalence in specific populations vary greatly, but have been reported as being as high as 9% in female textile workers in Turkey and 10% in metalworkers in Brazil ( Queiroz 2013 ).The change in diagnostic criteria does not appear to have significantly affected estimates of prevalence ( Wolfe 2013a ). Since specific treatment aimed at altering the pathogenesis is not possible, drug therapy that focuses on symptom reduction is ubiquitously employed. Description of the intervention Serotonin and noradrenaline (norepinephrine) reuptake inhibitors (SNRIs) act on noradrenergic and serotonergic neurons in the nervous system. Serotonin and noradrenaline are implicated in the mediation of endogenous pain inhibitory mechanisms. How the intervention might work Dysfunction of serotonin and noradrenaline transmission, which mediates endogenous analgesic mechanisms via the descending inhibitory pain pathways in the central nervous system, may play a key role in the pathophysiology of fibromyalgia. Researchers found that levels of metabolites of biogenic amines key to descending inhibition were lower than normal in at least three fibromyalgia body fluid compartments ( Legangneux 2001 ; Russell 1992 ). Imbalance or deficiency in serotonin and noradrenaline is also associated with other key symptoms of fibromyalgia such as fatigue and cognitive deficits ( Bradley 2009 ). Treatment with SNRI increases transmission of these neurotransmitters and may improve disease states associated with serotonin and noradrenaline deficiencies such as pain, fatigue and cognitive deficits. Why it is important to do this review There is a transatlantic difference in the approval of SNRIs as a treatment for fibromyalgia by drug agencies ( Briley 2010 ). The SNRIs duloxetine and milnacipran have been approved by the US Food and Drug Administration (FDA), but not by the European Medical Agencies (EMA), for the management of fibromyalgia. The FDA stated that the sponsors of the two drugs had provided adequate evidence of their benefits and harms to support their indication for the management of fibromyalgia ( Department of health & Human Services 2008 ; Department of health & Human Services 2009 ). The EMA, however, denied clinically relevant effects for both drugs, on the basis of a lack of robust evidence of efficacy, and because the adverse effects profile was considered to outweigh the benefits ( EMA 2008 ; EMA 2010 ). We conducted a systematic review on SNRIs in fibromyalgia which included randomized controlled trials that had not been evaluated by the FDA and EMA in 2013 ( H\u00e4user 2013 b ). Meanwhile, new randomized controlled trials with duloxetine ( Leombruni 2015 ; Murakami 2015 ), and milnacipran ( Bateman 2013 ; Matthey 2013 ; Staud 2015 ), were published that had not been evaluated by the FDA and EMA and by the previous version of this review ( H\u00e4user 2013 b ). With new data available, and in the light of the divergent appraisals of duloxetine and milnacipran by the FDA and EMA, we saw the need to evaluate the efficacy and safety of SNRIs according to recently established methodological standards of pain medicine ( Moore 2010a ), in order to assist people with fibromyalgia and doctors in shared decision making on pharmacological treatment options.",
        "summary": "In people with fibromyalgia, low\u2010quality evidence suggests that the serotonin and noradrenaline reuptake inhibitors (SNRIs) duloxetine and milnacipran may modestly reduce self\u2010reported pain and slightly improve patient global impression and quality of life compared with placebo. Low\u2010quality evidence also indicates that only duloxetine seems to modestly improve sleep problems. However, low\u2010quality evidence also suggests that SNRIs seem to be associated with larger numbers of withdrawals due to adverse events (with nausea, somnolence, and insomnia the most common) than placebo (on average, 191 vs 102 per 1000 people withdrew), although not with an increased incidence of serious adverse events (very low\u2010quality evidence). The US Food and Drug Administration has warned that SNRIs may be associated with increased risks of suicidal ideation, hepatotoxicity, and serotonin syndrome."
    },
    "CD009912": {
        "query": "What are the effects of psychosocial interventions for informal caregivers of people living with cancer?",
        "document": "Background Improved survival has resulted in a growing population living with and affected by cancer and, increasingly, cancer is being viewed as a chronic disease. Cancer prevalence is estimated to be 11 million in the US ( Horner 2006 ) and two million in the UK ( Maddams 2009 ). These prevalence figures are expected to increase by three percent per year and approximately one million per decade ( Maddams 2009 , Maddams 2012 ).This growing prevalence is apparent also in other developed countries such as Australia and the Republic of Ireland, with one in three people being diagnosed with cancer during their lifetime ( Ferlay 2008 ). The most common cancers worldwide are breast cancer, colorectal cancer and prostate cancer ( Maddams 2009 ). There is an increasing emphasis on the role of informal caregivers in terms of providing support for cancer patients following completion of active treatment, particularly as the prevalence of cancer rises, survival improves and new models of care evolve. Cancer may impact on caregivers (as well as patients) from the onset of symptoms through the illness trajectory to recovery and beyond. In the UK, approximately 40% of people caring for someone living with cancer spend more than 30 hours per week providing care ( Macmillan 2008 ).The impact of cancer on a patient and their family may be influenced by many factors including the stage and nature of the diagnosis ( Weitzner 1999 ), the associated treatment ( Langer 2003 ; Nijober 1999 ) and patient and carer personality ( Campbell 2004 ; Carver 1993 ). Caring may have detrimental effects on a caregiver's physical, psychological and social health and may reduce significantly their quality of life (QoL) ( Ferrell 1995 ; Northouse 2007 Northouse 2012 ; Stenberg 2009 ). Recent research on the effects of caring for a cancer patient identified a range of problems including enduring periods of back and muscular pain, disturbed sleep patterns and general fatigue ( Stenberg 2009 ), psychological difficulties such as intense worry about a patient's health, and stress associated with providing care and support while maintaining daily work and other responsibilities ( Hagedoorn 2000 ; Kim 2012 ; Northouse 2000 ,). Furthermore, the time and costs of providing care may lead to gaps in, or loss of, employment and education, reduced income, increased bills and overall financial strain. The problems and needs associated with informal caring may last for prolonged periods even when a patient is 'free' of disease ( Hodgkinson 2007 , Kim 2010 ). Informal caregivers may experience equal or greater levels of depression than patients with cancer ( Campbell 2009 ; Hodges 2005 ; Rhee 2008 ), live with fear that cancer may return ( Hodgkinson 2007 Northouse 2012 ) and suffer anxiety and reduced QoL ( Cella 1990 ; Mellon 2006 ; Sherif 2001 ). Caregivers may be overwhelmed by their changed circumstances and may need help and support to cope with the practical challenges of providing care. Increased needs among caregivers appear to be associated with being female, younger ( Daly 2009 ; Harding 2003 ), lower socioeconomic status ( Donnelly 2008 ), caring for someone with stage one or two cancer ( Daly 2009 ), unhealthy partner attachments, living alone with a patient and a higher level of patient dependency ( Nijober 1999 ). Carers whose relationship with a patient had been troubled previously ( Gritz 2004 ) or who lacked positive communication ( Kim 2008 ) may experience significant difficulties in their caring role. Female caregivers, compared to men, may experience a greater burden of care because they tend to have multiple responsibilities such as caring for children and household management alongside being a primary caregiver ( Campbell 2009 ; Matthews 2003 ). We lack a critically appraised synthesis of the best ways in which to meet the needs of informal cancer caregivers who care for close family relatives (or friends) who 'live with cancer'. For the purposes of this review, we used the term 'living with cancer' and its derivatives to define our review population as individuals who are living with potentially curable cancer or with cancer chronically. Description of the condition This review defined an informal caregiver as an unpaid individual who provides a family member or friend with emotional care and support in physical and/or practical life domains ( Candy 2011 ; Harding 2003 ). In developed healthcare systems, partners or spouses tend to provide most of the physical care and emotional support for cancer patients following hospital discharge ( Hodgkinson 2007 ), though the caring role may be undertaken by other relatives and, less commonly, by friends and neighbours ( Mills 2008 ). This review included partners, other relatives and friends in the definition of an informal caregiver, but excluded caregivers of patients who were in the terminal phase of disease as this was beyond the scope of this review. The effectiveness of interventions provided to informal caregivers of people with terminal cancer has been assessed in a previous review ( Candy 2011 ). Moreover, there is a need to recognise the particular demands, needs and roles of caregivers of patients who have terminal cancer compared to caregivers of patients who 'live with cancer' and thus interventions for each caregiver group should be tailored accordingly. The extent to which the health and social care needs of cancer caregivers (noted above) are met is unclear ( Soothill 2003 ). It is important to note that the potentially detrimental impact of caring on health may, in turn, affect negatively patient health and well\u2010being ( Northouse 1995 ; Roberts 1994 ). Description of the intervention There is a lack of consistency regarding definitions of the term 'psychosocial intervention', perhaps especially in published cancer care studies ( Hodges 2010 ). This review used a broad definition of a psychosocial intervention focused on non\u2010pharmacological interventions that were designed to inform, educate and increase the coping capacity of informal cancer caregivers and that were delivered verbally by a healthcare professional. Examples of professionally\u2010led, psychosocial interventions include psycho\u2010educational groups ( Bultz 2000 ), cognitive\u2010behaviour therapy ( Cohen 2006 ) and support groups ( Kozachik 2001 ; Northouse 2005 ). A broad range of formats and settings for the delivery of interventions were included in the review, for example, the review included interventions that were delivered to individuals, groups, caregivers only and caregiver\u2010patient dyads (pairs), respectively. A more detailed description of the types of psychosocial interventions that were included in the review is described below ( Types of interventions ). How the intervention might work Psychosocial interventions may be effective in terms of providing support and information to caregivers in a way that increases their coping capacity ( Cohen 2006 ), thereby enabling them to manage more effectively the difficulties associated with caring for someone with cancer. Receipt of a psychosocial intervention may reduce psychological distress ( Kozachik 2001 ) and so improve the QoL of caregivers. Interventions such as cognitive behavioural therapy (CBT) or psycho\u2010educational programmes may result in self\u2010efficacious beliefs among caregivers and new knowledge and skills regarding self\u2010management strategies and their implementation in practice ( Bandura 1977 ; Bandura 1997 ). Psychosocial interventions that focus their delivery towards couples may improve between\u2010partner communication and satisfaction and enhance or assist adjustment to diagnosis by both parties ( Bultz 2000 ; Hilton 1994 ). Furthermore, a psychosocial intervention for a caregiver may improve the quality of care afforded to their relative and lead to better patient psychosocial adjustment ( Roberts 1994 ); and recognition of a caregiver's role by healthcare professionals may improve the caregiving experience and reduce hospital admissions ( Minick 2010 ). Why it is important to do this review Informal caregiving is expected to continue to rise due to increasing cancer prevalence, improved survival and a drive to treat patients in their own communities. The potentially detrimental effects of caring on caregivers (as noted above) have been recognised internationally in cancer policy ( Department of Health 2007 ; Department of Health and Ageing 2008 ; United States DHSS 2003 ). An evidence base of appropriate and effective interventions is required in order to improve decisions about the best ways in which to provide care and support for caregivers and to contribute to the aims of cancer reform strategies in various countries including the UK ( Department of Health 2007 ). A Cochrane Review by our research team on psychosocial interventions for newly diagnosed cancer patients ( Galway 2012 ), found that an increasing number of interventions were being targeted at caregivers. This evidence comes from an examination of the studies that were excluded as part of the Galway 2012 review. There are related Cochrane Reviews that have focused on the effects of interventions for caregivers but which are not cancer\u2010specific ( Legg 2011 ; Vernooij\u2010Dassen 2011 ). Finally, studies of interventions that were delivered to informal caregivers of terminally ill patients were not included because they are the focus of another review ( Candy 2011 ). Interventions targeted specifically towards the growing population of informal caregivers of cancer patients 'living with cancer' warrants dedicated rigorous, systematic attention and synthesis.",
        "summary": "Very low\u2010 to low\u2010certainty evidence suggests that overall quality of life for informal caregivers of people living with cancer may be slightly improved immediately after they receive a multicomponent psychosocial intervention. However, this advantage was not sustained over time, and there may be little to no benefit for the people with cancer for whom they are caring. Low\u2010 to moderate\u2010certainty evidence suggests little to no benefit in terms of anxiety, depression, or psychological distress for caregivers or for the people with cancer receiving care, nor in the physical health status of the people with cancer, either immediately post intervention or at 6 to 12 months\u2019 follow\u2010up. Adverse events included increased distress, particularly related to sexual components of the intervention or sexual functioning. No trials reported on satisfaction with the intervention among caregivers or the people with cancer receiving care. Of note, all eligible trials were conducted in high\u2010income countries."
    },
    "CD004048": {
        "query": "How does lithium compare with antipsychotics for people with acute mania?",
        "document": "Background Description of the condition Bipolar disorder is a chronic, severe mental disorder characterised by episodes of elevated mood (mania), depression or mixed states. The estimated prevalence worldwide is 1%, but there is a wider group (1.5% to 2% total), who have clinically relevant milder symptoms that do not quite meet diagnostic criteria ( Montgomery 2000 ; Philips 2013 ). Both genders and all nationalities, ethnicities and cultures appear to be equally affected ( Philips 2013 ). Average age of onset is 15 to 19 years, although the mean delay in diagnosis between onset of symptoms and formal diagnosis is seven years ( Berk 2007 ). The impact of bipolar disorder is considerable: globally it accounts for 0.3% of disability\u2010adjusted life years, impacts upon the sufferer's ability to carry out normal daily activities and is associated with a high suicide rate ( Alsonso 2011 ; Chen 1996 ). Bipolar disorder also reduces life expectancy \u2010 this is due to a combination of a greater risk of physical health conditions and the high suicide rate ( Goldstein 2015 ; Laursen 2011 ). A diagnosis of bipolar disorder is usually made using one of the two major diagnostic classification systems, the Diagnostic and Statistical Manual of Mental Disorders (DSM\u2010IV; APA 2013 ) or the International Classification of Diseases 10th revision (ICD\u201010; WHO 1992 ). Two main subtypes are recognised: bipolar 1, which requires at least one manic or mixed episode with or without a history of depressive episode(s) and bipolar 2, which requires at least one hypomanic episode and a depressive episode. People who present with bipolar spectrum symptoms but do not fit criteria for bipolar 1 or 2 may be diagnosed with bipolar disorder not\u2010otherwise\u2010specified (bipolar\u2010NOS). Bipolar disorder is a chronic condition, at least 80% of people who have an episode of mania will have recurrent episodes ( NIMH\u2010NIH 1985 ). A manic episode is characterised by elevated or irritable mood, excess energy, racing thoughts, pressured speech, grandiosity, decreased need for sleep, poor attention and an increase in goal\u2010directed activities. Symptoms must last at least seven days (unless hospitalisation occurs before that time). It is often associated with an increase in risk\u2010taking behaviours (e.g. over\u2010spending, promiscuity, dangerous driving), which may be the precipitant for hospitalisation. Many people with bipolar disorder develop mood\u2010congruent psychotic symptoms, usually along grandiose or paranoid themes, and may show pronounced psychomotor agitation or aggression. Hypomania differs from mania in the degree of severity: mood must be elevated for four days and four additional typical manic symptoms must be present ( Samara 2017 ). A mixed episode is diagnosed when people with bipolar disorder experience manic and depressive symptoms (low mood, loss of energy, lack of interest in life) at the same time. Manic episodes may also occur in people who have symptoms of both schizophrenia and mood disorder (schizoaffective disorder). The costs of bipolar disorder are high for both the patient and health services. Admissions to hospital for a manic disorder typically last at least several weeks, and as treatment within a psychiatric intensive care unit is often necessary it can be very costly ( de Zelicourt 2003 ). During a manic episode, patients are typically at high risk of accidental injury due to reckless behaviour, of not eating and drinking sufficiently or of interfering with family or members of the general public and putting themselves at risk. There is a particularly high risk of harm to self during mixed episodes ( Balazs 2006 ). For the individual, as well as the period of acute illness, manic episodes often leave an aftermath of psychological, social and financial problems. Overall, 1/3 of people with bipolar disorder attempt suicide during their lifetime, with 10% to 12% eventually completing suicide ( Pallaskorpi 2017 ). Management of bipolar disorder has two main aspects: treatment of an acute mood episode and maintenance treatment. The latter is designed to prevent or reduce the either the frequency or the intensity of episodes of illness, or both. The pharmacological agent with the strongest evidence base for maintenance treatment in bipolar disorder is lithium ( Burgess 2001 ; Hayes 2016 ; Severus 2014 ). Systematic review evidence has consistently shown that lithium reduces the risk of a mood episode by about one\u2010third ( Severus 2014 ). In addition lithium independently reduces the risk of completed suicide in bipolar disorder and unipolar depression ( Cipriani 2013 ; Riblet 2017 ; Smith 2017 ). Other options for long\u2010term mood stabilisation include anticonvulsive agents (e.g. sodium valproate, lamotrigine, carbamazepine), or atypical antipsychotics, such as quetiapine or olanzapine ( Hayes 2016 ). Options for treatment of acute episodes of mood disorder depend upon the pole of illness; depressive, mania or a mixed state. The evidence base for treatment of bipolar depression is growing, but still in its infancy. Present guidelines recommend either fluoxetine combined with olanzapine/quetiapine, or quetiapine/ olanzapine alone as the first\u2010line option ( Goodwin 2016 ; NICE 2014 ). Other options include lithium plus an antidepressant (first line for those in whom lithium has been previously effective), lamotrigine, an atypical antipsychotic alone, sodium valproate or antidepressants alone ( Taylor 2014 ). The latter is frequently avoided due to the small risk of precipitating a manic episode. Mixed states are typically treated along the same guidelines as a manic episode. Treatment of acute mania has traditionally been with antipsychotics or mood stabilisers, with the addition of sedatives or anxiolytic drugs used as needed. There has been randomised controlled trial (RCT)\u2010level evidence that typical and atypical antipsychotics are effective in treating mania; meta\u2010analysis has suggested that olanzapine, risperidone, quetiapine and haloperidol are the most efficacious ( Cipriani 2011 ; Smith 2007 ; Yildiz 2015 ). For severe mania, or if drug treatments fail, electroconvulsive therapy (ECT) is an effective alternative. ECT has been used since the 1950s and involves passing an electric current through the brain to intentionally trigger a brief seizure. ECT is done under a general anaesthetic and the patient is given muscle relaxant, so the majority only experience mild twitching of their limbs during the few minutes of the procedure. It is not understood how ECT works, but it is known to cause sudden release of neurotransmitters and neurohormones, and effectively relieves symptoms of depression, mania and psychosis more quickly than other interventions. ECT is usually given twice weekly, and patients usually need between six to 12 sessions in total. The main side effects are those related to the general anaesthetic and short\u2010term memory loss, which for most patients does not persist long term. Description of the intervention Lithium (Li, from the Greek 'lithos', meaning stone), is a chemical element with the atomic number 3. It is a member of the alkali family that also includes sodium and potassium. These latter elements, in ionic form, are essential for physiological functioning in humans. The uses of lithium are numerous; it is widely used across the manufacturing and energy sectors, as well as in medicine for the treatment of mood disorders. Medical uses of lithium account for about 2% of global consumption per year, with the majority being used within the energy industry to produce lithium\u2010ion batteries ( Malhi 2017 ). Lithium was first used therapeutically by John Cade in 1949, to treat what he termed 'psychotic excitement'. Over the intervening 60 plus years, lithium has been widely shown to be an effective mood stabiliser and protect against completed suicide ( Burgess 2001 ; Cipriani 2013 ; Geddes 2010 ). It is now the first\u2010line drug for maintenance treatment of bipolar disorder, and an adjunctive treatment for unipolar depression. Lithium is prescribed as one of its salts \u2010 citrate or carbonate \u2010 and due to being a simple natural element is an inexpensive drug. Lithium is an oral medication and is well absorbed in the small intestine, with a bioavailability of 80% to 100% ( Malhi 2017 ). It distributes equally across intracellular and extracellular spaces. Lithium does not undergo any form of metabolism. Lithium is handled very similarly to sodium by the kidney. It is freely filtered by the glomerulus and reabsorbed (\u02dc80%) in the proximal tubule. Lithium renal excretion is in proportion to its plasma level; half\u2010life is 16 to 30 hours ( Bauer 2006 ). Lithium is excreted as a free ion. Clearance is influenced by intrinsic renal disease, age, body weight, low sodium intake, dehydration, cardiac failure and drugs that affect renal function (e.g. diuretics, non\u2010steroidal anti\u2010inflammatory drugs, angiotensin\u2010converting enzyme (ACE)\u2010inhibitors). Care is therefore needed if these situations should arise whilst a patient is taking lithium. Lithium therapy requires regular monitoring of plasma levels. This is because lithium has a narrow therapeutic index, meaning that the dose range that is therapeutic is very close to levels that can become toxic. Lithium is started at a low dose and gradually titrated over a few weeks, taking weekly blood levels 12 hours after a dose until the plasma level is within the therapeutic range, typically 0.5 to 0.8 mmol/L ( BNF 2017 ; NICE 2014 ). The usual maintenance dose varies from 400 mg to 1500 mg daily. Once the dose is stable, blood samples need only be taken every three months. Lithium toxicity is dangerous; coarse tremor, diarrhoea and nausea, muscle weakness, confusion and eventually seizures may occur. The main risk factors for toxicity are changes in sodium levels, for example, due to the drugs previously mentioned, dehydration or a low\u2010salt diet. Most side effects of lithium are dose\u2010related. Common minor symptoms include mild gastrointestinal upset (this usually resolves), fine tremor, polyuria and polydipsia. Longer term, there is a risk of thyroid dysfunction, especially hypothyroidism in women, and hyperparathyroidism ( Shine 2015 ). Thyroid function and calcium levels should be regularly monitored. Lithium is strongly associated with reduced urinary concentrating ability; this is due to a (mostly) reversible nephrogenic diabetes insipidus. In the great majority of patients, the risk of a clinically significant decline in renal function is very low, even in the long term ( McKnight 2012 ; Shine 2015 ). The risk of developing end\u2010stage renal function is extremely low, but renal function should be monitored in all people taking lithium ( McKnight 2012 ). Lithium is associated with a congenital cardiac malformation called Ebstein's anomaly, but the risk to the foetus if exposed to lithium is low, approximately 1:1000. Women of childbearing age can take lithium during pregnancy but the risks to the mothers' mood destabilising need to be carefully balanced against potential risks to the developing foetus ( McKnight 2012 ). How the intervention might work Lithium has been the mainstay of treatment of manic episodes since John Cade's serendipitous discovery of the antimanic effects of lithium and has repeatedly been shown to be effective ( Burgess 2001 ; Cipriani 2013 ; Geddes 2010 ). Lithium is handled by the body in a very similar way to sodium, which is essential for physiological homeostasis. Sodium (and therefore lithium) is present in all parts of the body and is involved in virtually all biological processes. Narrowing down the process by which lithium exerts its mood stabilising effect has therefore proved extremely challenging. Current evidence points towards lithium acting as a neuroprotective agent in the brain: reducing cell death (apoptosis) and enhancing new neuronal growth (neuroproliferation). On a macroscopic level, functional imaging has shown that people treated with lithium have a global increase in grey matter across the cerebrum, but especially concentrated in the prefrontal cortex, amygdala and hippocampus ( Malhi 2013 ). Compared to controls or non\u2010treated bipolar patients, lithium\u2010treated patients have greater grey matter volume. This is important because evidence has shown that bipolar disorder may well be a neurodegenerative condition ( Berk 2009 ). How these changes relate to mood stabilisation is not understood. At a neuronal level, lithium acts to modulate neurotransmission, probably by 'dampening down' the system ( Malhi 2013 ). Lithium appears to have an effect on both excitatory (glutamate/dopamine) and inhibitory (gamma\u2010aminobutyric acid (GABA)) transmission. There is strong evidence that in mania there is an excess of dopamine, with dopamine agonists inducing mania in healthy people, and elevated dopamine levels found in manic people ( Post 1980 ). It appears that lithium reduces dopamine\u2010induced excitatory neurotransmission by interacting with the G\u2010protein\u2010coupled post\u2010synaptic dopamine receptors ( Manji 2000 ). It is not clear how this is mediated at present. Similarly, increased levels of glutamate are seen during mania. Glutamate acts via the N\u2010methyl\u2010D\u2010aspartate (NMDA)\u2010receptor which, when glutamate and its cofactor glycine are absent, has magnesium bound to it. Lithium competes with magnesium to bind, and when bound, unlike magnesium, stimulates the receptor. Chronic lithium stimulation (such as during regular therapy) leads to downregulation of the NMDA\u2010receptors, and an overall reduction in glutamate transmission ( Tsapakis 2002 ). People with bipolar disorder are known to have lower levels of GABA\u2010neurotransmission than controls: this reduction in inhibition leads to excess excitation via glutamate/dopamine and eventually apoptosis and cell loss ( Ng 2009 ). Lithium counteracts this by facilitating inhibition via GABA. Lithium directly enhances GABA release and increases upregulation of the GABA\u2010B receptor ( Ahluwalia 1982 ). Lithium also increases brain\u2010derived neurotrophic factor (BDNF) and B\u2010cell lymphoma 2 (Bcl\u20102), which are neuroprotective factors, via activation of G s \u2010protein\u2010coupled receptors ( Quiroz 2010 ). It appears that these complex actions of lithium occur through multiple second messenger signalling cascades (cyclic adenosine monophosphate (cAMP), inositol phosphate (IP3), protein kinase C (PKC), myristoylated alanine\u2010rich C\u2010kinase substrate (MARCKS) and glycogen synthase kinase 3 (GSK\u20103)). In some way, not clear as yet, lithium moderates the actions of these cascades, which leads to changes in gene transcription and, ultimately, mood stabilisation. Why it is important to do this review Bipolar disorder is a common, chronic condition that represents a high burden of disability for the individual and society. Effective treatments are needed, both for acute mood episodes and maintenance. Unlike maintenance therapy, for which the evidence strongly supports the use of lithium as a first\u2010line treatment, it remains unclear which psychotropic drugs are the most effective for the treatment of mania ( Burgess 2001 ; Cipriani 2013 ). Systematic reviews and network analysis have suggested that multiple antipsychotics (especially haloperidol, olanzapine, risperidone and quetiapine) and mood stabilisers (lithium, valproate, carbamazepine) can all treat mania ( Cipriani 2011 ; Yildiz 2015 ). Current clinical guidelines recommend that if a person with bipolar disorder is not on medication, starting one of the antidopamine agents above should be first line. Other options include valproate, lithium, aripiprazole or carbamazepine ( Goodwin 2016 ; NICE 2014 ). As lithium is the most efficacious treatment for maintenance, and there is evidence that it is effective in mania, it is a strong contender for being a first\u2010line agent ( Cipriani 2011 ; Yildiz 2015 ). Lithium has the advantage that it can be used for both acute treatment and maintenance, which is attractive to many patients. Similarly, whilst lithium carries its own set of potential adverse events, it can be used in patients who have not tolerated anti\u2010dopaminergic agents (e.g. had extra\u2010pyramidal symptoms or raised prolactin) and has much less risk of teratogenicity than valproate. The previous network meta\u2010analyses that have been done using studies dating up to January 2014, had fairly narrow criteria (no children/adolescents, one main outcome measure), and therefore the knowledge base remains incomplete ( Cipriani 2011 ; Yildiz 2015 ). This review aims to assess the available evidence to date (up to January 2017) comparing the effectiveness of lithium to other antimanic agents in treating acute mania in all ages and settings.",
        "summary": "Lithium may be less likely than risperidone to induce a response, drowsiness/somnolence, or weight gain in adults and children with acute mania, and may result in an increase in withdrawals, gastrointestinal adverse effects, and dry mouth. There are probably no important differences in outcomes when lithium is compared with aripiprazole, and the effect when compared with other antipsychotics is uncertain. For adults with acute mania, there was little or no difference in response rates between lithium and chlorpromazine. More people may withdraw from treatment for any cause with lithium (on average, 231 vs 146 per 1000 people; low\u2010certainty evidence), and there was little or no difference in adverse effects between groups. When lithium is compared with risperidone, risperidone may be more effective in reducing manic symptoms (mean difference, around 7 points on a scale from 0 to 56; low\u2010certainty evidence) and it was reported that fewer people taking risperidone withdrew from treatment (on average, 256 vs 157 per 1000 people; moderate\u2010certainty evidence). More people may experience remission (on average, 688 vs 629 per 1000 people); however, this difference did not reach statistical significance, so the effect is imprecise (low\u2010certainty evidence). Lithium was associated with higher rates of most adverse events apart from drowsiness/somnolence. Moderate\u2010certainty evidence shows that there is probably no important difference in response, remission, or withdrawals for any cause when lithium is compared with aripiprazole (on average, 458 vs 468, 400 vs 403, and 513 vs 529 per 1000 people, respectively) nor on the rate of adverse effects. The effect of haloperidol or zuclopenthixol compared with lithium is uncertain. For comparison of lithium versus mood stabilizers, please see CCA 2753 ."
    },
    "CD011851": {
        "query": "Does mobile phone text messaging improve medication adherence for secondary prevention of cardiovascular disease (CVD)?",
        "document": "Background Description of the condition Worldwide, there are an estimated 13 million deaths due to coronary heart disease or stroke each year, and 80% of these deaths occur in low\u2010 and middle\u2010income countries ( Lozano 2012 ). It is estimated that approximately three times as many people will suffer non\u2010fatal cardiovascular events and that each year 35 million people have an acute coronary or cerebrovascular event. Worldwide, at least 100 million people are thought to have prevalent cardiovascular disease (CVD) ( Chambless 1997 ; WHO 2002 ; Yusuf 2011 ). This population has a five times greater chance of suffering a new cardiovascular event than people without known CVD ( Kerr 2009 ). Secondary CVD prevention is defined as action aimed to reduce the probability of recurrence of a cardiovascular event in patients with known atherosclerotic CVD. There are two main aspects to secondary CVD prevention: risk factor management and medications. Drug interventions (such as antiplatelet therapy, angiotensin\u2010converting enzyme (ACE) inhibitors/ angiotensin receptor blockers (ARBs), beta\u2010blockers and statins) have been shown to be cost\u2010effective in reducing the risk of subsequent fatal and non\u2010fatal cardiovascular events in patients with established atherosclerotic cardiovascular diseases and are recommended in international guidelines ( ESC 2012 ; Smith 2011 ; WHO 2003a ). Unfortunately there is a well\u2010documented knowledge\u2013practice gap in the implementation of these proven cost\u2010effective interventions. For example, the Prospective Urban Rural Epidemiology (PURE) study reported that in low\u2010 and middle\u2010income countries up to 75% of patients with known CVD are not using even one recommended medication ( Yusuf 2011 ). Even in high\u2010income countries, adherence to recommended treatments remains sub\u2010optimal. A cross\u2010sectional survey of 12 European countries showed only 26% of patients on antihypertensives achieving control of hypertension and less than 31% of patients on lipid\u2010lowering medication achieving cholesterol control ( Kotseva 2010 ). It has been shown that a considerable proportion of cardiovascular events could be attributed to poor adherence, with 9% of cardiovascular events in Europe attributed to poor adherence( Chowdhury 2013 ). It is estimated that good adherence may be associated with a 20% lower risk of CVD and a 35% reduction in all\u2010cause mortality ( Chowdhury 2013 ). This evidence\u2013practice gap might be influenced by different factors, including health system issues such as lack of accessibility and affordability; treatment complexity; or patients' non\u2010compliance with recommendations ( Nieuwlaat 2013 ). In order to influence non\u2010compliance there is a need to develop scalable and cost\u2010effective behaviour\u2010change interventions. Description of the intervention The global number of mobile phone subscribers is estimated at nearly seven billion ( ICT 2014 ). Even in low\u2010 and middle\u2010income countries the penetration rate of mobile phones is estimated to be 90% ( ICT 2014 ). The widespread ownership of mobile phones and the possibility of automation leads to a potential to deliver behaviour\u2010change interventions to large numbers of people at low cost. Mobile phone interventions are a potentially promising means to deliver messages to increase medication adherence. The use of mobile devices such as phones to support the delivery of medical care is commonly referred to as mHealth. How the intervention might work Mobile phone text messages have been shown to improve medication adherence for a variety of conditions including HIV ( Sharma 2012 ). The development of messages should follow some theoretical framework, and text messages should be developed specifically for the target population and intervention ( Abroms 2015 ). Text messages as an intervention are relatively cost\u2010effective and quick, and do not require that the intended audience need to search for information as it is delivered to them ( Douglas 2013 ). Two recent systematic reviews addressed the question of using mobile phones for all types of medication adherence ( Anglada\u2010Martinez 2015 ; Park 2014b ). The majority of studies found significant improvement in medication adherence through the use of text messages. Overall, few adverse events have been reported with mobile phone text messaging; however, potential rare adverse effects such as road traffic crashes may occur. Why it is important to do this review While there is a great deal of enthusiasm for mHealth interventions among researchers and policy makers, there is still limited evidence for its effectiveness ( Free 2013 ). Systematic reviews have recently been conducted on adherence to medications and reported promising results ( Anglada\u2010Martinez 2015 ; Park 2014b ; Thakkar 2016 ); however, to date no systematic review has been conducted evaluating specifically the effect of mobile phone text messaging on secondary CVD prevention. Furthermore, no review has examined how text messages are created, and if short message service (SMS) are tailored based on individual patient characteristics, and if some patients benefit more than others from interventions. Mobile phone text messaging is of particular interest in low\u2010 and middle\u2010income countries because of wider accessibility of mobile phones with text\u2010messaging capabilities than smart phones.",
        "summary": "For secondary prevention of CVD, it is unclear if mobile phone text messaging helps to improve medication adherence as only very low\u2010certainty evidence is available. Effects of this intervention on other outcomes are also unclear, as data were derived from single underpowered studies or were absent."
    },
    "CD012262": {
        "query": "How does adductor canal block compare with sham treatment or femoral nerve block for adults undergoing knee surgery?",
        "document": "Background Description of the condition Knee surgery (e.g. knee arthroplasty, arthroscopic knee surgery) is very commonly performed in western countries (knee replacement: USA 650,000 (2010); Germany 156,000 (2012)). Major goals following knee surgery include providing sufficient postoperative pain treatment to assist early physical therapy and allowing patients to return early to their physical capacity and to be discharged early from the hospital. Patients suffer from moderate to severe postoperative acute pain ( Gerbershagen 2013 ), and if this pain is insufficiently treated, it might become chronic ( Althaus 2014 ; Pogatzki\u2010Zahn 2012 ). Recently published data demonstrate that the incidence of chronic pain in adults undergoing total knee replacement is 10% to 34% after three months to five years on follow\u2010up pain measurement ( Beswick 2012 ), and around 20% of patients describe moderate to severe sleep disturbances and alterations in quality of life one year after surgery ( Grosu 2015 ). Finally, clear evidence suggests that use of regional analgesia, especially in joint arthroplasty surgery ( Guay 2017 ; Guay 2017a ), is associated with superior postoperative outcomes (pulmonary compromise, pneumonia, infection, acute renal failure, mechanical ventilation, blood product transfusion) ( Memtsoudis 2013 ), and it might reduce the risk of chronic postsurgical pain ( Weinstein 2018 ). Description of the intervention In recent years, adductor canal block through selective block of sensory nerves has become an interesting new option for postoperative pain treatment following knee surgery. The knee is innervated by the femoral nerve (via three vasti branches and the saphenous nerve), the posterior branch of the obturator nerve, and genicular branches of the tibial and common peroneal branches of the sciatic nerve ( Bendtsen 2014a ). The adductor canal includes the femoral vessels, the saphenous nerve, a nervous branch to the vastus medialis muscle, and sometimes the posterior branch of the obturator nerve ( Bendtsen 2014b ). The adductor canal is roofed by continuous fascia starting with the vasoadductor membrane distally ( Andersen 2015 ). Adductor canal block, which is performed most often via ultrasound, can be used as a single shot or as continuous nerve block provided through a catheter. How the intervention might work Postoperative pain following knee surgery can be managed with systemic analgesics or regional blockade (neuraxial blockade or peripheral nerve blocks). Neuraxial blocks (e.g. epidural catheters) are used less frequently for postoperative pain treatment following knee surgery; distal peripheral nerve blocks (e.g. femoral nerve blocks) are performed more frequently because they involve lower risk for severe adverse events (e.g. epidural bleeding) ( Cozowicz 2015 ). For a long time, femoral nerve block was the gold standard regional analgesic technique for postoperative pain treatment following knee surgery ( Chan 2014 ). However, adductor canal block might be associated with a lower degree of motor blockade than femoral nerve block, and might provide better conditions for early rehabilitation, quicker return to mobility, and less risk for accidental falls during hospital care compared with femoral nerve block ( Mariano 2014 ). It must be mentioned that two other large studies have indicated that appropriate fall prevention strategies should be used for all hospitalized patients, even those not receiving regional blockade ( Johnson 2014 ), and it is not clear whether regional analgesia definitively increases risk for inpatient falls ( Memtsoudis 2014 ). After the femoral vessels have been identified, the saphenous nerve might be blocked typically at two locations: subsartorially, or more distally within the adductor canal. Cadaveric studies have demonstrated that dye is normally spread freely into the adductor canal after a subsartorial injection, so that the primary injection site might not be clinically relevant for clinical efficacy ( Cowlishaw 2015 ; Tubbs 2007 ). Several cadaveric studies have revealed that a small amount of dye spreads to other nerves as well (e.g. sciatic, femoral), so that possible motor blockade cannot be definitively excluded ( Andersen 2015 ; Cowlishaw 2015 ; Gautier 2015 ). Why it is important to do this review In patients undergoing knee surgery, femoral nerve block (and epidural catheter for special cases such as bilateral knee arthroplasty) is believed to be the gold standard for acute pain management because it provides better analgesia than is provided by systemic analgesic treatment for adults undergoing knee surgery ( Chan 2014 ). However, this block might be associated with a higher degree of motor blockade, possibly increasing the risk for inpatient falls ( Johnson 2013 ; Wasserstein 2013 ). As has been mentioned, evidence regarding use of femoral nerve block and risk for inpatient falls is currently inconclusive. Many RCTs published in recent years have compared analgesic efficacy and safety between adductor canal block and other regional analgesic techniques (particularly femoral nerve block). A quantitative systematic review has not been conducted to analyse analgesic efficacy and adverse effects of adductor canal block compared with other regional analgesic techniques or systemic analgesic treatment for patients undergoing knee surgery.",
        "summary": "Adductor canal block may make little or no difference in postoperative pain at rest and during movement compared with sham treatment or femoral nerve block. Low\u2010certainty evidence suggests little or no difference in postoperative pain at rest at 2 and 24 hours post surgery or during movement at 2 or 24 hours among those receiving adductor canal block compared with those receiving sham treatment. The mean morphine requirement was found to be lower in those undergoing treatment with adductor canal block than in those receiving sham treatment (mean difference \u201015.88 mg; 5 RCTs with around 200 participants). Little or no difference was reported for opioid\u2010related adverse events (including nausea, vomiting, and sedation). When adductor canal block was compared with femoral nerve block, low\u2010 to high\u2010certainty evidence showed little or no difference in postoperative pain at rest and during movement at 2, 24, and 48 hours post surgery. Further, little or no difference in the mean morphine requirement and in opioid\u2010related adverse events was reported. In both comparisons, analyses of accidental falls and rates of failed block were underpowered to justify conclusions, and high heterogeneity in dose and duration of adductor canal block was observed across trials generally."
    },
    "CD005231": {
        "query": "Can smokers be helped to reduce the harm caused by cigarette smoking by smoking fewer cigarettes or using different tobacco products?",
        "document": "Background Description of the condition The morbidity and mortality associated with smoking is well established. People who stop smoking can reduce their risk of developing smoking\u2010related diseases ( Anthonisen 2005 ; Doll 2004 ), so the primary strategy for reducing harm due to tobacco smoking must be to encourage cessation. However, despite the fact that most people who smoke say that they want to stop, the prevalence of smoking is declining very slowly, even in those countries where tobacco control policies are well developed, and in some cases prevalence is still rising ( Bilano 2015 ). In 2005 the World Health Organization (WHO) Framework Convention on Tobacco Control formalized a global commitment to reduce tobacco use worldwide and make tobacco control a global health priority ( WHO 2003 ). In 2014, based on this Framework, the WHO member states went on to agree a target of a 30% relative reduction in tobacco use worldwide by 2025 ( WHO 2013 ). However, based on the actual decline in smoking prevalence, predictions of trends to 2025 suggest that only 25% of countries worldwide will be likely to experience this decrease in smoking men, and only 52% will be likely to experience this decrease in smoking women from 2010 to 2025. This means fewer than half of countries globally are likely to meet the WHO targets, and this issue is not limited only to low\u2010 and middle\u2010income countries ( Bilano 2015 ). Although it is important to continue to motivate and assist people to quit, it may therefore be reasonable to seek ways to reduce the harm from continued smoking for people who are not ready to or cannot quit, as a secondary strategy to improve global health. Description of the intervention There are multiple approaches that may have potential for harm reduction for people who do not want to give up tobacco or nicotine use completely. Shiffman 2002 has provided a catalogue of these with a conceptual structure of their characteristics. They cover many different intended intermediate effects and mechanisms. They also differ in the likely ease of the behaviour change needed to adopt them and related appeal to smokers, and their expected population risk. Shiffman 2002 categorizes tobacco harm reduction into the following four approaches: Methods to establish and adhere to tobacco abstinence, as acknowledged above; The use of tobacco products in a way or in a form that is less harmful than traditional products; The use of pharmaceutical products to reduce tobacco use or the harm caused; and Changes in behaviours that will reduce harm. Methods to establish and adhere to tobacco abstinence, as acknowledged above; The use of tobacco products in a way or in a form that is less harmful than traditional products; The use of pharmaceutical products to reduce tobacco use or the harm caused; and Changes in behaviours that will reduce harm. Categories two to four are the subject of this review. Products that fall within the second category are referred to as potential reduced\u2010exposure products (PREPs), which are: \"(a) modified tobacco products that contain reduced levels of one or more toxins (for example, cigarettes with reduced tobacco\u2010specific nitrosamines through new curing processes, the addition of catalysts to reduce polycyclic aromatic hydrocarbon carcinogens produced by smoke, the use of genetically modified plants to reduce nicotine or nitrosamines, or the use of filters to selectively reduce toxicants), (b) cigarette\u2010like devices, such as those that heat rather than burn tobacco, and (c) oral non\u2010combustible products, such as snus\" ( Hatsukami 2005a ). Some oral smokeless tobacco products have been estimated to be approximately 90% less harmful than smoking cigarettes ( Levy 2004 ). Research on alternative tobacco products has largely been conducted within the tobacco industry and has generally attempted to modify the characteristics of existing tobacco\u2010containing products, or to design new types of commercial tobacco products, to make tobacco use less dangerous. Very large, independently\u2010conducted, long\u2010term trials are required to fully evaluate their effects ( Murrelle 2010 ). The use of pharmaceutical products to reduce harm could, for example, refer to using any of the existing pharmacotherapies already available to help people to quit smoking, such as nicotine replacement therapy (NRT), varenicline or bupropion, to reduce tobacco consumption. There is variation in whether NRT is licensed for use as a reduction aid across countries; for example, the US Food and Drug Administration (FDA) have not approved the use of NRT for smokers who wish to cut down the amount they smoke without wanting to quit. However, the Medicines and Healthcare Regulatory Authority (MHRA) in the UK have licensed it for this purpose. Finally, \"changes in behaviors that will reduce harm\" applies to behaviour change interventions such as reducing the number of cigarettes smoked each day (CPD), otherwise known as 'controlled smoking', which could be carried out alongside the use of other aids, such as PREPs or pharmaceutical products. How the intervention might work There are two major routes through which we would expect the above harm reduction approaches to work: 1) by promoting subsequent smoking cessation, as a by\u2010product of harm reduction (rather than encouraging quitting specifically); and 2) by reducing the health effects of smoking without quitting completely. There is evidence that the smoking reduction potentially promoted by the majority of harm reduction approaches is associated with an increase in subsequent cessation. A qualitative systematic review ( Hughes 2006 ) of 19 observational studies and randomized controlled trials (RCTs) showed no indication that CPD reduction had a negative impact on future cessation; in fact, 16 of the 19 studies found reduction was associated with higher eventual quit rates.This association may be because reduction increases self\u2010efficacy, disrupts pharmacological conditioning or reduces dependence, or both. However, there is also evidence that attempts at smoking reduction can be undermined by other unconscious adjustments to smoking behaviour, for example taking longer, deeper puffs of a cigarette to maintain previous nicotine levels ( Scherer 1999 ). Using an alternative reduced\u2010harm nicotine source (such as NRT, snus, electronic cigarettes (ecigs)) to support behavioural reduction could help to compensate for this, by reducing nicotine withdrawal and subsequent cravings. In Sweden the use of snus as a cigarette substitute has been credited for a reduction in smoking among men, which is associated with lower tobacco\u2010related mortality rates in Sweden than in other European countries ( Ramstrom 2014 ). There is mixed evidence for the effects of smoking reduction on disease and health markers. For example, a systematic review reports that their largest included study found a reduction in lung cancer risk in smokers who reduced consumption compared with those who maintained their smoking behaviour ( Pisinger 2007 ); however, a later study found no evidence of a reduction in risk ( Hart 2013 ). There is some evidence that smoking reduction can reduce biomarkers related to the risk of cancer or carcinogen exposure, or both; however, it is unclear whether this reduces the incidence of cancer ( Pisinger 2007 ). Why it is important to do this review Most evidence investigating the link between smoking reduction and health currently comes from observational, epidemiological studies, and there are issues with this. Firstly, because smoking is generally measured at two time points and if the later one demonstrates a lower rate than the initial one, it is assumed that smoking reduction has been maintained throughout the follow\u2010up period, which may not be the case. Secondly, many of the studies have not used biomarkers to validate smoking consumption, which could mean that compensatory smoking (such as deeper puffs) is not accounted for. Finally, many of these studies have not included people who used alternative nicotine products to compensate for smoking, which may impact on the success of reduction, and therefore on the associated changes in health risks ( Begh 2015 ). The nature of RCTs means that all of these factors can be controlled for more effectively (although this is not always the case), and where this has been done we can be more confident that results indicating associations between reduction and health are causal. By reviewing these studies, we can make a valuable contribution to the literature and to the debate surrounding harm reduction approaches. However, the use of harm reduction strategies in relation to tobacco smoking is a controversial topic, which divides opinion. There are concerns that encouraging people to reduce their smoking may undermine their motivation to quit smoking in the long term, and that this may encourage the tobacco industry to market 'reduced risk' products and carry out biased research on their effectiveness ( Hatsukami 2004 ). The availability of ostensibly less harmful tobacco products could even lead never\u2010smokers to start smoking, or ex\u2010smokers to relapse, in the belief that the risks are acceptable. These are questions that cannot be answered by RCTs alone, and are outside the scope of this review; the results of this review should therefore be considered alongside research which investigates any potential negative impact of promoting harm reduction. Despite the ongoing debate regarding the promotion of tobacco use harm reduction, in 2013 the UK National Institute for Health and Care Excellence (NICE) published their first version of guidance on how to offer harm reduction approaches to smokers unwilling or unable to quit in one step, who may want to use a long\u2010term safer substitute for smoking, or may only be ready to reduce the amount they smoke ( NICE 2013 ). This guidance therefore recommends some non\u2010traditional interventions where the ultimate goal is still to quit (reducing smoking to quit), but also recommends behavioural smoking reduction with or without the use of NRT, and temporary abstinence with or without NRT for smokers who need to stop smoking for a set period of time, for example, during working hours, a long\u2010haul flight, or a hospital stay. The guidance highlights that the health benefits of smoking reduction, rates of relapse and progression to stopping smoking among people who have opted to reduce the amount they smoke are still unclear, and that better evidence is therefore still required to support and inform the harm reduction approach. In addition, new products, such as ecigs, that have the potential to be used in a harm\u2010reduction capacity have also become available since the last update of this review. Evidence is needed to inform whether these harm reduction interventions could be useful in reducing tobacco\u2010related harm in smokers who cannot, or do not wish to, quit.",
        "summary": "Some people may be willing to cut down the number of cigarettes smoked but may not be ready to quit completely. Long\u2010term change in health status would be a direct indicator of whether cutting down reduces long\u2010term smoking\u2010related harm, but no RCT evidence investigating this was found. Reviewers instead found studies looking at smoking reduction and/or cessation rates and found that people can be helped to reduce the number of cigarettes and to quit over the long term by using nicotine replacement therapy (NRT). However, findings were based on a relatively small number of trials and do not directly show the harm reduction and evidence is of low or very low certainty . Eight trials with over 3000 participants found that more people who used NRT than placebo reduced the number of cigarettes smoked per day by at least half (52 vs 30 per 1000 people). Low\u2010certainty evidence suggests that NRT was also more likely than placebo to help people quit smoking (73 vs 39 per 1000 people stopped). Two trials with just over 1300 participants found that when NRT was provided together with counseling, more people reduced the number of cigarettes smoked per day by half compared with those receiving brief advice only (170 vs 97 per 1000 people). RCT evidence supporting other harm reduction intervention aids (bupropion, varenicline, electronic cigarettes, low\u2010nicotine cigarettes) or behavioral reduction advice is of low or very low certainty (one trial per comparison). Adverse events were sparsely and inconsistently reported. Three studies reported that NRT was associated with more nonserious adverse events compared with placebo (31% vs 9%; 82 events vs 26 events; 506 events vs 370 events, respectively). Adverse events from bupropion, varenicline, or electronic cigarettes were comparable across groups."
    },
    "CD001181": {
        "query": "How does antimicrobial prophylaxis affect outcomes in people undergoing colorectal surgery?",
        "document": "Background Abdominal surgical wound infections in patients having operations on the large intestine occur in about 40% of those who do not receive antibiotic prophylaxis ( Baum 1981 ). When an infection does occur, it often involves more than simple drainage of subcutaneous pus and dressing changes at home. Indeed, the risk of death is doubled , intensive care unit admission is more likely and average hospital stay is lengthened by five days ( Kirkland 1999 ). In patients with surgical wound infection, length of stay is one week longer and hospital cost averages USD 13,746 more than in those without infection (USD 2,009 Mahmoud 2009 ) and an average of USD 6,200 in home care costs are incurred after discharge (USD 2,004, Smith 2004 ). The risk of hospital readmission is also greatly increased ( Kirkland 1999 ). Contrary to traditional beliefs about pus being a good sign, it seems that survival rates in patients who have colon cancer removed are reduced when wound infection has occurred ( Nespoli 2004 ), though the reasons for this are unknown. Reducing the risk of surgical wound infection is clearly a priority in terms of patient safety and cost containment of medical care. In 1981, an early systematic review that compared wound infection risk in elective colorectal surgery patients receiving antibiotic prophylaxis to those randomised to placebo or no treatment found that infection risk was so diminished with antibiotics that the review concluded that future studies in this field that included no treatment controls would no longer be ethical ( Baum 1981 ). It was also stated that a gold\u2010standard antibiotic should be established, so that in all future studies one arm of the study would include the gold standard as the acceptable benchmark from which to judge the new antibiotic. Since then, guidelines have been published that suggest an optimal choice of antibiotic and also dosage regimens ( Medical Letter 2012 ). However, a survey of American hospitals found that these guidelines are followed approximately half of the time ( Bratzler 2005 ). Published summaries of evidence are found in a number of systematic reviews. The first, cited above, dealt only with any antibiotic versus no treatment controls ( Baum 1981 ). Another looked only at one aspect of route of administration: oral plus intravenous antibiotics versus intravenous antibiotics alone ( Lewis 2002 ), as did Bellows 2011 . A fourth was a global review of studies published from 1984 to 1997, but focused more on individual comparisons rather than the global issues mentioned above ( Song 1998 ). This is just a sample of many other such reviews. This systematic review was undertaken in order to determine whether evidence exists to reconfirm the need for antibiotic prophylaxis, to determine what spectrum of bacteria needs to be addressed by the choice of antibiotic (e.g. Gram\u2010negative versus Gram\u2010positive bacteria), to determine the best timing and route of antibiotic administration and, finally, to see whether any antibiotic performs better than the gold standard currently recommended in published guidelines.",
        "summary": "In patients undergoing colorectal surgery, there is high quality evidence that antibiotic prophylaxis reduces abdominal wound infections (125 per 1000 people taking antibiotic had infections vs. 368 per 1000 people with no antibiotic). Various antibiotics have been used across these studies and no single antibiotic regimen was specifically analyzed against placebo. Combined aerobic and anaerobic coverage was associated with reduced wound infections compared with usage of aerobic cover alone. There is no evidence to support prolonged use of antibiotics (for more than 3 days) over shorter use (primarily single dose) following surgery based on trials in 5,000 participants"
    },
    "CD004307-0": {
        "query": "Can providing incentives to stop smoking to adult smokers of mixed populations increase quit rates?",
        "document": "Background Description of the condition Smoking is the leading cause of preventable death and disease worldwide. Most adults who smoke wish to quit, but quitting is challenging and despite the presence of effective evidence\u2010based cessation methods, quit rates remain low. Quitting smoking can lead to substantial health gains, even later in life. The earlier someone quits smoking, the more they reduce their risk of developing smoking\u2010related diseases ( WHO 2018 ). Description of the intervention There is interest and support for incentive\u2010based programmes to change unhealthy behaviours, including smoking, weight loss, and alcohol consumption, and to increase levels of physical activity ( Giles 2014 ; NICE 2010 ). However, financial incentives to promote behaviour change are controversial. Qualitative research demonstrates that public acceptability of incentives varies ( Giles 2015 ), perhaps due to misinformation or a lack of education ( Robertson 2018 ), and a concern about commissioning (funding of) incentive\u2010based schemes. There has also been a concern that incentive schemes may only be effective for the duration of time that incentives are offered. There may be cultural variation in acceptability, such that implementation of incentive\u2010based programmes may prove more difficult in some settings ( Berlin 2018 ). Many developing countries, particularly in Latin America, operate conditional national or regional cash transfer programmes of monetary rewards for behaviour change or compliance, often targeting improvements in child and maternal health ( Lagarde 2009 ; Paes\u2010Sousa 2011 ; Powell\u2010Jackson 2011 ). In the UK, incentive schemes often focus on encouraging pregnant women to quit smoking, with well\u2010established programmes such as 'Give It Up For Baby' ( Ballard 2009 ; Radley 2013 ), conducted in Tayside (Scotland) and awarding grocery vouchers for verified abstinence. A series of studies included in the last update of this review, conducted in the USA ( Donatelle 2000a ; Donatelle 2000b ; Donatelle 2002 ; Heil 2008 ; Higgins 2004 ; Higgins 2014 ) and a large randomised trial in the UK ( Tappin 2015a ) also attest to the tobacco control community's interest in the feasibility of rewarding pregnant women who smoke for achieved abstinence. How the intervention might work Incentives and rewards (terms used interchangeably in studies contributing to this review) routinely feature in smoking cessation programmes. Theory suggests they might work according to behavioural processes of operant conditioning (positively rewarding the desired behaviour), or by providing short\u2010term gain for behaviour change that ultimately results in long\u2010term gain, but is perceived as less proximal to the individual (delay discounting) ( Gneezy 2011 ; Miglin 2017 ). Incentives can be used to encourage recruitment into the programme, to reward compliance with the process, and to reward cessation achieved at predefined stages, usually contingent on production of a biochemically\u2010confirmed cessation outcome. A variety of rewards have been used for these purposes, including cash payments, vouchers exchangeable for goods (excluding alcohol and cigarettes) or leisure activities, salary bonuses, or promotional items such as T\u2010shirts, pens and bags. Rewards can be given for attendance at the programme and at follow\u2010up appointments, irrespective of subsequent smoking status (i.e. guaranteed or non\u2010contingent), or can be paid and scaled relative to the participant's success in smoking cessation (i.e. contingent) ( Higgins 2002 ). Recent trials and systematic reviews have explored variations in the type, the scale, and the scheduling of rewards ( Adams 2014 ; Crossland 2015 ; Giles 2014 ; Jochelson 2007 ; Leeks 2010 ; Sigmon 2012b ), and in their acceptability as a mechanism for behaviour change ( Hoddinott 2014 ; Thomson 2014 ). This review focuses on rewards for abstinence (as opposed to attendance, etc.). Why it is important to do this review This updated review is a modified version of our previous review ( Cahill 2015 ). Over the thirteen\u2010year lifetime of this review, the debate about incentive\u2010based smoking cessation programmes has shifted from their feasibility (i.e. can they work?) to their effectiveness (i.e. do they work?), relative success or limitations of the mechanisms deployed ( Higgins 2012 ; Promberger 2012 ), the merits of rewards ('carrots') versus penalties ('sticks') ( Adams 2014 ; Lynagh 2013 ; Volpp 2014 ), the extent to which achieved changes can be maintained ( Jochelson 2007 ; Strickland 2014 ), the possibilities of unintended consequences ( Marteau 2009 ; Thomson 2014 ), and the acceptability and implementation of incentive\u2010based programmes ( Berlin 2018 ). Although many of the older included studies may not address these issues, our review contributes to a growing evidence base that defines the rationale for incentive\u2010based programmes and identifies areas for further investigation. In this update we also explore the use of incentives in sub\u2010populations of participants, consider the longevity of effects of incentives, and the cumulative value of incentives optimal for cessation outcomes.",
        "summary": "High\u2010certainty evidence shows that providing incentives such as cash for abstinence (contingent rewards) or monetary incentives in the form of vouchers or more complex payment schedules to adult smokers from mixed populations improves smoking cessation rates at 6 to 24 months. However, quit rates were very low in both groups (70 vs 47 per 1000 people). In addition, some trials compared contingent reward and automatic reward groups, with the latter serving as the control group. Trials did not assess adverse events associated with incentives."
    },
    "CD002745-1": {
        "query": "If there is an outbreak of influenza A in the community, do amantadine and rimantadine given prophylactically prevent the development of influenza in children?",
        "document": "Background Description of the condition Influenza is an acute and usually self limiting respiratory illness caused by influenza A and B viruses, which are members of the Orthomyxoviridae family ( Nicholson 1992 ). Influenza may cause annual epidemics and intermittent pandemics ( Sasaki 2011 ). Typically, seasonal influenza occurs most frequently during autumn and winter in temperate regions, but in some tropical countries it may occur throughout the year with one or two peaks during rainy seasons ( Monto 2008 ; Yang 2010 ). Although the natural transmission of the influenza virus predominantly occurs via aerosols dispersed by coughing or sneezing, it is also transmitted by nasal secretions and contact with contaminated surfaces. While all respiratory viruses, including influenza, use the nose as the entry channel, they can also enter through the tear ducts, draining into patients' sinuses and airways ( Bitko 2007 ). The virus particles are deactivated by the ultraviolet rays in sunlight and common disinfectants such as soap ( Barik 2012 ). The illness is characterised by an abrupt onset of symptoms. These symptoms include headache, fever, general aches, weakness and myalgia, accompanied by respiratory tract signs, particularly cough and sore throat. However, a wide spectrum of clinical presentations may occur, ranging from a mild, febrile upper respiratory illness, to severe prostration and respiratory and systemic signs and symptoms. The most common complication that occurs during outbreaks of influenza is pneumonia (both viral and bacterial). A number of extra\u2010pulmonary complications may also occur. These include Reye's syndrome in children (most commonly between two and 16 years of age), myocarditis, pericarditis and central nervous system (CNS) diseases. Again these include encephalitis, transverse myelitis and Guillain\u2010Barr\u00e9 syndrome ( Barik 2012 ; Wiselka 1994 ). An interesting and clinically relevant aspect of pandemic and epidemic influenza that sets it apart from seasonal influenza is the induction of the so\u2010called cytokine storm, consisting of interleukin\u20106, tumour necrosis factor and interferon\u2010g. Together, these proinflammatory cytokines cause systemic inflammatory response syndrome, leading to multi\u2010organ failure that includes airway destruction, vascular endothelial damage and plasma leakage ( Barik 2012 ; Cheung 2002 ) Description of the intervention Nowadays there are two main measures for the treatment and prophylaxis of influenza viruses: immunisation using influenza vaccines directly isolated from influenza A and B viruses and antiviral agents ( Demicheli 2000 ; Noah 2013 ). Vaccination is the primary strategy for the prevention of influenza ( Antanova 2012 ; Hsu 2012 ). Nevertheless, there are a number of likely scenarios for which effective antiviral agents would be of utmost importance. For example, the available evidence on the safety, efficacy or effectiveness of influenza vaccines for people aged 65 years or older is of poor quality ( Jefferson 2010 ; Thomas 2011 ). Vaccination among the elderly may not be as effective as their immune systems are less responsive ( Sasaki 2011 ). Influenza vaccines are efficacious in children older than two but little evidence is available for children under two ( Demicheli 2012 ). During any influenza season, antigenic drift in the virus may occur after formulation of the year's vaccine. The vaccine can therefore be less protective and outbreaks can more easily occur in high\u2010risk populations. In the course of a pandemic, vaccine supplies would be inadequate. Moreover, vaccine production by current methods cannot be carried out with the speed required to halt the progress of a new strain of influenza virus. Therefore, it is likely that vaccines would not be available for those infected by the first wave of the virus ( Hayden 2004 ). Additionally, in a study published in 2013, the author stated that vaccination\u2010only strategies were not cost\u2010effective for any pandemic scenario, saving few lives and incurring substantial vaccination costs ( Kelso 2013 ). Vaccination, coupled with long duration social distancing, antiviral treatment and antiviral prophylaxis, was considered to be cost\u2010effective for moderate and extreme pandemics, as it can save lives while simultaneously reducing the total pandemic cost ( Kelso 2013 ). Antiviral agents therefore form an important part of a rational approach to influenza management ( Kelso 2013 ; Moscona 2005 ). Antiviral drugs for influenza are currently divided into two classes: M2 ion channel inhibitors and neuraminidase inhibitors. The first class includes amantadine and rimantadine and the latter zanamivir, oseltamivir, laninamivir (approved in Japan) and peramivir (approved in Japan and Korea) ( Barik 2012 ). M2 ion channel inhibitors affect ion channel activity through the cell membrane and are reported to be effective by interfering with the replication cycle of type A viruses (but not type B). The neuraminidase inhibitors interfere with the release of influenza virus progeny from infected host cells and are effective against influenza A and B ( Moscona 2005 ). Both drug classes have shown partial effectiveness for the prevention and treatment of influenza A viruses, although neuraminidase inhibitors are less likely to promote the development of drug\u2010resistant influenza ( Moscona 2005 ). Resistance to M2 inhibitors remained low until 2003 ( Bright 2005 ; Ziegler 1999 ). An epidemiological study into resistance to amantadine carried out from 1991 to 1995 described a frequency of 1% (16/2017) of resistant variants among H1N1 and H3N2 viruses ( Ziegler 1999 ). However, there was a subsequently a dramatic increase in strains of influenza A (H3N2) with a specific mutation (Ser31Asn). An increase in resistance to amantadine was showed in communities located in Asia and North America ( Bright 2005 ; Bright 2006 ). This resistance in 70% to 90% of strains occurred despite the absence of sustained selective drug pressure ( Bright 2005 ; Bright 2006 ). During the 2005 to 2006 season, 16% of H1N1 and 91% of H3N2 viruses were resistant around the world. Although the estimate for the proportion of resistance in H1N1 viruses was very low, an analysis conducted in China showed that the frequency of resistant H1N1 viruses had greatly increased from 28% (8/29) in the 2004 to 2005 season to 72% (33/46) in the 2005 to 2006 season. Similar studies were conducted in other countries in the 2005 to 2006 season. The following frequencies of resistance were obtained: 45% (13/29) in Europe, 24% (4/17) in Taiwan and 33% (1/3) in Canada ( Deyde 2007 ). A global pandemic emerged in 2009, caused by a new influenza A strain (H1N1) ( WHO 2010a ). All influenza A (H1N1) viruses tested in WHO Collaborating Centres to date have been shown to be resistant to amantadine and rimantadine ( WHO 2011 ; WHO 2012 ). When an avian influenza A (H7N9) virus was detected as the cause of human infections in China, its susceptibility to antiviral drugs was assessed. The outbreak viruses carried the established adamantine resistance marker. Once again neuraminidase inhibitors remained the only licensed treatment option ( Li 2014 ). Influenza A resistance to amantadine and rimantadine has been frequently reported over the last few years and, as such, it may seem unnecessary to continue testing sensitivity to these drugs. However, patterns of sensitivity and resistance of influenza viruses to antiviral drugs may change over time and so we consider it necessary to continue monitoring sensitivity and resistance. How the intervention might work The use of amantadine and rimantadine for the treatment and prevention of influenza A in adults has already been the topic of a review ( Jefferson 2006b ). The results of that review confirmed that amantadine and rimantadine had a comparable efficacy and effectiveness in the treatment of influenza A in healthy adults, although their effectiveness in interrupting transmission was probably low. As previous pandemics proved to be susceptible to this class of drugs, it seems reasonable to review the evidence for amantadine and rimantadine for treating and preventing influenza A in children and the elderly ( Hayden 2006b ). Why it is important to do this review Although the disease occurs in all age groups ( Pineda Solas 2006 ), the risks of complications, hospitalisations and deaths from influenza are higher among three groups of people: 1) persons older than 65 years; 2) young children; and 3) persons of any age who have medical conditions that place them at increased risk. Rates of infection are highest amongst children and they are also one of the most important links for transmission ( Dolin 2005 ). Pandemics occur when influenza spreads globally, infecting 20% to 40% of the world's population in one year. This results in as many as 10 million deaths ( WHO 2003 ). They usually arise in China, where pigs, ducks and humans live in close proximity to each other, and spread westward to the rest of Asia, Europe and the Americas ( Bonn 1997 ). In the past 110 years there have been five pandemics caused by different influenza A viral subtypes. The Spanish influenza pandemic (1918 to 1919) is considered to have caused an estimated 40 million deaths worldwide. Most years, typical influenza epidemics infect 5% to 20% of the population and result in anywhere between 250,000 and 500,000 deaths, according to the World Health Organization (WHO), although other estimates accounting for deaths due to complications of influenza are as high as 1 million to 1.5 million. In 2009, a new influenza A strain (H1N1) caused a global pandemic. According to the WHO, as of 24 January 2010, more than 214 countries and overseas territories had reported laboratory\u2010confirmed cases of pandemic influenza H1N1, resulting in at least 18,449 deaths ( WHO 2010a ). In an earlier version of a Cochrane review in adults, the review authors stated that neuraminidase inhibitors were effective in reducing symptoms and complications, however there are now doubts about their effectiveness against complications ( Jefferson 2014 ). In a Cochrane review published in 2007, the review authors concluded that oseltamivir may be considered for the treatment of children aged one to 12 years with influenza infection ( Matheson 2007 ). This antiviral is likely to shorten the duration of symptoms, hasten the return to normal activities and reduce the incidence of secondary complications. Nevertheless, the review authors also concluded that more data were needed to clarify the benefits of neuraminidase inhibitors for the treatment of influenza in asthmatic children (including addressing the potential confounder of prior vaccination). Nowadays, neuraminidase inhibitors are used as a prescription drug for patients suffering from influenza on the recommendation of the WHO ( WHO 2010b ). Governments have spent billions of dollars stockpiling neuraminidase inhibitors as a public health measure ( WHO 2010b ). In previous pandemics, the influenza A virus was susceptible to amantadine and rimantadine. Therefore, these antivirals could be a less expensive alternative in the management of influenza if the circulating strain proves to be susceptible to amantadine and rimantadine ( Hayden 2006b ). However, we should emphasise the resistance patterns of the pandemic viruses in 2009. All influenza A (H1N1) viruses tested in WHO Collaborating Centres to date were sensitive to zanamivir and all were resistant to amantadine and rimantadine ( WHO 2011 ). These facts reinforce the importance of conducting and maintaining reviews of a variety of treatments, especially less expensive ones, for the treatment and prevention of influenza.",
        "summary": "There is low quality evidence that amantadine is effective in prevention of influenza A in children compared with placebo or no treatment: the incidence of influenza in children receiving amantadine was 3 per 100 people (95% CI 1 to 7) compared with 11 per 100 receiving placebo. There was no statistically significant protective effect for rimantadine compared with placebo, although there is some evidence of a protective effect that may have not been shown because of lack of power in the analysis. The reviewers found no studies comparing amantadine or rimantadine versus neuraminidase inhibitors. Neuraminidase inhibitors are indicated for influenza prophylaxis despite low level of evidence for their effectiveness in reducing severe complications. Amantadine should not be used unless the influenza strain(s) causing the outbreak is susceptible to amantadine, keeping in mind that the resistance of the influenza A virus to amantadine and rimantadine has increased dramatically in recent years. There is no evidence of adverse effects of amantadine compared with placebo in children."
    },
    "CD000166": {
        "query": "How does planned cesarean section compare with planned vaginal birth for term breech delivery?",
        "document": "Background Description of the condition Breech presentation occurs in 3% to 5% of all pregnancies at term, accounting for the greatest proportion of non\u2010cephalic presentations. Sixty\u2010five to seventy per cent of breech babies are in the frank breech position, in which the baby's legs are flexed at the hip and extended at the knees (with feet near the ears). Non\u2010frank breech position include complete breech (both the baby's hips and knees are flexed) and footling (presenting one or both feet first, which is more common with premature fetuses than at term). Some babies will spontaneously turn to a cephalic position before birth, and others can be rotated using external cephalic version. However, for those persisting in the breech position, a decision will need to be taken to deliver the baby vaginally or by caesarean section. Factors which have been associated with breech presentation include: nulliparity; previous breech birth; uterine anomaly; contracted pelvis; use of anticonvulsant drugs; placenta praevia; cornual placenta; decreased or increased amniotic fluid volume; extended fetal legs; multiple pregnancy; prematurity; short umbilical cord; decreased fetal activity; impaired fetal growth; fetal anomaly; and fetal death. Breech babies tend to be at higher risk of adverse outcomes, with increased neonatal morbidity and mortality ( Conde\u2010Agudelo 2000 ), although it is unclear whether this is due to pre\u2010existing vulnerabilities (perhaps also the factors that caused the initial breech presentation), or the effects of delivery in this position. The interpretation of observational studies that compare outcomes after vaginal breech birth and cephalic birth is confounded by the fact that breech presentation per se appears to be a marker for poor perinatal outcome. For example, the incidence of childhood handicap among singleton breech babies, born at term, has been found to be high (19.4%) and similar for those delivered following trial of labour and those following an elective caesarean section ( Danielian 1996 ). Thus, poor outcomes following vaginal breech birth may be the result of underlying conditions causing breech presentation rather than damage during delivery. However, the care during labour, the delivery methods used, and skill of the birth attendant may also influence outcome. Description of the intervention There is concern that vaginal delivery for babies in the breech position increases the risks of compression of the umbilical cord causing oxygen deprivation and distress, cord prolapse, head entrapment, rapid decompression of the head, spinal cord injuries, and other birth trauma. Delivering a breech baby by caesarean section avoids these potential complications and may result in fewer poor outcomes for infants. However, it carries risks for the mother during delivery, in the postoperative recovery, and in future pregnancies, e.g. repeat caesarean section, risk of ruptured scars, placental invasion of the uterus and hysterectomy ( Lawson 2012 ). The routine use of caesarean section for breech presentation became widespread prior to evidence from randomised trials that the benefits of such a policy outweighed the risks. As caesarean section has increased for breech delivery, the skills for vaginal breech delivery have become scarcer, and more birth attendants have become inexperienced at vaginal breech delivery. In a review of two randomised trials and seven cohort studies, the risk difference between trial of labour and planned caesarean section for any perinatal injury or death was 1.1% ( Gifford 1995 ), findings similar to a previous review ( Cheng 1993 ). An observational prospective study with an intent\u2010to\u2010treat analysis was conducted in France and Belgium in 174 maternity units where vaginal breech birth is commonly practised ( Goffinet 2006 ). The study included 8105 pregnant women delivering singleton term breech babies. Multivariate analysis was used to control for confounding variables. The composite outcome fetal and neonatal mortality and severe neonatal morbidity was low in both groups. In the planned vaginal delivery group it was 1.60%; (95% confidence interval (CI) 1.14 to 2.17). This was not significantly different from that in the planned caesarean delivery group (unadjusted odds ratio = 1.10, 95% CI 0.75 to 1.6; adjusted odds ratio = 1.40, 95% CI 0.89 to 2.23). The authors concluded that \"where planned vaginal delivery is a common practice and when strict criteria are met before and during labour, planned vaginal delivery of singleton fetuses in breech presentation at term remains a safe option\". A large observational study in Canada found increased perinatal mortality and morbidity following vaginal birth or caesarean section during labour than following carsarean section without labour ( Lyons 2015 ). However, cohort studies are fundamentally flawed by the fact that factors which influence the choice of method of delivery may have more to do with the outcome for the baby than the method of delivery. Why it is important to do this review Information from randomised trials is required to determine whether benefits (if any) of routine caesarean section for the infant are sufficient to justify subjecting mothers to the increased current and future risks of caesarean section. Attention should be paid to the selection criteria for allowing a trial of labour and the skill and experience of the clinician at delivery.",
        "summary": "Planned cesarean section for women with breech pregnancies may reduce peri/neonatal mortality, but may be more frequently associated with short\u2010term maternal morbidity, compared with planned vaginal birth. None of the evidence was high quality, making it difficult to draw firm conclusions. Three trials with 2388 pregnant women showed that perinatal/neonatal mortality (excluding fatal malformations) was lower with planned cesarean section compared with planned vaginal birth (on average 2 versus 6 deaths per 1000 women); event rates were very low. There was moderate\u2010quality evidence that in countries with a low perinatal mortality rate (\u2264 20 per 1000 births), perinatal/neonatal death or severe neonatal morbidity was lower with planned cesarean section (4 versus 57 deaths per 1000 women). This benefit was not evident overall, or in countries with a high perinatal mortality rate. In terms of short\u2010term maternal morbidity, this was more frequently associated with planned cesarean section (111 versus 86 per 1000 women), but the evidence was low quality. The trials observed no difference between planned cesarean and vaginal births in terms of five minute Apgar score of < 7, brachial plexus injury or birth trauma (low\u2010quality evidence)."
    },
    "CD004690": {
        "query": "How does cognitive\u2010behavioral therapy (CBT) compare with wait\u2010list control and alternative psychological therapies for children and adolescents with anxiety disorders?",
        "document": "Background Description of the condition Anxiety disorders are amongst the most common psychiatric disorders, occurring in 5% to 19% of all children and adolescents ( Costello 2004 ). In children younger than 12, prevalence varies between 2.6% and 5.2%, and separation anxiety is the most common disorder ( Costello 2004 ; Ford 2003 ). One of the diagnostic challenges in this age group involves distinguishing normal, developmentally appropriate worries, fears and shyness from anxiety disorders. Distinguishing features of pathological anxiety include severity, persistence and associated impairment. An understanding of the developmental patterns of various anxieties is also important. School\u2010age children commonly have worries about injury and natural events, whereas older children and adolescents typically have worries and fears related to school performance, social competence and health issues. The presentation of anxiety disorders varies with age. Young children can present with undifferentiated worries and fears and multiple somatic complaints \u2010 muscle tension, headache or stomachache \u2010 and sometimes angry outbursts. The latter may be misdiagnosed as oppositional defiant disorder (ODD), as the child tries to avoid anxiety\u2010provoking situations. Social anxiety disorder typically presents after puberty. Anxiety disorders with an onset in childhood often persist into adolescence ( Last 1996 ) and early adulthood ( Last 1997 ), and yet they often remain untreated, with many cases of social phobia (SOP) first diagnosed more than 20 years after onset ( Schneier 1992 ). The International Classification of Diseases (ICD) and Diagnostic and Statistical Manual (DSM) diagnostic systems distinguish various types of anxiety disorders, including generalized anxiety disorder (GAD), panic disorder (PD), social anxiety disorder (SAD) and specific phobias. These anxiety disorders are often associated with significant impairment in personal, social and academic functioning ( Pine 2009 ). Comorbidities are common and include depression ( Kovacs 1989 ), substance abuse ( Kushner 1990 ) and subsequent adolescent anxiety disorders, social phobia (SOP), attention\u2010deficit/hyperactivity disorder (ADHD), conduct disorder ( Bittner 2007 ), suicidal behaviours and suicide ( Hill 2011 ), Social anxiety disorder (SAD) which peaks in adolescence, is associated with more malignant depression later on ( Beesdo 2007 ). It is clear that anxiety disorders in this age group present serious health issues; therefore effective and readily accessible treatments are needed. Description of the intervention Current treatments for anxiety disorders in this age group include behavioural therapy, particularly for simple phobias, cognitive behavioural therapy (CBT) and/or medication. No specific guidelines on the indications for psychological treatment versus medication have been put forth, although given the prevalence of these disorders, the age of onset and public views on the acceptability of psychological treatments, these are often preferred as first\u2010line therapy. CBT is a collaborative psychological treatment that can be delivered in various formats\uff0dindividual, child or adolescent, group, parents or family. CBT for anxiety disorders in children and adolescents usually takes nine to twenty sessions. One of the first manualised CBT programmes was Coping Cat ( Kendall 1994 ), which consisted of education, modification of negative cognitions, exposure, social competence training, coping behaviour and self\u2010reinforcement sessions. Others have followed, including the Cool Kids programme (http://education.qld.gov.au/studentservices/protection/sel/cool\u2010kids.html), the Coping Koala programme ( Barrett 1996 ), Skills for Academic and Social Success (SASS) ( Masia\u2010Warner 2007 ), ACTION ( Waters 2009 ), Intervention With Adolescents With Social Phobia (IAFS) ( S\u00e1nchez\u2010Garc\u00eda 2009 ), the TAPS ( Masia\u2010Warner 2011 ) and Building Confidence programme ( Galla 2012 ). Some programmes have been specifically adapted for children with autism spectrum disorders (ASDs), including the Multimodal Anxiety and Social Skills Intervention (MASSI) programme ( White 2012 ), TAFF ( Schneider 2011 ) and Facing Your Fears (FYF) ( Reaven 2012 ). How the intervention might work CBT for anxiety disorders in children and adolescents involves helping the child to (1) recognise anxious feelings and bodily or somatic reactions to anxiety, (2) clarify thoughts or cognitions in anxiety\u2010provoking situations (e.g. unrealistic or negative attributions and expectations), (3) develop coping skills (e.g. modifying anxious self\u2010talk into coping self\u2010talk) and (4) evaluate outcomes. Behavioural training strategies include modelling, reality exposure (in vivo exposure), role playing and relaxation training. Behavioural treatment is based on the premise that fear and anxiety are learnt responses (classically conditioned) that can be \"unlearnt\". A key CBT procedure is exposure ( Silverman 1996 ). An element of treatment known as systematic desensitisation involves pairing anxiety stimuli, in vivo or by imagination, in a gradually increasing hierarchy with competing relaxing stimuli such as pleasant images and muscle relaxation. Cognitive strategies are used, the most common of which are self\u2010control strategies which rely on self\u2010observation, self\u2010modification, self\u2010evaluation and self\u2010reward. For instance, according to the STOP acronym ( Silverman 1996 ), children and adolescents learn first to identify when they are feeling anxious or Scared (S), then to identify their anxious Thoughts (T). Next, they learn to modify or restructure their anxious thoughts by generating Other alternative coping thoughts and behaviours (O). Finally, they learn to reward or Praise themselves for confronting their fears (P). CBT has been adapted to include family and parents. The main aspects of CBT parent/family treatment guidelines involve (1) modifying parents\u2019 beliefs about ways to help their anxious child and assisting parents to respond appropriately to the child\u2019s anxious and avoidant behaviours, and (2) assisting parents to manage their own anxiety. CBT can be applied only after the child has reached a certain level of cognitive development. Kendall ( Kendall 1990b ) argued that the ability to measure a thought or belief against the notion of a rational standard and the ability to understand that a thought or belief can cause a person to behave and feel in a certain way were central to its proper use. The question arises: At what age does a child have the cognitive capacities to undertake these cognitive operations? A recent study ( Hirshfeld\u2010Becker 2010 ) reported positive effects in children younger than age 6; however, it is not clear whether children younger than 6 years of age are able to use de\u2010centring techniques such as narratives or stories. In line with this, recent work suggests that young children may be more responsive to the behavioural than the cognitive elements of this approach ( Essau 2004 ). Recent work indicates that treatment of anxiety disorders in very young children may be effected by working directly with parents alone ( Cartwright\u2010Hatton 2011 ). Why it is important to do this review Anxiety disorders in children and adolescents represent a considerable source of morbidity and are associated with later adult psychopathology. However, despite high prevalence and substantial morbidity, anxiety disorders in childhood remain under\u2010recognised and under\u2010treated ( Esbj\u00f8rn 2010 ) and as such represent an important public health issue. The evidence base for treatment of anxiety in children and adolescents is growing. Initial trials of CBT ( Kendall 1994 ; Barrett 1996 ; Kendall 1997 ) were positive ( Kendall 1997 ), and further randomised controlled trials (RCTs) and indeed reviews followed. What is needed now is synthesis of the growing body of evidence. This is an update of a previous Cochrane review of CBT for anxiety disorders in children and adolescents ( James 2005 ), which found a positive response rate for the remission of anxiety diagnoses for CBT in 56% of cases. The current review was undertaken to provide comprehensive and up\u2010to\u2010date evidence on the efficacy of CBT in the treatment of anxiety disorders in children and adolescents, including the use of differing CBT formats\uff0dindividual, group and family/parent. Further, this review will examine the efficacy of CBT relative to active treatments, such as educational support and treatment as usual (TAU). The question of the comparative efficacy of medication versus CBT and the combination of CBT and medication needs to be addressed. Last, this review will aim to assess whether treatment effects of CBT are maintained at long\u2010term follow\u2010up. It is recognised that children and adolescents with autism spectrum disorders (ASDs) have high rates of anxiety disorders, particularly SOP ( Settipani 2012 ); however, a recent review of CBT for anxiety disorders in ASD ( Lang 2010 ) included only a few studies. Furthermore, it is unclear how anxiety disorders are recognised or, indeed, treated in those with intellectual impairments, indicating a pressing need for work in this particular area. Review of the efficacy of CBT in children and adolescents with ASD will be undertaken. Recognition of the importance of anxiety disorders in childhood is increasing, as can be seen in several reviews on the treatment of anxiety in children and adolescents, including Cochrane reviews for post\u2010traumatic stress disorder (PTSD) ( Bisson 2007 ), obsessive\u2010compulsive disorder (OCD) ( O'Kearney 2006 ) and pharmacological treatments ( Ipser 2009 ). These disorders will not be included in this review.",
        "summary": "CBT may be beneficial in terms of reducing anxiety diagnosis and symptoms in children and adolescents (aged 4 to 18 years), but RCT evidence was of insufficient quantity or quality to draw any firm conclusions . There was moderate\u2010certainty evidence of higher rates of remission of anxiety diagnosis and low\u2010certainty evidence of a reduction in anxiety symptoms in children having CBT compared with wait\u2010list control (WL)/no treatment. RCT evidence was insufficient to determine whether the remission of anxiety diagnosis and the reduction in symptoms were maintained in the longer term. Moderate\u2010certainty evidence showed no apparent difference in anxiety or symptom remission when CBT was compared with non\u2010CBT active psychological group interventions. However, there was a suggestion that anxiety diagnosis could be lower in the longer term with CBT. These analyses may have been underpowered, making reliability of the results uncertain. When CBT and treatment as usual were compared, moderate\u2010certainty evidence showed no apparent difference in remission of diagnosis or reduction in symptoms. There was no apparent difference in acceptability, measured as withdrawals from treatment, between CBT and WL, or between non\u2010CBT active psychological group interventions and treatment as usual, but the evidence was considered low certainty. Reviewers did not assess adverse events."
    },
    "CD010685": {
        "query": "How does post consolidation retinoic acid affect outcomes in patients with high\u2010risk neuroblastoma treated with autologous hematopoietic stem cell transplantation?",
        "document": "Background Description of the condition Neuroblastoma is a rare malignant disease and mainly affects infants and very young children ( GARD 2017 ). Tumours develop in the sympathetic nervous system such as adrenal medullary tissue or paraspinal ganglia, and may be localised or metastatic at diagnosis ( Cole 2012 ). The median age at diagnosis is 17 months and the incidence rate of neuroblastoma is age\u2010dependent, with an incidence rate of 64 per million children in the first year of life reducing to 29 per million children in the second year of life ( Goodman 2012 ). The incidence rate in adults is less than one per million a year, but adults have a considerably worse prognosis ( Esiashvili 2007 ). Cohn 2009 proposed the International Neuroblastoma Risk Group (INRG) classification system (see Table 1 ). Of 8800 people with neuroblastoma, 36.1% had high\u2010risk neuroblastoma. Berthold 2005 estimated that 50% of people with neuroblastoma have metastatic disease at diagnosis. Patients in the high\u2010risk group had a five\u2010year event\u2010free survival rate (defined as the time from diagnosis until the time of first occurrence of relapse, progression, secondary malignancy, or death, or until the time of last contact if none of these occurred) of less than 50%. Matthay 2012 addressed new approaches with targeted therapy that may improve the outcome in people with high\u2010risk neuroblastoma. INRG stage Age (months) Histologic category Grade of tumour differentiation MYCN 11q aberration Ploidy Pretreatment risk group Code Interpretation L1/L2 \u2013 Ganglioneuroma maturing; ganglioneuroblastoma intermixed \u2010 \u2013 \u2013 \u2013 A Very low L1 \u2013 Any, except ganglioneuroma or ganglioneuroblastoma \u2013 Not amplified \u2013 \u2013 B Very low Amplified \u2013 \u2013 K High L2 < 18 Any, except ganglioneuroma or ganglioneuroblastoma \u2013 Not amplified No \u2013 D Low Yes \u2013 G Intermediate \u2265 18 Ganglioneuroblastoma nodular; neuroblastoma Differentiating Not amplified No \u2013 E Low Yes \u2013 H Intermediate Poorly differentiated or undifferentiated Not amplified \u2013 \u2013 H Intermediate \u2013 Amplified \u2013 \u2013 N High M < 18 \u2013 \u2013 Not amplified \u2013 Hyperdiploid F Low < 12 \u2013 \u2013 Not amplified \u2013 Diploid I Intermediate 12 to < 18 \u2013 \u2013 Not amplified \u2013 Diploid J Intermediate < 18 \u2013 \u2013 Amplified \u2013 \u2013 O High \u2265 18 \u2013 \u2013 \u2013 \u2013 \u2013 P High MS < 18 \u2013 \u2013 Not amplified No \u2013 C Very low Yes \u2013 Q High Amplified \u2013 \u2013 R High Reference: Cohn 2009 . The INRG consensus classification schema includes the criteria INRG stage, age, histologic category, grade of tumour differentiation, MYCN status, presence/absence of 11q aberrations, and tumour cell ploidy. Sixteen statistically or clinically different pretreatment groups of patients (lettered A through R), or both, were identified using these criteria. The categories were designated as very low (A, B, C), low (D, E, F), intermediate (G, H, I, J), or high (K, N, O, P, Q, R) pretreatment risk subsets. An abdominal mass is the most common presentation of neuroblastoma. In general, neuroblastoma occurs at a single location, usually the medulla of the adrenal gland or along the paravertebral sympathetic chain. Organ\u2010specific symptoms may be caused by the local presence of metastases, such as eye problems associated with retrobulbar tumours, pancytopenia associated with bone marrow infiltration, abdominal distension and respiratory problems associated with liver enlargement, paralysis, and Horner syndrome associated with ganglion involvement ( Berthold 2005 ; NCI PDQ 2017 ). Furthermore, there are general signs and symptoms like tiredness, weakness or pain. Some neuroblastomas regress spontaneously without therapy, while others progress with a fatal outcome despite therapy. One study of infants younger than 12 months showed that nearly half of the study population within three years of follow\u2010up had a spontaneous regression at diagnosis ( Hero 2008 ). A tumour mass may be confirmed by ultrasound, X\u2010rays, computed tomography, or magnetic resonance imaging. Guidelines for using imaging methods have been developed in response to the increased importance of image\u2010defined factors in staging and risk assessment ( Brisse 2011 ). The International Neuroblastoma Staging System provides the current definitions for diagnosis, the stages 1, 2A, 2B, 3, 4, and 4S shown in Table 2 , and treatment response shown in Table 3 ( Brodeur 1993 ). The INRG classification system provides the current definitions for the very low, low, intermediate, and high\u2010risk groups shown in Table 1 ( Cohn 2009 ). Stage Definition 1 Localised tumour with complete gross excision, with or without microscopic residual disease; representative ipsilateral lymph nodes negative for tumour microscopically (nodes attached to and removed with the primary tumour may be positive) 2A Localised tumour with incomplete gross excision; representative ipsilateral nonadherent lymph nodes negative for tumour microscopically 2B Localised tumour with or without complete gross excision, with ipsilateral nonadherent lymph nodes positive for tumour. Enlarged contralateral lymph nodes must be negative microscopically 3 Unresectable unilateral tumour infiltrating across the midline a , with or without regional lymph node involvement; or localised unilateral tumour with contralateral regional lymph node involvement; or midline tumour with bilateral extension by infiltration (unresectable) or by lymph node involvement 4 Any primary tumour with dissemination to distant lymph nodes, bone, bone marrow, liver, skin and/or other organs (except as defined for stage 4S) 4S Localised primary tumour (as defined for stage 1, 2A or 2B), with dissemination limited to skin, liver, and/or bone marrow b (limited to infants < 1 year of age) Reference: Brodeur 1993 . Note: Multifocal primary tumours leg, bilateral adrenal primary tumours should be staged according to the greatest extent of disease, as defined above, and followed by a subscript letter M e.g. 3 M . a The midline is defined as the vertebral column. Tumours originating on one side and crossing the midline must infiltrate to or beyond the opposite side of the vertebral column. b Marrow involvement in stage 4S should be minimal, i.e. < 10% of total nucleated cells identified as malignant on bone marrow biopsy or on marrow aspirate. More extensive marrow involvement would be considered to be stage 4. The (Meta\u2010iodobenzylguanidine) MIBG scan (if performed) should be negative in the marrow. Response Primary tumour Metastatic sites Complete response No tumour No tumour; catecholamines normal Very good partial response Decreased by 90% to 99% No tumour; catecholamines normal; residual 99 Tc bone changes allowed Partial response Decreased by more than 50% All measurable sites decreased by > 50%. Bones and bone marrow: number of positive bone sites decreased by > 50%; no more than 1 positive bone marrow site allowed Minimal response No new lesions; > 50% reduction of any measurable lesion (primary or metastases) with < 50% reduction in any other; < 25% increase in any existing lesion No response No new lesions; < 50% reduction but < 25% increase in any existing lesion Progressive disease Any new lesion; increase of any measurable lesion by > 25%; previous negative marrow positive for tumour Reference: Brodeur 1993 . Description of the intervention Retinoic acid is a derivative of vitamin A (retinol) that includes 13\u2010cis retinoic acid, also known as isotretinoin, among others. Retinoic acid regulates the growth and development of epithelial cells, and inhibits growth of human neuroblastoma cells ( Sidell 1982 ). It reduces morphological signs characteristic of several malignant human neuroblastoma cell lines ( Sidell 1983 ). In a phase I clinical trial, 13\u2010cis retinoic acid was used in children with neuroblastoma after autologous haematopoietic stem cell transplantation (HSCT) without signs of myelosuppression ( Villablanca 1995 ). In a phase III clinical trial, 13\u2010cis retinoic acid was added in people with high\u2010risk neuroblastoma after receiving high\u2010dose chemotherapy (HDCT) followed by autologous HSCT as well as after receiving standard\u2010dose chemotherapy (SDCT) ( Matthay 1999 ). The test intervention of this review is the addition of retinoic acid as part of a therapy that comes after the consolidation therapy, which constitutes HDCT followed by autologous HSCT. Yal\u00e7in 2015 included three randomised controlled trials (RCTs) in a Cochrane Review including the study Matthay 1999 . The objective of the review was to compare the efficacy of HDCT followed by autologous HSCT with standard\u2010dose chemotherapy in children with high\u2010risk neuroblastoma. How the intervention might work Retinoic acid induces the differentiation of human neuroblastoma cell lines and stops uncontrolled cell growth in vitro ( Reynolds 2003 ). Thus, retinoic acid might reduce the relapse rate, which occurs frequently after intensive chemotherapy with or without autologous HSCT in people with high\u2010risk neuroblastoma. Why it is important to do this review People with high\u2010risk neuroblastoma might have improved survival if retinoic acid as part of a postconsolidation therapy were added after HDCT followed by autologous HSCT. However, it is possible that a considerable number of patients may not respond to the addition of retinoic acid. It is therefore important to evaluate the current evidence base for the efficacy and the possible adverse events associated with this treatment. This review is an update of an earlier published Cochrane Review ( Peinemann 2015 ).",
        "summary": "The limited amount of available randomized controlled trial evidence does not allow us to draw any definitive conclusions on retinoic acid consolidation treatment for high\u2010risk neuroblastoma. In patients with high\u2010risk neuroblastoma treated with autologous hematopoietic stem cell transplantation, low\u2010quality evidence suggests that post\u2010consolidation retinoic acid may have no therapeutic effect in terms of either overall survival or event\u2010free survival. Other key outcomes, such as treatment\u2010related mortality, progression\u2010free survival, toxicity and quality of life, were not assessed."
    },
    "CD002314-0": {
        "query": "How do anti\u2010leukotriene agents compare with inhaled corticosteroids in the management of recurrent and/or chronic asthma in children?",
        "document": "Background Asthma is a condition that affects the airways, it is characterised by bronchoconstriction and underlying inflammation. Infiltration of bronchial airways with eosinophils and neutrophils with release of inflammatory mediators is characteristic of asthma ( Murphy 1993 ). The cysteinyl leukotrienes are considered as the most potent inflammatory mediators in asthma. They are produced by the 5\u2010lipoxygenase pathway of the arachidonic acid metabolism. These mediators stimulate the production of airway secretions, cause micro vascular leakage and enhance eosinophilic migration in the airways; thus, leukotrienes are believed to play a pivotal role in mediating bronchoconstriction and inflammatory changes in the pathophysiology of asthma ( Peters\u2010Golden 2007 ). All recent consensus statements on asthma advocate aggressive treatment of airway inflammation ( Australia 2006 ; NAEPP 2007 ; Lougheed 2010 ; GINA 2010 ; BTS 2011 ). Although several drugs such as ketotifen, sodium cromoglycate and sodium nedocromil have anti\u2010inflammatory properties, inhaled glucocorticoids remain the cornerstone of asthma management because of their efficacy, tolerability and rapid onset of action ( Spahn 1996 ). Prolonged low dose administration of inhaled corticosteroids is generally considered safe, although there is considerable concern about the long\u2010term effects of steroids among some consumers ( Elwyn 2010 ). However, when moderate or high doses are required to control symptoms, adverse effects such as growth stunting in children ( Sharek 2000 ; Richard 2006 ), suppression of the adrenal axis ( Bisgaard 1988 ; Phillip 1992 ; Padfield 1993 ; Z\u00f6llner 2007 ), and osteopenia ( Todd 1996 ; Heuck 1997 ) may be observed. Anti\u2010leukotrienes form a class of anti\u2010inflammatory drugs that interfere with leukotriene production (5\u2010lipoxygenase inhibitors) or with leukotriene receptors (leukotriene receptors antagonists, LTRAs). Anti\u2010leukotrienes have the advantage of being administered orally in a single or twice daily dose and importantly, seem to lack the adverse effects on growth, bone mineralization and adrenal axis, associated with long\u2010term or high\u2010dose systemic glucocorticoid therapy. Why it is important to do this review The previous version of this review ( Ducharme 2004 ) summarised the accumulating evidence derived from 25 randomised controlled trials (three paediatric and 22 adult) and concluded that low doses of inhaled corticosteroids were superior in efficacy than anti\u2010leukotrienes. With the publications of several new randomised controlled trials especially in children, an update of the systematic review was deemed useful to review the safety and efficacy of anti\u2010leukotrienes as monotherapy as compared to inhaled corticosteroids and to provide better insight into the influence of patient and treatment characteristics on the magnitude of effects. In addition, several national guidelines currently advocate their use as second choice monotherapy after inhaled corticosteroids in patients with mild asthma and as adjunct therapy with inhaled corticosteroids as an alternative of combination of long acting \u03b2 2 \u2010agonist and inhaled corticosteroids in patients with moderate asthma ( Australia 2006 ; NAEPP 2007 ; Lougheed 2010 ; GINA 2010 ; BTS 2011 ).",
        "summary": "In children with mild to moderate chronic asthma the randomized controlled trial evidence favors the use of inhaled corticosteroids over anti\u2010leukotriene agents as monotherapy. Montelukast seemed less effective than inhaled corticosteroid monotherapy (between 100\u2010500 mcg HFA\u2010BDP equivalent) in reducing the number of exacerbations needing systemic corticosteroids, although the difference between groups did not quite reach statistical significance. There seemed to be no difference between groups in hospital admission due to exacerbation of asthma, change in FEV 1 , number of days off school or overall adverse effects. However the number of symptom free days over 36\u201052 weeks was higher for those children taking inhaled corticosteroid monotherapy. Given the difficulties in performing accurate and reproducible lung function on children, this long term clinical outcome is considered a particularly important measure of asthma control, supporting the use of inhaled corticosteroids over anti\u2010leukotriene agents as monotherapy for children with mild to moderate asthma."
    },
    "CD011792": {
        "query": "What are the benefits and harms of treatment for prevention of sudden unexpected death in epilepsy (SUDEP)?",
        "document": "Background Description of the condition Sudden Unexpected Death in Epilepsy (SUDEP) is defined as sudden, unexpected, witnessed or unwitnessed, non\u2010traumatic or non\u2010drowning death of people with epilepsy, with or without evidence of a seizure, excluding documented status epilepticus, and in whom postmortem examination does not reveal a structural or toxicological cause for death. SUDEP has a reported incidence of 1 to 2 per 1000 patient years and represents the most common epilepsy\u2010related cause of death ( Shorvon 2011 ). Other deaths related to epilepsy include non\u2010recovery from status epilepticus, seizure\u2010related accidents, and suicide. SUDEP claims the lives of young adults, causing devastation to families. Its prevention is of paramount importance. Discussions around SUDEP are often difficult for patients and health professionals, although patients should be given and have access to information on SUDEP. Evidence from a UK\u2010based study suggests that SUDEP is not commonly discussed with patients and families ( Morton 2006 ). The highest SUDEP incidence reported in patients undergoing presurgical evaluation or having previously failed surgery was 9.3 per 1000 patients years ( Dasheiff 1991 ). A pooled meta\u2010analysis of four case\u2010control studies ascertained that the presence and frequency of generalised tonic\u2010clonic seizures (GTCS) was the strongest predictor of risk of SUDEP. For example, compared with patients with no GTCS, patients having three or more GTCS per month had an odds ratio greater than 15. Male gender, age of onset of epilepsy of under 16 years old, duration of epilepsy for over 15 years, and polytherapy are also significant predictors of risk for SUDEP ( Hesdorffer 2011 ; Hesdorffer 2012 ). The exact pathophysiology of SUDEP is currently unknown, although GTCS\u2010induced cardiac, respiratory, and brainstem dysfunction appears likely ( Langan 2000 ). Information has been gleaned from rare monitored cases of SUDEP, and demonstrates severe postictal electroencephalogram (EEG) suppression. In the retrospective MORTality in Epilepsy Monitoring Unit Study (MORTEMUS) of 16 SUDEP cases and nine near\u2010SUDEP cases, available cardiorespiratory data were analysed in 10 monitored cases of SUDEP. The study ascertained a consistent pattern of changes that initially began with rapid breathing (18 to 50 breaths per minute) immediately following a GTCS and followed within three minutes by postictal generalised EEG suppression, terminal apnoea, and severe bradycardia with subsequent cardiac arrest. In this study the cardiorespiratory collapse was terminal in one\u2010third of patients. In the remaining two\u2010thirds, there appeared to be a transient restoration of cardiac function but with abnormal respiratory effort potentially aggravated by the prone position, leading eventually to terminal apnoea and terminal asystole. There were less consistent patterns of cardiorespiratory change in five of nine monitored near\u2010SUDEP cases. In these cases, postictal apnoea followed by asystole, ictal asystole, ventricular fibrillation, and postictal cardiorespiratory arrest were observed. The study highlighted a potential window for lifesaving intervention, in that those patients who received cardiopulmonary resuscitation within three minutes (seven near\u2010SUDEP cases) were successfully resuscitated. The study suggested improved supervision within epilepsy\u2010monitoring units, particularly nocturnal supervision ( Ryvlin 2013a ). Ictal hypoxia, dysfunction in subcortical and brainstem networks controlling electrogenesis, and the release of adenosine and endogenous opioids within the brain and brainstem following a GTCS have been proposed as mechanisms to explain the above clinical findings ( Nashef 2009 ). Seizure\u2010related arrhythmias were originally proposed as a mechanism for SUDEP, but many believe this to be a self limiting process ( Schuele 2010 ). However, there are cases of death of patients exhibiting rare sodium and potassium channelopathies, although the exact circumstances of death are uncertain ( Tu 2011 ). Appropriately chosen antiepileptic drug treatment can render around 70% of people with epilepsy free of all seizures. However, around one\u2010third of people with epilepsy will remain drug refractory despite polytherapy. Continuing seizures place patients at risk of SUDEP, depression, and reduced quality of life. Preventative strategies for SUDEP are therefore of paramount importance. Description of the intervention Experts within the field of SUDEP believe it results from a multifactorial neurovegetative breakdown. Thus any preventative intervention must target contributing factors ( Ryvlin 2013b ). These include: reducing the occurrence of GTCS by timely referral for presurgical evaluation in people with lesional epilepsy and advice on lifestyle measures (e.g. concordance with optimal anti\u2010epileptic drug treatment); enhancing the ability to detect cardiorespiratory distress through clinical observation and seizure, respiratory, and heart rate monitoring devices; preventing airway obstruction through nocturnal supervision and safety pillows; reducing central hypoventilation through physical stimulation and enhancing serotonergic mechanisms of respiratory regulation using selective serotonin reuptake inhibitors (SSRIs); reducing adenosine and endogenous opioid\u2010induced brain and brainstem depression. reducing the occurrence of GTCS by timely referral for presurgical evaluation in people with lesional epilepsy and advice on lifestyle measures (e.g. concordance with optimal anti\u2010epileptic drug treatment); enhancing the ability to detect cardiorespiratory distress through clinical observation and seizure, respiratory, and heart rate monitoring devices; preventing airway obstruction through nocturnal supervision and safety pillows; reducing central hypoventilation through physical stimulation and enhancing serotonergic mechanisms of respiratory regulation using selective serotonin reuptake inhibitors (SSRIs); reducing adenosine and endogenous opioid\u2010induced brain and brainstem depression. How the intervention might work Patients with a surgical epileptogenic target should be referred for presurgical evaluation since targeted surgery may improve the likelihood of successful seizure control. Whilst studies report a greater risk of SUDEP in patients who have failed surgery, it is unclear whether the surgery itself is preventative for SUDEP or whether risk differences relate to the underlying pathologic process ( Ryvlin 2006 ). Patient education is important in ensuring compliance with treatment and should include discussion of lifestyle factors that may adversely affect seizure control and drug effectiveness (e.g. sleep deprivation, stress, and excess alcohol). Similarly a care plan for seizure clusters and discussions around contraception, pregnancy, gastrointestinal disorders, and any other medications that may affect anti\u2010epileptic drug treatment are all important discussion points. This may be delivered through formal educational programmes, advice leaflets, or verbal consultation. Seizure\u2010monitoring devices, which include bed sensors, fall alarms, and tracking devices, range from technology worn at the wrist that detects seizures via changes in vibrations to more sophisticated multimodal systems monitoring movements and vital signs, including heart rate, via a sensitive bed mat. These systems alert a carer or parent to potential seizure activity, which in turn may prevent SUDEP. Safety pillows, which have small holes, are thought to reduce postictal respiratory distress and thus SUDEP when the person is face down on the pillow in a prone position ( Devinsky 2012 ). One retrospective study reported that around 71% of people who died of SUDEP were in the prone position, suggesting position may play a significant role in the condition ( Kloster 1999 ). People in postictal hypoxic coma will not be able to correct their position and are thus at risk of respiratory failure. The utility of oxygen therapy following a seizure is used in most epilepsy monitoring units and has been shown to prevent SUDEP in a mouse model of seizure\u2010induced SUDEP by reducing the risk of respiratory distress ( Venit 2004 ). Nocturnal supervision would allow turning of the person from a prone to a recovery position, reducing the risk of respiratory distress and reducing central hypoventilation. One case\u2010control study found that nocturnal supervision was protective against SUDEP ( Langan 2005 ). Serotonergic nuclei within the lower brainstem play an important role in regulating respiratory function, particularly in the context of recurrent hypoxia. Abnormalities of these nuclei are documented in sudden infant death and also in mice models of SUDEP ( Uteshev 2010 ). The SSRI fluoxetine has been shown to prevent apnoea in these mice models. A clinical retrospective study ascertained that people undergoing videotelemetry and taking an SSRI were significantly less likely to have ictal/postictal hypoxia than those not taking an SSRI ( Bateman 2010 ). Another study observed the effect of peri\u2010ictal nursing interventions (supplemental oxygen, oropharyngeal suction, and patient repositioning) on respiratory dysfunction and postictal EEG suppression. It showed a reduced duration of hypoxemia and EEG suppression with early peri\u2010ictal interventions ( Seyal 2013 ). Inhibitors of adenosine and opiate substances may prevent SUDEP by reducing the severity of postictal EEG depression and brainstem dysfunction. Caffeine, an antagonist of adenosine receptors, is potentially proconvulsant. However, naloxone, an opiate receptor antagonist, has not been demonstrated to have a proconvulsant effect and thus may have a use in preventing SUDEP. Patients with a surgical epileptogenic target should be referred for presurgical evaluation since targeted surgery may improve the likelihood of successful seizure control. Whilst studies report a greater risk of SUDEP in patients who have failed surgery, it is unclear whether the surgery itself is preventative for SUDEP or whether risk differences relate to the underlying pathologic process ( Ryvlin 2006 ). Patient education is important in ensuring compliance with treatment and should include discussion of lifestyle factors that may adversely affect seizure control and drug effectiveness (e.g. sleep deprivation, stress, and excess alcohol). Similarly a care plan for seizure clusters and discussions around contraception, pregnancy, gastrointestinal disorders, and any other medications that may affect anti\u2010epileptic drug treatment are all important discussion points. This may be delivered through formal educational programmes, advice leaflets, or verbal consultation. Seizure\u2010monitoring devices, which include bed sensors, fall alarms, and tracking devices, range from technology worn at the wrist that detects seizures via changes in vibrations to more sophisticated multimodal systems monitoring movements and vital signs, including heart rate, via a sensitive bed mat. These systems alert a carer or parent to potential seizure activity, which in turn may prevent SUDEP. Safety pillows, which have small holes, are thought to reduce postictal respiratory distress and thus SUDEP when the person is face down on the pillow in a prone position ( Devinsky 2012 ). One retrospective study reported that around 71% of people who died of SUDEP were in the prone position, suggesting position may play a significant role in the condition ( Kloster 1999 ). People in postictal hypoxic coma will not be able to correct their position and are thus at risk of respiratory failure. The utility of oxygen therapy following a seizure is used in most epilepsy monitoring units and has been shown to prevent SUDEP in a mouse model of seizure\u2010induced SUDEP by reducing the risk of respiratory distress ( Venit 2004 ). Nocturnal supervision would allow turning of the person from a prone to a recovery position, reducing the risk of respiratory distress and reducing central hypoventilation. One case\u2010control study found that nocturnal supervision was protective against SUDEP ( Langan 2005 ). Serotonergic nuclei within the lower brainstem play an important role in regulating respiratory function, particularly in the context of recurrent hypoxia. Abnormalities of these nuclei are documented in sudden infant death and also in mice models of SUDEP ( Uteshev 2010 ). The SSRI fluoxetine has been shown to prevent apnoea in these mice models. A clinical retrospective study ascertained that people undergoing videotelemetry and taking an SSRI were significantly less likely to have ictal/postictal hypoxia than those not taking an SSRI ( Bateman 2010 ). Another study observed the effect of peri\u2010ictal nursing interventions (supplemental oxygen, oropharyngeal suction, and patient repositioning) on respiratory dysfunction and postictal EEG suppression. It showed a reduced duration of hypoxemia and EEG suppression with early peri\u2010ictal interventions ( Seyal 2013 ). Inhibitors of adenosine and opiate substances may prevent SUDEP by reducing the severity of postictal EEG depression and brainstem dysfunction. Caffeine, an antagonist of adenosine receptors, is potentially proconvulsant. However, naloxone, an opiate receptor antagonist, has not been demonstrated to have a proconvulsant effect and thus may have a use in preventing SUDEP. Why it is important to do this review SUDEP has devastating consequences for patients and families. Various devices and interventions are available to people with epilepsy that are thought to reduce the risk of SUDEP. However, no robust evidence is available to confirm this preventative effect. A systematic review of the literature base will inform patients and healthcare professionals on treatment policy and prevention of SUDEP.",
        "summary": "An observational study identified living people with epilepsy who matched (by age and geographical region) adolescents and adults with a postmortem diagnosis of sudden unexpected death in epilepsy (SUDEP). Use of an intervention to prevent SUDEP was determined for each participant who died (before entry into the study or subsequently). Fewer people died when an intervention to reduce SUDEP was employed (on average, 213 fewer deaths per 1000 people with nocturnal supervision and 184 fewer deaths per 1000 people with special precautions). However, data were available for only 367 of the 616 live participants recruited, and evidence was of very low quality, making firm conclusions impossible."
    },
    "CD001970": {
        "query": "How does delaying introduction of progressive enteral feeds affect outcomes in very preterm or very low birth weight infants?",
        "document": "Background Description of the condition Necrotising enterocolitis (NEC) is an important cause of morbidity, mortality and neuro\u2010disability in very preterm (less than 32 weeks' gestation) or very low birth weight (VLBW: less than 1500 g) infants. Extremely low birth weight (ELBW: less than 1000 g) and extremely preterm (less than 28 weeks' gestation) infants are at greatest risk ( Bisquera 2002 ; Holman 2006 ; Rees 2007 ; Berrington 2012 ). Intrauterine growth restriction may be an additional specific risk factor, especially if associated with circulatory redistribution demonstrated by absent or reversed end\u2010diastolic flow velocities in antenatal Doppler studies of the fetal aorta or umbilical artery ( Bernstein 2000 ; Garite 2004 ; Dorling 2005 ; Kamoji 2008 ). Description of the intervention Most very preterm or VLBW infants who develop NEC have received enteral milk feeds. Evidence exists that feeding with artificial formula rather than human milk increases the risk of developing NEC ( Quigley 2014 ). The timing of the introduction and the rate of progression of enteral feed volumes may also be modifiable risk factors for the development of NEC ( Brown 1978 ; Uauy 1991 ; Henderson 2009 ). Data from observational studies suggest that using feeding regimens that include delaying the introduction of progressive enteral feeds until beyond about four to seven days after birth reduces the risk of NEC ( Patole 2005 ; Hay 2008 ). Why it is important to do this review In current clinical practice, the introduction of progressive enteral feeds for very preterm or VLBW infants is often preceded by a period of enteral fasting or 'minimal enteral nutrition' ( Boyle 2004 ; Patole 2004 ; Hay 2008 ; Klingenberg 2012 ). However, there may also be potential disadvantages associated with delaying the introduction of progressive enteral feeds. Because gastrointestinal hormone secretion and motility are stimulated by enteral milk, delayed enteral feeding could diminish the functional adaptation of the gastrointestinal tract ( Berseth 1990 ; Burrin 2002 ). Prolonging the duration of use of parenteral nutrition may be associated with infectious and metabolic complications that increase mortality and morbidity, prolong hospital stay, and adversely affect growth and development ( Flidel\u2010Rimon 2004 ; Stoll 2004 ). It has been argued that the risk of NEC should not be considered in isolation of these other potential clinical outcomes when determining feeding policies and practice for very preterm or VLBW infants ( Flidel\u2010Rimon 2006 ; Hay 2008 ; Hartel 2009 ). This review focused on the comparison of delayed versus earlier introduction of progressive enteral feeding; that is, advancing the volume of milk feeds beyond minimal enteral nutrition levels. We addressed the effect of minimal enteral nutrition, the early introduction of small volume enteral feeds (up to 24 mL/kg/day) without advancing the feed volumes for at least five days versus enteral fasting in another Cochrane review ( Morgan 2013a ).",
        "summary": "Randomized controlled trials including around 1000 infants, most of whom were very preterm (28\u201032 weeks) or very low birth weight (< 2000 g) found that delaying the introduction of feeds beyond four days after birth did not decrease the risk of developing necrotizing enterocolitis or reduce mortality prior to discharge from hospital. More limited evidence from studies including around 200\u2010400 infants suggested that there may also be no difference between delayed and early feed introduction in growth, feed intolerance or invasive infection. Delaying the introduction of progressive enteral feeds increased the time taken to reach full feeds by 2\u20104 days; however it is unclear what effect this has on infant outcomes. Insufficient data are currently available to determine effects of delayed versus early introduction of feeds in subgroups such as mostly formula fed, mostly human milk fed, and extremely preterm/low birth weight infants. Long\u2010term growth parameters and neurodevelopmental outcomes were not assessed by any of the trials."
    },
    "CD009009": {
        "query": "What are the effects of general health checks for reducing morbidity and mortality from disease in adults?",
        "document": "Background Description of the condition General health checks are common elements of health care in some countries ( Han 1997 ; Holland 2009 ), sometimes as systematic national programmes ( Nakao 2018 ; Robson 2016 ). The evolution of medicine in the latter half of the 20th century has led to a great increase in diagnostic methods and increased expectations that many diseases can be prevented or discovered before there is irreversible damage. Description of the intervention General health checks involve a contact between a health professional and a person that is not motivated by symptoms, and where several screening tests are performed to assess general health. The purpose is to prevent future illness through earlier detection of disease or risk factors, or to provide reassurance. The terminology is confusing. Multiphasic screening, periodic health examination, annual physicals, and preventive health checks are examples of terms used to describe the intervention. Some studies investigated the effect of a single health check and some examined the effect of consecutive checks, and the diagnostic tests included vary considerably. We use the broad term 'general health check', which is frequently used by lay people and in advertising. Few of the screening tests commonly included in general health checks have been evaluated according to accepted criteria, that is, in high\u2010quality randomised trials ( UK National Screening Committee 2010 ). Whilst the benefits and harms of treatments for conditions such as hypertension and diabetes have been extensively studied in randomised trials, screening asymptomatic people for these conditions has been studied very little ( Piper 2015 ; Selph 2015 ). Assessing cardiovascular risk with a risk score is common in health checks, but it is unclear whether it helps ( Karmali 2017 ). When screening for individual conditions has been studied in randomised trials, the conclusions have varied. For example, screening for prostate cancer likely does not reduce disease\u2010specific mortality but has important harms ( Ilic 2013 ; Martin 2018 ), whereas testing for faecal occult blood reduces colorectal cancer mortality, though at the cost of a large number of invasive examinations in healthy people ( Holme 2013 ). Health checks may be offered systematically to the general population as part of a national policy or private health insurance, or employers may offer them to their employees. They may also be purchased by the individual from commercial providers or provided by general practitioners. Health checks may be quite comprehensive and use advanced technologies, such as computed tomography or magnetic resonance imaging, although these interventions are not recommended for health checks because of unproven benefit and risk of harms ( FDA 2018 ). Some general health checks include a conversation with a health professional, possibly a questionnaire, and sometimes also a physical examination by a doctor. In essence these are screening tests, although a conversation may not be perceived as such. Lifestyle interventions are also frequently administered during a health check, for example, advice on diet and smoking. This is not screening but behavioural intervention, and appears to be of varying value. For example, systematic reviews have not shown a value for multiple risk factor interventions in general populations ( Ebrahim 2011 ). There may be a small effect of modification of dietary fat intake, but the results are not clear ( Hooper 2011 ; Hooper 2015 ). However, simple advice on quitting smoking has been shown to have an effect ( Stead 2013 ). Importantly, primary care physicians sometimes advise health checks or selected screening tests for patients that they think might benefit from them when they see the patients for other reasons. Such clinically motivated testing is often considered an integral part of primary care practice and the effects of systematic health checks are measured as an addition to this practice. How the intervention might work General health checks are expected to reduce morbidity and mortality through earlier detection and treatment of diseases and risk factors for diseases. For example, early detection of hypertension can lead to reductions in morbidity and mortality through treatment. Screening may detect precursors to disease, for example, colorectal adenomas or cervical dysplasia, the treatment of which may prevent cancer from developing. Also, identification of signs or symptoms of manifest disease that the person had not deemed important may be beneficial. Counselling on diet, weight and smoking may also be of value. Healthy people may feel reassured, which could decrease worry. The preventive nature of general health checks implies that most effects would be expected to have a latency of several years. Screening healthy people can also be harmful. While we cannot be certain that screening leads to benefit, all medical interventions can lead to harm. A well\u2010known example is overdiagnosis of latent cancers or carcinoma in situ, which might not have progressed to become symptomatic or might have regressed spontaneously ( Welch 2004 ). Furthermore, false\u2010positive test results can lead to unnecessary invasive diagnostic tests that may cause harm, and drug treatment of people with risk factors such as high cholesterol and elevated blood glucose can have adverse effects. False\u2010positive test results may cause unnecessary worry ( Brewer 2007 ), and false\u2010negative results may lead to a false sense of security and delay medical attention when needed. Further, being labelled as having a disease, or even just as being at increased risk of getting a disease, may negatively affect healthy peoples' views of themselves ( Barger 2006 ; Hamer 2010 ; Haynes 1978 ). It may also make it more difficult to obtain life and health insurance in some countries. Last but not least, there is a financial cost for patients and society in identifying and treating risk factors and diseases that might never have manifested themselves as illness or shortened life. Why it is important to do this review General health checks are mixtures of screening tests, few of which have been adequately studied, and it is not clear whether they do more good than harm. Systematic reviews of health checks have not found effects on morbidity and mortality, but some have found effects on surrogate outcomes such as blood pressure and cholesterol ( Dyakova 2016 ; Krogsb\u00f8ll 2012 ; Si 2014 ). We saw a need for a broad and comprehensive review of the randomised trials, with a focus on clinically important outcomes rather than surrogate outcomes. We chose not to review observational studies because the risk of bias is too great in relation to the expected effect sizes. This is the first update of the review published in 2012 ( Krogsb\u00f8ll 2012 ).",
        "summary": "High\u2010 to moderate\u2010certainty evidence shows that screening for more than one disease or risk factor through health checks does not impact all\u2010cause, cardiovascular, or cancer mortality, nor rates of ischemic heart disease or stroke, over a 4 to 30\u2010year period of follow\u2010up. Regular health checks had no apparent impact on hospitalizations (five trials; 22,651 participants). One large trial (10,713 participants) showed that more people who had health checks rather than no health checks reported that they had a chronic condition (61% vs 54% of people). Effects of health checks on specific types of morbidity (cardiovascular, respiratory, endocrine, cancer\u2010related) were inconsistent across trials and often were self\u2010reported. Therefore, it is difficult to determine the benefits of health screening."
    },
    "CD008350": {
        "query": "How does pneumatic retinopexy compare with scleral buckle for repairing simple rhegmatogenous retinal detachments?",
        "document": "Background Description of the condition A retinal detachment is the separation of the sensory retina from the underlying retinal pigment epithelium ( Sodhi 2008 ). Retinal detachments are classified according to the cause of this separation; the four categories are rhegmatogenous, tractional, combined tractional/rhegmatogenous, and exudative (serous) ( Sodhi 2008 ). In rhegmatogenous retinal detachment (RRD), the primary pathology is a full\u2010thickness break in the sensory retina caused by vitreous traction on the retina. This retinal break allows fluid from the vitreous cavity to enter the subretinal space; RRD is the result of these two factors, along with a liquefied vitreous ( Ghazi 2002 ). RRD is usually treated with surgery, the urgency of which depends on the status of the macula (attached or detached). In tractional retinal detachment, a vitreoretinal membrane generates tractional strain without a full\u2010thickness tear in the retina. In exudative retinal detachment, serous fluid accumulates underneath the sensory retina. Common causes of exudative retinal detachment include inflammatory conditions such as posterior uveitis; therapy involves treatment of the underlying ocular condition. In this review, we considered the surgical interventions for repairing RRD. In the United States, roughly 36,000 cases of RRD occur annually, as estimated from regional studies. Worldwide, the reported incidence rate of RRD varies dramatically in different countries. It was reported as 14 per 100,000 people per year in Sweden ( Algvere 1999 ); 12.6 per 100,000 people per year in Minnesota, United States ( Rowe 1999 ); and 7.98 per 100,000 people per year in Beijing ( Li 2003 ). RRD occurs most commonly in people aged 40 to 70 years with pre\u2010existing or concurrent posterior vitreous detachments that lead to retinal tears. While some studies report a success rate of up to 90% to 95% for surgical reattachment of the retina, as many as 40% of these participants have final visual acuities of 20/50 or lower. The occurrence of RRD in the general population is low (12 per 100,000 people, 0.01% annual risk, 0.06% lifetime risk), but there are several factors that increase the risk of experiencing RRD, including lattice degeneration, extreme myopia, cataract surgery, and ocular trauma or infection ( Sodhi 2008 ). Vitreoretinal traction and underlying weakness in the peripheral retina combine to cause the retinal breaks responsible for RRD. However, not all retinal breaks will in turn cause a retinal detachment ( Byer 1998 ). In fact, there are numerous adhesive forces that can counteract the deleterious effects of retinal breaks and maintain the stability of the vitreous\u2010retina border. For instance, the movement of ions and fluid by retinal pigment epithelium cells, choroid\u2010subretinal oncotic pressure differentials, intraocular pressure\u2010associated hydrostatic forces, and subretinal adhesive\u2010like mucopolysaccharides all can work to offset a retinal break, thus preventing progression to RRD ( Ghazi 2002 ). When these adhesive forces are not sufficient to compensate for vitreoretinal traction, fluid can enter the subretinal space and RRD can occur ( Sodhi 2008 ). People with RRD often have a history of flashing lights, vitreous floaters, or both, caused by an acute posterior vitreous detachment. After a variable period of time, the person may notice a peripheral visual field defect, which may progress to involve central vision ( Gariano 2004 ). However, some people do not experience these premonitory symptoms. In these people, the first sign of RRD can be a black shadow that may or may not affect visual acuity. Involvement of the macula in a RRD, which is a common cause of decreased vision in a retinal detachment, is an important prognostic marker; people without macular involvement will have better visual outcomes. In the clinic, examination of RRD patients may reveal pigmented cells, also known as tobacco dust, in the vitreous and the anterior chamber. Other clinical findings include transparent subretinal fluid and an opaque, furrowed\u2010appearing retina that may ripple with the patient\u2019s eye movements ( Ross 2000 ). Description of the intervention Three separate surgical interventions are used in current clinical practice to repair retinal break(s) in RRD: pneumatic retinopexy, scleral buckle, and vitrectomy. Pneumatic retinopexy may be performed as an outpatient clinical procedure. In this procedure, cryotherapy or laser is applied to the area of the retinal tear and a gas bubble is injected into the vitreous cavity to provide tamponade for the detached retina. Eyes with RRD meeting the following criteria are ideal cases for surgery: single retinal tear less than or equal to one clock hour in size, tear located in the superior half of the retina, and no associated peripheral retinal degeneration. However, eyes not meeting these criteria (e.g., larger breaks, limited lattice degeneration) also may be candidates for pneumatic retinopexy. The scleral buckle procedure involves localizing the position of all retinal breaks, treating all retinal breaks with the cryoprobe, and supporting them with the scleral buckle. The scleral buckle can be positioned radially, in a segmental fashion, or it can encircle the entire eye. Vitrectomy involves operating inside the eye and removing the vitreous to relieve vitreoretinal traction. The retina is reattached by various techniques depending on the location and extent of the detachment. At the conclusion of the vitrectomy, a gas bubble is usually injected into the eye to provide tamponade for the retina to heal (reattach). Scleral buckle surgery can be combined with vitrectomy when the retinal detachment is complex. How the intervention might work Surgical intervention, while of dubious benefit to asymptomatic RRD patients, is the clear course of action for those who experience symptoms; if symptomatic RRD is not treated, the affected eyes will be at risk for involvement of the entire retina and further vision loss. In pneumatic retinopexy, retinal breaks are tamponaded by the intravitreal gas bubble, closed, and sealed by the chorioretinal adhesion induced by cryotherapy. The scleral buckle indents the eye wall, brings the detached retina closer to the eye wall, and relieves vitreoretinal traction. In vitrectomy, vitreous is removed and all of the vitreoretinal traction on any of the breaks excised. The patient's retina is flattened intraoperatively by using a gas bubble. Why it is important to do this review RRD can progress to significant loss of visual acuity. Despite the wide array of surgical interventions available for management of RRD, repair success rates remained stable from 1979 to 1999, and likely to the present ( Minihan 2001 ). This plateau indicates that choosing the most effective surgical approach for people with RRD is crucial. While various studies have proposed different paradigms for management of retinal detachment, few sufficiently powered randomized controlled trials (RCTs) have established any one therapy as clearly superior ( Sodhi 2008 ). In this review we systematically examined the evidence on the effectiveness of pneumatic retinopexy versus scleral buckle as two major surgical treatments for RRD. A separate Cochrane review is under way that compares vitrectomy versus scleral buckle for repairing RRD ( Znaor 2012 ).",
        "summary": "In people with retinal detachment considered good candidates for pneumatic retinopexy, moderate\u2010quality evidence suggested that fewer people may have retina reattachment (on average 729 versus 819 reattachments per 1000 people) and more people recurrent retinal detachment (240 versus 133 recurrent detachments per 1000 people), within six months of pneumatic retinopexy, compared with scleral buckle. However, the numbers of participants in these analyses was small (218 participants), therefore the analyses were underpowered and neither reached statistical significance; whether these would have reached statistical significance with more people, is unclear. A \u2018good candidate\u2019 for retinopexy was defined as a patient who had a single retinal tear \u22641 clock hour in size, tear located in superior half of retina, and absence of proliferative vitreoretinopathy or uncontrolled glaucoma; 56% of eyes were macular\u2010detached. A single trial with 196 participants reported that 71/103 (69%) eyes with pneumatic retinopexy and 50/95 (53%) eyes with scleral buckle had BCVA of 20/40 or better at 6\u2010month follow\u2010up; at 24\u2010month follow\u2010up this was 81/92 (88%) eyes with pneumatic retinopexy and 57/77 (74%) eyes with scleral buckle. Low\u2010quality evidence showed no apparent difference between groups in operative ocular adverse events or proliferative vitreoretinopathy; the numbers of participants in these analyses were too few to detect differences between groups if these were present."
    },
    "CD004778": {
        "query": "What are the benefits and harms of laparoscopic repair in people with perforated peptic ulcer disease?",
        "document": "Background Description of the condition The appearance of laparoscopy in the late 1980s marked a milestone in surgery. Its advantages of diminished pain, surgical wound complications, hospital stay and global costs in uncomplicated cases of gallbladder disease ( Gadacz 2000 ) led to the expansion of its use to other intra\u2010abdominal organs such as the distal oesophagus, the proximal stomach ( Chekan 1999 ; Consensus 1997 ; Horgan 1997 ; Klingler 1999 ) and the colon ( Rickard 2001 ; Tisminezky 2000 ). Most of the early laparoscopic approaches were confined to elective surgery. However, with the improvement of technology and the gaining of experience the laparoscopic approach for acute intra\u2010abdominal pathologies can be applied more widely ( Bergamaschi 2000 ; Pamoukian 2001 ; Sauerland 2004 ). Peptic ulcer perforation is the second most frequent abdominal perforation that requires surgery, following perforated appendicitis. Peptic ulcer is a common disease in the general population. It is estimated that almost 10% of American men will suffer from duodenal ulcer in their lifetime, although its incidence varies within a country ( Paimela 1991 ) as it is more frequent in men and the incidence increases with age. Peptic ulcer disease has been associated with many etiological factors such as Helicobacter pylori infection, non\u2010steroidal anti\u2010inflammatory drug (NSAID) use, stress, cigarette smoking, diet and genetics but multifactorial hypotheses are widely accepted. Complications of peptic ulcer include bleeding, obstruction and perforation and they are still treated by general surgeons. Elective surgery for peptic ulcer disease has decreased significantly over the years due to the introduction of effective medical therapies, first with histamine type 2 (H2)\u2010receptor antagonists and more recently with proton pump inhibitors. However, the principal complications of perforation and hemorrhage remain indications for surgery. ( Paimela 1991 ; Svanes 1995 ). Description of the intervention Since the first description of surgery for acute perforated peptic ulcer many techniques have been recommended. Ulcers can be repaired by hand suturing the edges of the wound or using surgical stapling devices, covering the defect using an omental patch, or closing it with a fibrin sealant or a gelatin plug product ( Darzi 1993 ; Matsuda 1995 ; Tate 1993 ; Walsh 1993 ). Since the early 1990s, some authors have suggested that in cases of perforated peptic ulcer the laparoscopic approach may offer theoretical advantages over the open approach. Such advantages include reduced size of the surgical wound and diminished postoperative pain; fewer postoperative complications; less intestinal manipulation, which should diminish postoperative ileus and the long\u2010term risk of future adhesive obstructive complications; and the global cost savings derived from a shorter hospital stay and an earlier return to daily activities ( Benoit 1993 ; Michelet 2000 ; Mouret 1990 ; Naesgaard 1999 ; Sunderland 1992 ). Furthermore, it has been suggested that laparoscopic repair could be the best choice for patients with adverse prognostic factors such as advanced age and coexisting cardiopulmonary diseases, or a clinical evaluation delayed beyond 12 hours from the onset of symptoms ( Chou 2000 ; Hermansson 1999 ). However, some authors have also found that laparoscopic repair presents a somewhat higher incidence of leaks and is a more time\u2010consuming procedure ( Lau 1995 ; Lee 2001 ). Why it is important to do this review Controlled trials have been carried out trying to evaluate this approach. However, the results are inconclusive because of methodological weaknesses in the trials and the small numbers of participants ( Druart 1997 ; Gomez\u2010Ferrer 1996 ; Katkhouda 1999 ; Kum 1993 ; Lau 1995 ; Lau 1996 ; Lau 1998 ; Michelet 2000 ; Ozmen 1995 ; Robertson 2000 ; Siu 2002 ). A systematic review is, therefore, appropriate as meta\u2010analysis may prove informative as to the comparative efficacy and complication rates for the two surgical approaches. Thus, the present systematic review was developed to answer the following question: is laparoscopic treatment of perforated peptic ulcer associated with reduced wound complications, postoperative intra\u2010abdominal sepsis, duration of hospitalization and overall cost compared to the conventional (open) approach?",
        "summary": "Low quality evidence provides insufficient data to either support or refute the use of laparoscopic repair over open repair for perforated peptic ulcer. Surgical repair is recommended in people with perforated peptic ulcer. This can be performed laparoscopically or by open surgery. Three randomized trials including 315 participants compared laparoscopic with open repair of perforated peptic ulcer. The estimated effects of laparoscopic repair compared with open repair of perforated peptic ulcer on septic abdominal complications, pulmonary complications, surgical site infection, suture dehiscence, mortality, operating time, and hospital stay were imprecise and clinically important benefits or harms of laparoscopic compared with open repair cannot be ruled out. Quality of life was not assessed."
    },
    "CD008521": {
        "query": "What are the benefits and harms of vaccines for the prevention of rotavirus diarrhea?",
        "document": "Background Description of the condition Rotavirus is the leading known cause of severe gastroenteritis in infants and young children worldwide ( Parashar 2006a ; Vesikari 1997 ; WHO 2013 ). While nearly every child experiences at least one rotavirus infection in early childhood regardless of setting, the vast majority of rotavirus\u2010associated deaths occur in children in low\u2010 and middle\u2010income countries, particularly in sub\u2010Saharan Africa and in the Indian subcontinent. Prior to the rollout of rotavirus vaccination, rotavirus caused 37% of diarrhoeal deaths (\u02dc 450,000 deaths worldwide in 2008) in children younger than five years. Five countries accounted for more than half of all deaths, and 22% of deaths attributable to rotavirus infection occurred in India ( Tate 2012 ). In high\u2010income countries, where deaths due to rotavirus are rare, rotavirus accounted for 40% to 50% of hospital admissions due to diarrhoeal disease in the pre\u2010rotavirus vaccine period ( Linhares 2008 ; Parashar 2006a ; Tate 2012 ). Rotavirus is transmitted primarily via the faecal\u2010oral route, with symptoms typically developing one to two days following infection. Rotavirus infection occurs throughout life, and successive rotavirus infections occur during infancy and early childhood. The first rotavirus infection typically results in the most severe disease outcome; subsequent rotavirus infections are associated with milder disease or may be asymptomatic. However, differences in the age of first infection and number of infections required to acquire protection from symptomatic disease vary from one population to another. Rotavirus diarrhoea is particularly associated with severe outcomes between the ages of three and 35 months ( Parashar 2006b ), with a peak incidence of all episodes occurring between six and 24 months ( CDC\u2010ASIP 1999 ; Linhares 2008 ). The peak incidence of severe rotavirus disease occurs earlier in high\u2010mortality countries than in low\u2010mortality countries; an estimated 43% of all rotavirus hospitalizations in children aged under five occur by eight months of age in Africa compared with 27% in Europe ( Crawford 2017 ; Sanderson 2011 ). Typically, infants in low\u2010income countries experience a greater number of symptomatic episodes ( Gladstone 2011 ; Vel\u00e1zquez 1996 ). In temperate countries rotavirus infections display marked seasonality, with distinct peaks during the winter months and few infections identified outside this period, whereas rotavirus infections occur year\u2010round in most tropical countries. Rotaviruses are double\u2010stranded (ds) RNA viruses: genus Rotavirus , family Reoviridae . Each of the 11 dsRNA segments, contained within the core of a triple\u2010layered viral particle, encodes one or more viral proteins. Rotavirus A, which causes most human disease, is genetically diverse in each of its 11 genome segments (called genotypes), and a nucleotide sequence\u2010based, complete genome classification system is used. Because of their importance in protective immunity, the outer capsid proteins VP7 and VP4 have been most extensively investigated. Species A rotaviruses are classified into G and P genotypes, based on the sequence diversity of the RNA segments encoding VP7 and VP4, respectively; 32 G genotypes and 47 P genotypes have been described ( Crawford 2017 ) (see Figure 1 for details). Rotavirus vaccines are designed to protect against disease caused by the most prevalent strain types; globally, G1P[8], G2P[4], G3P[8], G4P[8], G9P[8] and G12 in combination with P[6] or P[8] account for over 90% of the genotypes that infect humans ( B\u00e1nyai 2012 ). A simplified diagram of the location of rotavirus structural proteins (source: Graham Cohn, Wikipedia (public domain image)): Rotaviruses are segmented, double\u2010stranded RNA viruses. The mature, triple\u2010layered virus particle comprises a core (which contains the viral genome), a middle layer (comprised of viral protein (VP)6, and an outer layer (comprised of VP7 and VP4) as shown in the figure. VP6 defines rotavirus group, and most rotaviruses that infect humans are of group A. The two outer capsid proteins independently induce neutralizing antibodies: VP7, a glycoprotein, defines G\u2010serotype; and the protease\u2010sensitive VP4 protein defines P\u2010serotype. G\u2010serotype determined by serological methods correlates precisely with G\u2010genotype obtained through molecular assays, whereas there is an imperfect correlation of P\u2010serotype and P\u2010genotype; P\u2010genotype is thus included in square brackets. Description of the intervention This review evaluates three vaccines, including a monovalent rotavirus vaccine (RV1; Rotarix, GlaxoSmithKline Biologicals) and a pentavalent rotavirus vaccine (RV5; RotaTeq, Merck & Co., Inc.), which have been evaluated in several large trials and are in routine use in many countries; and a further monovalent vaccine (Rotavac, Bharat Biotech Ltd.), which is currently licensed in India only. All three vaccines are listed as prequalified vaccines by the WHO ( Dellepiane 2015 ; WHO 2018 ). As of April 2018, 95 countries have introduced rotavirus vaccines into their immunization programmes ( ROTA council 2018 ). RV1 is an oral, live\u2010attenuated, human rotavirus vaccine derived from the most common circulating wild\u2010type strain G1P[8]. RV1 is based on a rotavirus of entirely human origin and is administered to infants in two oral doses with an interval of at least four weeks between doses. The manufacturer states that the \"vaccination course should preferably be given before 16 weeks of age, but must be completed by the age of 24 weeks\" ( EMA 2011 ). As of May 2016, RV1 had been introduced in national immunization programmes in 63 countries around the world ( PATH 2016 ). RV5 is an oral, live, human\u2010bovine, reassortant, multivalent rotavirus vaccine developed from an original Wistar calf 3 (WC3) strain of bovine rotavirus. The vaccine contains five live, human\u2010bovine reassortant rotavirus strains. Four reassortant rotavirus strains each express one of the common human VP7 (G) types including G1, G2, G3, and G4, and the fifth reassortant expresses the common human VP4 (P) type P[8]. The three\u2010dose liquid vaccine is intended for infants aged between six and 32 weeks, with the first dose given at six to 12 weeks and subsequent doses administered at four\u2010 to 10\u2010week intervals; however, the third dose should not be given after 32 weeks of age ( Merck 2008 ). As of May 2016, RV5 had been introduced in national immunization programmes in 22 countries around the world ( PATH 2016 ). Rotavac is a live\u2010attenuated, monovalent vaccine derived from a naturally\u2010occurring reassortant G9P[11] strain [116E] isolated from a newborn child in India ( Yen 2014 ). This oral vaccine was developed by Bharat Biotech Ltd. in India and was licensed in India in 2014 ( VAC Chandola 2017\u2010IND ). Three doses are recommended, to be administered at 6, 10, and 14 weeks of age. There are a further three rotavirus vaccines that have been licensed and approved for use in individual countries, but are not yet prequalified by the WHO. Lanzhou lamb rotavirus vaccine (LLR; Lanzhou Institute of Biomedical Products) which is licensed and used in China; a bovine rotavirus pentavalent vaccine (BRV\u2010PV, Rotasiil, Serum Institute of India Ltd.) which is licensed and used in India; and a monovalent vaccine (Rotavin\u2010M1, POLYVAC) which is licensed and used in Vietnam. Several vaccines, including the first licensed rotavirus vaccine (RRV\u2010TV; RotaShield, Wyeth Laboratories) were developed, tested in trials, and later abandoned or withdrawn from use. These vaccines are covered in a separate Cochrane Review ( Soares\u2010Weiser 2004 ). RRV\u2010TV, a tetravalent rhesus\u2010human reassortant vaccine, was withdrawn from use in 1999 following reports of intussusception (bowel obstruction which occurs when one segment of bowel becomes enfolded within another segment). Evaluations have since suggested that the risk of intussusception was age\u2010related, with 80% of intussusception cases occurring in infants who were more than 90 days old when the first vaccine dose was administered ( Simonsen 2005 ). Although it is still currently licensed, this vaccine is no longer in clinical use ( Dennehy 2008 ). How the intervention might work Vaccination with RV1 and RV5 was first recommended in 2006 in Europe and the Americas, where clinical trials had demonstrated vaccine efficacy of 85% to 100% ( RV1 Ruiz\u2010Palac 06\u2010LA/EU ; RV5 Vesikari 2006b\u2010INT ). In April 2009, following clinical trials of RV1 and RV5 in low\u2010 and middle\u2010income countries in Africa and Asia, the WHO Strategic Advisory Group of Experts (SAGE) on Immunization recommended \"the inclusion of rotavirus vaccination of infants into all national immunization programmes\", with a stronger recommendation for countries where \"diarrhoeal deaths account for \u226510% of mortality among children aged <5 years\" ( SAGE 2009 ). Due to an age\u2010related risk of intussusception identified with RRV\u2010TV ( Murphy 2001 ), SAGE recommended administering the first dose of RV1 or RV5 to infants of six to 15 weeks of age, with the last dose administered before 32 weeks of age ( SAGE 2009 ). In April 2012, SAGE relaxed the age restricted recommendation and advised to vaccinate \"as soon as possible after the age of six weeks\" because \"the current age restrictions for the first dose (< 15 weeks) and last dose (< 32 weeks) are preventing vaccination of many vulnerable children\" ( Patel 2012 ; SAGE 2012 ). Many oral vaccines, including rotavirus vaccines, have demonstrated lower immunogenicity and efficacy in low\u2010 and middle\u2010income countries in Africa and Asia compared to high\u2010income countries in North America, South America, and Europe ( Levine 2010 ). A systematic review demonstrated a correlation between lower vaccine efficacy against severe rotavirus diarrhoea and high child mortality rates ( Fischer Walker 2011 ). The reasons for reduced oral vaccine efficacy in countries with higher child mortality rates are unknown; factors may include interference by maternal antibody, co\u2010administration with oral poliovirus vaccine, histoblood group antigen, diverse rotavirus strain types, micronutrient deficiencies, endemic infections such as malaria, tuberculosis, or HIV, concomitant enteric infections, gut inflammation, and altered gut microbiota ( Czerkinsky 2015 ). The safety and efficacy of the licensed vaccines for the prevention of rotavirus gastroenteritis in infants have been assessed in several randomized controlled trials (RCTs) worldwide. The goal of this review is to systematically assess these trials and evaluate vaccine efficacy against rotavirus diarrhoea, all\u2010cause diarrhoea, and diarrhoea\u2010related medical visits and hospitalization. We also examine the occurrence of deaths and serious adverse events, including intussusception, to provide decision\u2010makers, clinicians, and caregivers with the relevant information to aid decisions about vaccine use. Why it is important to do this review The original Cochrane Review of rotavirus vaccines ( Soares\u2010Weiser 2004 ) examined vaccines in use and other vaccines, including those that were no longer in use or were in development. Soares\u2010Weiser 2004 concluded that more trials were needed before routine vaccine use could be recommended. An update in 2009 included a new search, revised inclusion criteria (only vaccines in use in children), updated review methods and new authors. The review was updated again in 2010 with nine new studies ( Soares\u2010Weiser 2010 ). The 2010 version of the review concluded that RV1 and RV5 are both effective vaccines for the prevention of rotavirus diarrhoea. Another update in February 2012 added a further nine new studies, GRADE \u2018Summary of findings' tables and, again, new authors joined the team ( Soares\u2010Weiser 2012a ). The November 2012 update included a new search, major restructuring of analyses, including re\u2010evaluating primary outcomes in consultation with the WHO to reflect the observation that vaccine efficacy profiles are different in countries with different mortality rates ( Soares\u2010Weiser 2012b ). This current update adds a further 10 RV1 and RV5 studies to the review and four studies of a new vaccine, Rotavac, that has been prequalified by the WHO since the previous version of the review.",
        "summary": "Rotavirus vaccines (RV) currently recommended by the World Health Organization (WHO) reduce the incidence of diarrhea in infants up to six months of age in countries with baseline low and high mortality rates. Their impact on other outcomes, such as mortality, intussusception, and adverse events, is less clear. Currently, three types of RV are used, namely, RV1 (monovalent vaccine), RV5 (pentavalent vaccine), and Rotavac (monovalent vaccine). RV1, RV5, and Rotavac all reduced the risk of rotavirus diarrhea (any episode, severe episode) in infants within a one\u2010 or two\u2010year period when compared with placebo (between around 12,000 and 60,000 infants in analyses). At two years\u2019 follow\u2010up, on average, 31 per 1000 infants who received RV1, 41 per 1000 who received RV5, and 93 per 1000 who received Rotavac had rotavirus diarrhea of any severity, compared with 85, 89, and 142 who received placebo, respectively. The incidence of severe rotavirus diarrhea at two years was lower, with on average 4 per 1000 infants who received RV1, 13 per 1000 who received RV5, and 21 who received Rotavac developing severe diarrhea compared with 24, 35, and 47 who received placebo, respectively. Researchers detected little or no difference in all\u2010cause mortality between infants who received RV1, RV5, or Rotavac vaccine and those who received placebo. Although analyses included 10,000 to 100,000 infants, event rates were extremely low (0.1%) and evidence was low and very low certainty. Rates of intussusception and Kawasaki disease (the latter reported for RV1 only) were similar between groups, but again, event rates were very low (\u2264 0.1%) and the evidence was very low to low certainty. No clear and consistent differences between vaccine and placebo were noted in rates of reaction to the vaccines, including fever, diarrhea and vomiting, withdrawal due to adverse events, and serious adverse events. When trials conducted in countries with low and high mortality rates were analyzed separately for RV1 and RV5 vaccines, results showed that both were effective in reducing the incidence of diarrhea (any episode, severe episodes, and episodes that required hospitalization). All trials of Rotavac were conducted in high\u2010mortality countries. The numbers of infants assessed in high\u2010mortality countries were much smaller than the numbers in low\u2010mortality countries; therefore, uncertainty surrounding the expected magnitude of these effects was greater. In clinical practice, it would be expected that absolute benefits would be greater in high\u2010mortality countries due to the larger numbers of infants currently and potentially affected by rotavirus."
    },
    "CD012414": {
        "query": "What are the effects and optimal frequency of administration of phosphodiesterase 5 inhibitors (PDE5Is) for men with postprostatectomy erectile dysfunction?",
        "document": "Background Prostate cancer is the most common non\u2010skin cancer in men. In 2014 in the UK alone, there were 46,700 new cases of prostate cancer diagnosed accounting for about 13% of all new cancer diagnoses. Prostate cancer in 2016 resulted in about 11,500 deaths in the UK making it the second most common cancer related cause of death in men ( Cancer Research UK ). In the USA, prostate cancer accounted for 172,258 new cancer diagnoses and caused 28,343 deaths in 2014 ( U.S. Cancer Statistics Working Group ). For organ\u2010confined prostate cancer (pT2), treatment options with curative intent include mainly radical prostatectomy (RP) and radiotherapy. RP can be undertaken as an open procedure typically through a retropubic approach (RRP), laparoscopic (LRP) or robotic\u2010assisted (RARP). Radiotherapeutic options for prostate cancer include external beam radiotherapy (EBRT) typically delivered as 2 Gy fractions over seven weeks to a total of 70 Gy with or without concomitant hormone treatment. Other therapeutic options that involve radiotherapy include intensity\u2010modulated radiotherapy and brachytherapy. Active surveillance of prostate cancer also falls into the category of treatments with curative intent. This treatment approach consists of an active decision not to treat the prostate cancer at the time of diagnosis but rather to monitor the man closely to enable the proper timing of curative treatment, taking into account the man's life expectancy. It is advocated by European Heidenreich 2014 and American Sanda 2017 urological guidelines in men with low\u2010risk organ\u2010confined prostate cancer. RP has the potential to completely remove the tumour and remains a preferred and effective treatment modality utilised as a first option in approximately 33% of prostate cancer cases with organ\u2010confined disease and in 52% of cases in men aged over 62 years of age ( Lalong\u2010Muh 2012 ; American Cancer Society 2014 ). In 2010 in the US alone, there were 11,290 prostatectomies, two\u2010thirds of which were RARP. These figures compared to the data from 2004, when there were 6188 prostatectomies, of which only 8% were RARP, suggests that RP rates have risen exponentially since the introduction of RARP ( Lowrance 2012 ). The common adverse effects of RP include erectile dysfunction (ED) and urinary incontinence. Many factors influence the incidence and severity of postoperative ED, including man's age, tumour stage, preoperative potency, length of surgical intervention and experience of the surgeon ( Wang 2014 ). Despite meticulous dissection in an attempt to preserve the neurovascular bundles with nerve\u2010sparing surgery, ED remains common. Even with nerve\u2010sparing surgery, there is a period of neuropraxia during which the man has no spontaneous erections. This can lead to penile hypoxia and long\u2010lasting damage to the erectile tissue ( Burnett 2005 ; Raina 2010 ). The length of time that neuropraxia and consequent ED will last is difficult to predict, with some studies suggesting many men require more than two years to recover erectile function satisfactorily ( Rabbani 2010 ). The introduction of the robot\u2010assisted technology has refined nerve\u2010sparing procedures mainly through three\u2010dimensional magnification and movement calibration that could result in reduced postprostatectomy ED rates. One systematic review evaluated the prevalence and the potential risk factors of ED after RARP. Their findings suggested that the prevalence of ED ranged from 54% to 90% at 12 months and 63% to 94% at 24 months ( Ficarra 2012 ). RARP had a significant advantage over RRP with an ED prevalence of 24.2% with RARP versus 47.8% with RRP at 12 months ( Ficarra 2012 ). However, despite these technological advances, ED is still significant in this patient population. This has led to the development of penile rehabilitation programmes that aim to promote male sexual function before and after any insult to the penile erectile physiological axis. Penile rehabilitation has now become an integral part of patient management after RP and most urologists advocate that this should be commenced as soon as possible following surgery. Description of the condition Male sexual dysfunction related to prostate cancer treatment can be divided into three broad categories: ED and changes in penile size; ejaculatory and orgasmic dysfunction; and psychosexual impairment with changes in sexual desire, intimacy and mental health ( Chung 2013 ). ED is defined as the inability of a man to achieve and maintain an erection of sufficient strength for satisfactory sexual activity ( NIH Consensus Conference 1993 ). It's incidence reported in the literature after RP varies dramatically from 20% to 90% ( Fowler 1993 ; Rabbani 2000 ; Stanford 2000 ; Kundu 2004 ; Rozet 2005 ; Penson 2008 ; Alemozaffar 2011 ). The discrepancy in the reported rates of erectile function after RP is due to many factors. These include variations in study population demographics, methods of data acquisition, variability in questionnaire use, duration of postoperative follow\u2010up, variations in baseline erectile function status, inconsistency in defining adequate erectile function, surgical technique, and the definition of quality and consistency of erection ( Mulhall 2009 ). ED can have a major impact on the man's self\u2010esteem, quality of life (QoL), confidence and life satisfaction, causing depression in certain cases ( Kubin 2003 ). Quantifying accurately the prevalence of ED after RP is of utmost importance in evaluating the burden of this treatment\u2010related adverse effect, in order to set appropriate expectations and facilitate medical decision making. One analysis identified 24 studies that originated from major cancer centres and reported ED recovery outcomes after RP, in large participant cohorts ( Mulhall 2009 ). In these studies, the mean overall rates of erectile function recovery were 48% (standard deviation (SD) 25%; range 12% to 96%). When nerve sparing was accounted for, as it was in 14 (58%) of the 24 articles reviewed, mean erectile function recovery rates were 50% (SD 24%) for bilateral and 34% (SD 16%) for unilateral nerve\u2010sparing surgery. The starting point for analysing data on penile rehabilitation is objectively defining ED and reaching a consensus as to the definition of return to potency following RARP. Unfortunately, there remains significant heterogeneity in the literature in terms of definitions of ED after RP, and a significant number of studies do not clearly state their definitions of ED or return to sexual function. Scoring systems such as Sexual Health Inventory For Men (SHIM) scores, International Index of Erectile Function (IIEF\u20105), sexual questionnaires, and patient and partner reporting are all prone to inaccuracies. Therefore, evaluating return of potency following RARP in the absence of a consensus definition was a challenge for this review. For the purposes of this study as outlined in more detail in the 'methodology' section, we included men with erectile function sufficient for intercourse. According to the IIEF\u20105 and IIEF questionnaires, we defined 'sufficient for intercourse' as men with mild (IIEF\u20105 greater than 17) or no (IIEF greater than 19) ED. Therefore, we defined return to sexual function as return to baseline IIEF\u20105/IIEF scores. Description of the intervention Penile rehabilitation following RP revolves around the use of medications (alone or in combination) or devices to preserve erectile tissue health and maximise erectile function recovery or both medications and devices ( Mulhall 2010 ). The treatment options include: phosphodiesterase\u20105 inhibitors (PDE5I) (sildenafil citrate; tadalafil; vardenafil) scheduled or daily dosing; alprostadil preparations (prostaglandin E1, such as Viridal Duo or Caverject as injectables, or Medicated Urethral System for Erections (MUSE) as urethral pellets), and vacuum erection or vacuum constriction devices (VED/VCD) ( Steggall 2011 ; Weyne 2015 ). These interventions have been used singly or in combination, either presurgery or following successful trial without catheter following surgery, and at different strengths, dosing frequencies and combinations, to attempt to identify the most suitable option to prevent or limit neuropraxia, recover erections and restore sexual activity. How the intervention might work The main pathophysiological mechanism which underlies the development of ED after RP is damage to the cavernosal nerves and vascular injury. Damage to these nerves occurs either due to their complete transection during non\u2010nerve\u2010sparing procedures or due to neuropraxia which commonly occurs during nerve\u2010sparing RP. Neuropraxia is defined by the transient block of nerve transmission despite an anatomically intact nerve, caused in this case by direct trauma, stretching, heating due to electrocautery, ischaemia and local inflammation ( Fode 2013 ). Vascular injury primarily involves damage to the accessory pudendal arteries. This, together with the direct effect of loss of cavernosal nerve function results in a reduction in the oxygenation of penile tissues due to structural changes in vascular smooth muscle and endothelium. This ultimately causes loss of smooth muscle due to apoptosis ( Kendirci 2006 ), impaired veno\u2010occlusive function, collagen accumulation and penile fibrosis ( Hatzimouratidis 2009 ; Kacker 2013 ). Collectively these physiological changes result in ED and penile shortening. Surgical intervention is known to induce hypoxia in a time\u2010dependent manner, such that the potential for recovery of erectile function decreases with time. The goal of early intervention with penile rehabilitation strategies is to improve the oxygenation of cavernosal tissue during the period of neuropraxia, to prevent uninhibited deterioration of penile tissues and to minimise (if not abrogate) the adverse structural and physiological changes that occur in the penis following RP. Penile rehabilitation also ensures that the man is well\u2010placed to regain presurgery erectile function and not remain dependent on erectile aids following surgery ( Burnett 2013 ; Segal 2013 ). Oral PDE5I by virtue of their ease of use, are often considered as the mainstay of ED management. They are generally well\u2010tolerated, have proved to be relatively safe and are the preferred treatment after RP in some centres. Nevertheless, there are a number of men with postsurgery ED, who do not respond to PDE5I, or who become less responsive and less satisfied as treatment progresses. In some men, PDE5I are contraindicated by virtue of the use of nitrate medication and the risk of consequent hypotension. Apart from the oral PDE5I, the other options for management of postprostatectomy ED (including MUSE and intracavernosal injections (ICIs)) are invasive, uncomfortable, unappealing and sometimes ineffective for some men. While PDE5I may be appealing as they appear 'easy' to use, there are limited data examining whether PDE5I aid penile rehabilitation in a time\u2010dependent manner, which is critical as men often prefer to manage their incontinence before their erections, and if treatment is not introduced early, there is a risk of penile atrophy that will make the recovery of erections more problematic. Why it is important to do this review ED is a common adverse event of RP and it significantly affects QoL. The aforementioned new insights into the pathophysiology of post\u2010RP ED have led to the development of a multitude of different penile rehabilitation strategies which aim to improve the oxygenation of penile tissues during the period of neuropraxia that inevitably follows RP in the hope to reduce the rate of postprostatectomy ED and restore sexual activity without the use of erectogenic aids. Several randomised controlled trials (RCTs) have been published which address the question of whether these treatment modalities (alone/in combination and at different dosages or dosing schedules) are of any benefit in reducing the incidence of ED after RP and hasten the return to unassisted sexual function. Currently there is still controversy regarding the effectiveness of rehabilitation programmes. The purpose of this review is to systematically evaluate these treatment options and combinations to identify whether any of these can recover erections and restore sexual activity in addition to evaluating other important clinical outcomes such as adverse events, treatment acceptability by patients, treatment discontinuation rates and QoL. Our further aim is to compare, where evidence exists, different treatment modalities between them and determine which, if any, of these treatments may be most beneficial to restoring unassisted erectile function in men with postprostatectomy ED.",
        "summary": "For men with postprostatectomy erectile dysfunction, scheduled PDE5Is seem to have little to no effect on erectile function compared with no treatment or on\u2010demand PDE5Is. However, most evidence was of very low to low certainty, and the adverse event profile remains unclear. Compared with placebo or no treatment for men with postprostatectomy erectile dysfunction, scheduled PDE5Is appear to have little to no effect on short\u2010term (up to 11 months) self\u2010reported potency and erectile function but may result in fewer serious adverse events compared with placebo (23 vs 71 per 1000 men). However, researchers did not describe the nature of the serious adverse events, and the evidence for both these outcomes was low to very low, making it difficult to draw any conclusions. The only longer\u2010term outcomes (> 12 months) reported by the reviewers were sexual quality of life and treatment discontinuation; both appear to show little to no effect (moderate\u2010certainty evidence). Compared with on\u2010demand PDE5Is, reviewers found little to no difference with daily PDE5Is in the International Index of Erectile Function at short term (high\u2010certainty evidence) nor in sexual quality of life over the longer term (moderate\u2010certainty evidence). Daily PDE5Is appear to result in little to no difference in both short\u2010term and long\u2010term self\u2010reported potency, erectile function, treatment discontinuation, and serious adverse events, but the evidence for these outcomes was of very low to low certainty."
    },
    "CD009326-1": {
        "query": "How do home visits compare with hospital clinic visits in the early postpartum period?",
        "document": "Background Description of the condition The postpartum period, defined by the World Health Organization (WHO) as the period from childbirth to the 42nd day following delivery ( WHO 2005 ), is critical for both mothers and newborns. An estimated 529,000 maternal deaths occur worldwide each year because of pregnancy\u2010related complications in the antenatal, intrapartum, and postpartum periods, especially in resource\u2010limited settings ( WHO 2005 ).These deaths are often sudden and unpredictable, with 11% to 17% occurring during childbirth itself and 50% to 71% occurring during the postpartum period ( WHO 2005 ). Maternal health problems commonly observed in the postpartum period include postpartum haemorrhage, fever, abdominal and back pain, abnormal discharge, puerperal genital infection, thromboembolic disease, and urinary tract complications ( Bashour 2008 ), as well as psychological and mental health problems such as postnatal depression. The postpartum period is also critical for newborns. Every year approximately 3.7 million babies die in the first four weeks of life. Most of these infants are born in developing countries and most die at home. Nearly 40% of all deaths of children younger than five years old occur within the first 28 days of life (neonatal or newborn period). Just three causes\u2014infections, asphyxia, and preterm birth\u2014account for nearly 80% of these deaths ( WHO/UNICEF 2009 ). Moreover, the postpartum period is a time of transition for women and their families, who are adjusting on physical, psychological, and social levels ( Shaw 2006 ). In most developed countries, postpartum hospital stays are often shorter than 48 hours following a vaginal birth; thus most postpartum care is provided in community and ambulatory\u2010care settings. Early intervention in the postpartum period may prevent health problems from becoming chronic with long\u2010term effects on women, their babies, and their families. Description of the intervention The purpose of a home\u2010visiting program is to provide support at home for mothers, babies, and families by health professionals or skilled attendants. However, a single clearly defined methodology for this intervention does not exist. Further, the term \"home visiting\" is used differently in various contexts ( AAP 2009 ). Since the 1970s, the length of hospital stay after childbirth has fallen dramatically in many high\u2010resource settings. Early postnatal discharge of healthy mothers and term infants does not appear to have adverse effects on breastfeeding or maternal depression when women are offered at least one nurse\u2010midwife home visit after discharge ( Brown 2002 ). Home\u2010visiting programs provide breastfeeding and hygiene education, parenting and child health instruction, and general support to families, successfully addressing many of the barriers to access including transportation issues, initiation of timely care, and completeness of services ( AAP 1998 ; AAP 2009 ). Several trials have assessed the impact of home\u2010visiting programs, especially effects on child abuse and neglect in vulnerable families ( Donovan 2007 ; Olds 1997 ; Quinlivan 2003 ). Others focused on the effectiveness and cost\u2010effectiveness of intensive home\u2010visiting programs ( Barlow 2006 ; Carabin 2005 ; McIntosh 2009 ). Some home\u2010visiting programs have specifically targeted high risk groups such as women suffering domestic abuse (intimate partner violence) or families that are economically or socially disadvantaged. Home\u2010visiting programs for high risk groups or those by child health nurses may include components during pregnancy and may continue over many months or years; such programs are outside the scope of this review and have been addressed in other Cochrane reviews ( Bennett 2008 ; Jahanfar 2013 ; Macdonald 2008 ; Turnbull 2012 ). In this review we focus on the early postnatal period following discharge from hospital. In 2009, WHO and the United Nations Children's Fund recommended home visits by a skilled attendant in resource\u2010limited settings. In high\u2010mortality settings and where access to facility\u2010based care is limited, at least two home visits are recommended for all home births: the first visit should occur within 24 hours of the birth, the second visit on day three, and if possible, a third visit should be made before the end of the first week of life (day seven). For babies born in a healthcare facility, the first home visit was recommended to be made as soon as possible after the mother and baby return home with remaining visits following the same schedule as for home births ( WHO/UNICEF 2009 ). A recent review demonstrated the effectiveness of community\u2010based intervention packages in improving neonatal outcomes and reducing maternal and neonatal morbidity and mortality in resource\u2010limited settings; home visiting is the one of the main components in each of these intervention packages. This review offers encouraging evidence of the value of integrating maternal and newborn care in community settings ( Lassi 2010 ). We, therefore, did not include intervention packages of continuous care with components of antenatal or hospital care in our review. How the intervention might work In high\u2010resource settings healthy women and babies are frequently discharged from hospital within one or two days of the birth, and in low\u2010resource settings women may be discharged within hours of the birth or give birth at home ( Brown 2002 ). Potentially, home visits in the first few days of the birth by healthcare professions or trained support workers offer opportunities for assessment of the mother and newborn, health education, infant feeding support, emotional or practical support and, if necessary, referral to other health professionals or agencies ( Carabin 2005 ; Donovan 2007 ; Lassi 2010 ; Shaw 2006 ). Postpartum visits may prevent health problems developing or reduce their impact by early intervention or referral. Home visits have improved coverage of key maternal and newborn care practices such as early initiation of breastfeeding, exclusive breastfeeding, skin\u2010to\u2010skin contact, delayed bathing, attention to hygiene (e.g. hand washing and water quality), umbilical cord care, infant skin care. In addition, home visits may identify conditions that require additional care or check\u2010up, as well as counselling regarding when to take the mother and newborn to a healthcare facility ( WHO/UNICEF 2009 ). Home visits may involve not only the assessment of the mother and newborn for physical problems but also assessment of maternal mental health, family circumstances and the home environment. Depending on the context, home visits may take a non\u2010judgmental and supportive role or a more directive approach in which the goals are to monitor family compliance with standards of parenting care and ensure the newborn's health and welfare.The type of approach used can influence the ability of the carers to engage mothers and newborns, resulting in acceptance or rejection of the help offered and potential for further disengagement ( Doggett 2005 ). Why it is important to do this review Despite many studies and reviews, evidence regarding the effectiveness of different types of home\u2010visiting programs in the early postnatal period is not sufficient. In some contexts once women have been discharged from hospital there may be no, or very limited postnatal follow\u2010up. In higher\u2010resource settings once women are at home, services may be provided by a range of health and social care agencies (newborn health visitors, social workers, paediatricians and general practitioners) and may be fragmented; postnatal home visits potentially allow continuity of care after hospital discharge and for the assessment and referral of the mother and newborn. This review addresses the following questions: do different schedules of postpartum home\u2010visiting programs reduce maternal/neonatal mortality and morbidities, and if they do, what is the optimal schedule for postpartum home visits? This review includes reports evaluating the frequency, timing, duration and intensity of home visits.The optimal schedule has been set out by WHO/UNICEF 2009 , however, there was no clear evidence underpinning recommendations.",
        "summary": "Compared with attending a hospital clinic for postnatal checks, women in the early postpartum period receiving postnatal checks by nurses at home were more likely to be satisfied with the postnatal care received (on average, 841 vs 667 per 1000 women were satisfied with postnatal care), but randomized controlled trials (RCTs) reported no clear differences between groups for any other outcomes assessed (severe maternal or neonatal morbidity, postnatal depression, and breastfeeding), and failed to report a large number of clinically relevant outcomes (maternal or neonatal mortality, secondary postpartum hemorrhage, neonatal morbidity, neonatal immunization, infant failure to thrive, and infant abuse/neglect). In addition, reviewers did not assess the quality of the evidence, but most trials either failed to describe key methods or described methods that could introduce bias (such as failure to blind outcome assessors and high attrition rates). Therefore, conclusions of equivalence for the safety and efficacy of home and hospital clinic visits cannot be drawn."
    },
    "CD006301": {
        "query": "In children with high\u2010risk neuroblastoma, how does high\u2010dose chemotherapy plus autologous hematopoietic stem cell rescue compare with conventional chemotherapy?",
        "document": "Background Description of the condition Neuroblastoma is the most common extracranial solid tumour in children comprising 8% to 10% of all childhood cancers. The incidence is nearly 10 per 1,000,000 children under the age of 15 years and 90% of cases are diagnosed in the first 10 years of life. Children with neuroblastoma mostly present with abdominal disease and more than half of the cases have advanced disease (defined as stage III or IV disease) ( Aydin 2009 ; Brodeur 2006 ; Goldsby 2004 ). Neuroblastoma is one of the most challenging and enigmatic neoplasms of childhood because of its biological heterogeneity and contrasting patterns of clinical behaviour. Some young infants with favourable disease may experience complete spontaneous regression while older children with metastatic disease mostly relapse despite initial response to chemotherapy. The prognosis and management of children with neuroblastoma is highly dependent on clinical, histopathological and biological characteristics, and they are stratified into risk groups based on prognostic factors ( Goldsby 2004 ; Maris 2005 ; Maris 2007 ; Weinstein 2003 ). For pre\u2010treatment risk stratification traditionally an age of one year was used as a cut\u2010off point. The low\u2010risk disease group included cases younger than one year who had stage I, II or IV\u2010S disease with favourable histopathology and no MYCN oncogene amplification ( Brodeur 2006 ; Goldsby 2004 ; Maris 2005 ; Maris 2007 ; Weinstein 2003 ). High\u2010risk neuroblastoma cases were traditionally characterised by an age older than one year, disseminated disease, MYCN oncogene amplification and unfavourable histopathologic findings ( Brodeur 2006 ; Goldsby 2004 ; Maris 2005 ; Weinstein 2003 ). However, in recent years a new neuroblastoma risk classification system has been developed by the International Neuroblastoma Risk Group (INRG) Task Force resulting in standardised approaches for the initial evaluation and treatment stratification of neuroblastoma patients ( Cohn 2009 ). In this INRG classification system an age cut\u2010off of 18 months is used for pre\u2010treatment risk stratification, as opposed to the traditional cut\u2010off of one year. In many of the currently published studies patients with what is now understood to be intermediate\u2010risk disease were classified as high\u2010risk patients. Consequently the relevance of the results of these studies to current practice can be questioned. Description of the intervention Substantial improvement has been achieved in the cure of patients with low\u2010risk neuroblastoma resulting in survival rates up to 90% ( Brodeur 2006 ; Goldsby 2004 ). In high\u2010risk cases, despite intensified combination chemotherapies, surgery, radiotherapy and the use of differentiation agents, prognosis improved only modestly with long\u2010term survival in less than one\u2010third of patients ( Brodeur 2006 ; De Bernardi 2003 ; Goldsby 2004 ; Weinstein 2003 ; Peinemann 2015a ). In the last two decades higher remission rates have been achieved with intensive induction chemotherapy regimens combined with surgical resection, external irradiation or both ( Brodeur 2006 ; Castel 1995 ; Goldsby 2004 ; Kaneko 2002 ; Kushner 2004 ; Laprie 2004 ; Sawaguchi 1990 ). The effects of retinoic acid post consolidation therapy for high\u2010risk neuroblastoma patients treated with autologous hematopoietic stem cell transplantation are currently not clear ( Peinemann 2015b ). The challenge is to maintain remission since more than half of patients with high\u2010risk disease develop systemic disease recurrence with or without a relapse at the primary tumour site ( Brodeur 2006 ; Goldsby 2004 ; Matthay 1993 ; Weinstein 2003 ). AntiGD2 antibody\u2010based immunotherapy has been shown to improve event\u2010free and overall survival in high\u2010risk neuroblastoma patients in remission after multimodality treatment ( Yu 2010 ). Therapy failures are mostly attributed to development of resistance to chemotherapy and minimal residual disease is considered an important cause of recurrence ( Burchill 2004 ; Keshelava 1998 ; Reynolds 2001 ; Reynolds 2004 ). The idea that further increasing dose intensity may overcome chemotherapy resistance has provided a rationale for aggressive high\u2010dose chemotherapy consolidation protocols ( Cheung 1991 ; Pritchard 1995 ). Such myeloablative chemotherapy regimens utilise effective high\u2010dose drug combinations which can be safely escalated to levels above those causing bone marrow ablation. Rapid bone marrow reconstitution can be achieved by autologous stem cell rescue. Autologous peripheral blood stem cells (PBSC) are the preferred source for rescue as this has several advantages over autologous bone marrow grafts. For example, the procedure of PBSC collection is easier, the incidence of tumour cell contamination is lower, and the yield of stem cells is higher ( Brodeur 2006 ; Cohn 1997 ; Ladenstein 1994 ). A possible limitation of using autologous products is the risk of tumour cell contamination in the graft, which has been shown to contribute to relapse. Considerable efforts have been made to detect and remove tumour (negative purging) or select progenitor cells (positive purging) before reinfusion ( Ladenstein 2004 ). Disease status prior to stem cell rescue has a crucial influence on final outcome. Patients in complete, very good partial or partial remission have a better prognosis, while those with stable disease or no response have a poor outcome ( Ladenstein 2004 ). Patients undergoing stem cell rescue may experience toxicities related to conditioning regimens like veno\u2010occlusive disease of the liver or haemorrhagic cystitis ( Bollard 2006 ). Growth failure, endocrine disorders such as gonadal or thyroid dysfunction, hearing impairment, renal impairment, orthopedic complications as well as the occurrence of secondary malignancies are among the late complications following high\u2010dose chemotherapy and stem cell rescue ( Bollard 2006 ; Trahair 2007 ). Why it is important to do this review Many retrospective and prospective non\u2010randomised studies support the use of a myeloablative consolidation in neuroblastoma. Retrospective studies mostly suggest that intensification of consolidation therapy with autologous stem cell rescue following high\u2010dose chemotherapy improves survival ( Castel 1995 ; Di Caro 1994 ; Matthay 1995 ; Philip 1997 ; Stram 1996 ; Verdeguer 2004 ). The results of non\u2010randomised pilot studies by the Children's Cancer Group also suggest a modest prolongation of event\u2010free survival for children with high\u2010risk neuroblastoma ( Matthay 1995 ). This is the second update of the first systematic review evaluating the current state of evidence on the efficacy of high\u2010dose chemotherapy and autologous haematopoietic stem cell rescue compared with conventional therapy in patients diagnosed with high\u2010risk neuroblastoma ( Yal\u00e7in 2010 ; Yal\u00e7in 2013 ).",
        "summary": "In children with high\u2010risk neuroblastoma, evidence (based on data from three randomized controlled trials enrolling 700 patients) shows that myeloablative therapy coupled with autologous hematopoietic stem cell transplantation improves event\u2010free survival when compared with conventional chemotherapy (HR 0.79, 95% CI 0.70 to 0.90). There does not seem to be an impact of the combined treatment on overall survival (HR 0.86, 95% CI 0.73 to 1.01). Owing to lack of data, no definitive conclusions can be drawn regarding adverse effects and quality of life, although possible higher levels of adverse effects should be considered when proposing the combined treatment."
    },
    "CD001395": {
        "query": "In women with menopausal vasomotor symptoms, is there randomized controlled trial evidence to support the use of phytoestrogens?",
        "document": "Background Description of the condition Menopause is a significant event in the lives of most women, as it marks the end of a woman's natural reproductive life. The perimenopausal and early postmenopausal years are typically characterised by falling levels of endogenous oestrogen, which can give rise to vasomotor symptoms that are severe and disruptive, particularly in the early and late menopausal transition and in early postmenopause, as categorised by the STRAW (STages of Reproductive Aging Workshop) criteria ( Harlow 2012 ). These vasomotor symptoms include hot flushes (also known as 'hot flashes'), sweating and sleep disturbances. Hot flushes are described as sudden feelings of heat in the face, neck and chest ( WHO 1996 ). Hot flushes are frequently accompanied by skin flushing and perspiration, followed by a chill as core body temperature drops ( Freedman 2001 ; Kronenberg 1990 ). Flushes vary in frequency, duration and severity and may be spontaneous and unpredictable ( Freedman 1995 ). Hot flushes that occur during the night are typically referred to as night sweats. Flushes and night sweats are events of concern in themselves because they can disrupt sleep patterns and alter daily activities, which can lead to fatigue and decreased quality of life ( Ayers 2013 ; NAMS 2004 ). Hot flushes are thought to result from both the brain's response to diminished hormones and hormonal fluctuations that occur during the menopausal transition, which leads to instability of thermoregulatory mechanisms (that regulate temperature) in the hypothalamus ( Deecher 2007 ; Freedman 2001 ; Kronenberg 1987 ). The prevalence of vasomotor symptoms varies with ethnicity. Flushes are less common among East Asian women (median 16%) than among American and European women (median 55%) ( Freeman 2007 ). Up to 40% of Western women are affected severely enough to seek medical help ( Freeman 2007 ; Gold 2006 ). An Australian prospective study with 13\u2010year follow\u2010up reported that the mean duration of troublesome vasomotor symptoms was 5.5 years ( Col 2009 ). A study of more than 10,000 British women 54 to 65 years of age found that more than half (54%) were currently experiencing vasomotor symptoms (averaging 34 hot flushes or night sweats per week), which were problematic in 40% of cases and were fairly stable across the age range ( Hunter 2012 ). Although hot flushes are reported as more prevalent and intense in the perimenopausal and early postmenopausal years, they continue to be important in up to 14.6% of women in their sixties and in 8.6% of women in their seventies ( Roussouw 2007 ). Description of the intervention Most therapies designed to combat menopausal vasomotor symptoms aim to supplement levels of circulating oestrogen ( Sikon 2004 ). The treatment of choice has traditionally been hormone therapy (HT), but, despite its effectiveness for symptom reduction, a marked and global decline has occurred in the prescription and use of HT because of concerns about long\u2010term use, particularly worry about increased risk of chronic diseases ( Bestul 2004 ; Haas 2004 ; Travers 2006 ). Although the combination of HT and unopposed oestrogen therapy was previously prescribed to prevent the onset of cardiovascular events as women grew older, a report of the Women's Health Initiative (WHI) trial, in 2002, indicated that the risks of this treatment outweighed the benefits ( Roussouw 2002 ). Combined therapy was linked with increased risk of breast cancer, stroke, thromboembolism (blood clots), gallbladder disease and dementia. Unopposed oestrogen therapy increased the risk of stroke, thromboembolism and gallbladder disease, and other studies reported an increase in the incidence of breast cancer ( Beral 2003 ). Data now available from 11 years of follow\u2010up provided by WHI show that risks are influenced by the age of the woman, the time since menopause and whether the HT was combined or consisted of oestrogen only ( NAMS 2012 ). Contraindications to HT include a family history or increased risk of cardiovascular disease, blood clotting disorders, venous thromboembolism or certain hormone\u2010sensitive cancers ( Anderson 2003 ; Grady 2000 ). Some women report adverse effects when taking HT ( Bakken 2004 ; Bjorn 1999 ); potential side effects include breast tenderness, bloating and genital bleeding. Regulatory bodies around the world are now advocating that HT should be prescribed only in the smallest dose and for the shortest possible time ( Europ Med Ag 2006 ; UK MHRA 2007 ). Potential health risks associated with HT and further uncertainty surrounding actual benefits to be gained from it have caused many women to seek non\u2010medical alternatives ( Bair 2005 ; Newton 2002 ). 'Natural' therapies appear to be very popular among women; a survey of 866 women 45 to 65 years of age reported that 61% agreed or strongly agreed with the statement that natural approaches are better than hormone pills for menopausal symptoms ( Newton 2002 ). In a national survey on women's use of complementary alternative medicine (CAM), more than 50% of CAM users indicated that such use was consistent with their beliefs, and 55% said that they wanted a natural approach to treatment ( Chao 2006 ). However, sufficient research on the risks and benefits of these approaches is lacking. A survey of women seen at a university clinic reported that 70% of women taking dietary supplements did not inform their doctors about their use, and only 4% had received information about such supplements from a healthcare provider ( Mahady 2003 ). In a national survey, when women using CAM for menopausal symptoms consulted a doctor, their disclosure rate (of CAM) was much higher, with only 36% of women reporting that they did not disclose their self treatment with CAM to their doctors ( Wade 2008 ). Therapies based on phytoestrogens are among the most common alternatives to HT. Phytoestrogens are nonsteroidal plant compounds of diverse structure that are found in many fruits, vegetables and grains ( Knight 1996 ; Thompson 1991 ). The most common types of phytoestrogens are coumestans, lignans and isoflavones. These compounds structurally resemble oestradiol (E2) and are shown to have weak oestrogenic activity ( Makela 1994 ; Setchell 1998 ). When ingested in relatively large quantities, dietary phytoestrogens have been shown to have significant biological effects in several animal species ( Adlercreutz 1995 ) and in humans ( Wilcox 1990 ). In humans, they appear to have both oestrogenic and anti\u2010oestrogenic effects, depending on the concentrations of circulating endogenous oestrogens and oestrogen receptors ( Bolego 2003 ). Isoflavones are among the most oestrogenically potent phytoestrogens; the major dietary isoflavones, genistein and daidzein, are found almost exclusively in legumes such as soy, chick peas, lentils and beans ( Cassidy 1993 ). Urinary excretion of equol, a weak oestrogen, in humans eating soy\u2010supplemented diets can greatly exceed the concentration of urinary endogenous oestrogens; this enhances the plausibility of human physiological health effects ( Setchell 1984 ). Other classes of phytoestrogens\u2014lignans and prenylated flavonoids\u2014also have potent oestrogenic activity but are not as well studied ( Adlercreutz 1987 ; Milligan 1999 ). Soy, a particularly abundant source of isoflavones, is a staple ingredient in the traditional Asian diet. It is postulated that high intake of soy among Asian women may account for lower rates of some menopausal symptoms in this group. Asian populations, such as those in Japan, Taiwan and Korea, are estimated to consume 20 to 150 mg per day of isoflavones, with a mean of about 40 mg from tofu (soy bean curd) and miso (soy bean paste). Soy includes such products as tofu, miso, aburage (fried thin tofu) and fermented or boiled soy beans. Further evidence that soy might be beneficial is suggested by a cohort study of Japanese women ( Nagata 2001 ), which found a significant inverse association between frequency of flushes and higher levels of soy consumption. However, the findings of this study are contradicted by data from a cross\u2010sectional study, which found that women who frequently consumed soy products were not less likely to report hot flushes or night sweats than women who never consumed soy products ( Sievert 2007 ). Thus it is not clear whether frequent soy consumption explains the lower rate of hot flushes among different ethnic groups. Red clover ( Trifolium pratense ), another source of isoflavones, contains compounds that are metabolised to genistein and daidzein after consumption. The most studied red clover product is Promensil. Potential adverse effects of phytoestrogens have included deficits in sexual behaviour in rats and impaired fertility in livestock ( Bennetts 1946 ). No specific examples of toxicity among humans have been noted in countries in which soy is consumed regularly ( Setchell 1997 ). It is generally considered difficult for humans to consume the quantity of isoflavones from natural soy foods needed to reach toxicological levels that induce pathological effects, as recorded in animals. How the intervention might work No clear explanation is known for how phytoestrogens might work in reducing hot flushes among perimenopausal and postmenopausal women. It has been suggested that phytoestrogens act as selective oestrogen receptor modulators (SERMs), exerting anti\u2010oestrogenic effects in the high\u2010oestrogen environment of premenopause and oestrogenic effects in the low\u2010oestrogen environment of postmenopause, where they act as weak agonists by stimulating oestrogen receptors ( Seibel 2003 ). Phytoestrogens appear to show greater affinity for the oestrogen receptor beta (ER\u03b2) than for the classical oestrogen receptor alpha (ER\u03b1). As a result, they preferentially express oestrogenic effects in the central nervous system, blood vessels, bone and skin without causing stimulation of the breast or uterus ( Kuiper 1997 ). Thus, phytoestrogens may reduce vasomotor symptoms through their action on the vascular system without causing unwanted oestrogenic effects on other body systems. Why it is important to do this review Current use of phytoestrogen products among perimenopausal and postmenopausal women with vasomotor symptoms is high; an American cross\u2010sectional analysis of more than 2,000 women (Study of Women's Health Across the Nation (SWAN)) reported that 11% of women with vasomotor symptoms used flaxseed products and 19% used soy products ( Gold 2007 ). Several reviews have examined the efficacy of phytoestogen products in alleviating menopausal symptoms, but most have found no benefit or a very slight reduction in the frequency of daily hot flushes compared with placebo. Government agencies and healthcare organisations have also scrutinised the effects of phytoestrogens, particularly isoflavones ( AFSSA 2005 ; Com Tox 2003 ). The North American Menopause Society (NAMS) position statement on the treatment of menopause\u2010associated vasomotor symptoms suggests that women should consider isoflavone supplementation if their menopausal flushing does not respond to other interventions ( NAMS 2004 ; NAMS 2011 ). However, NAMS acknowledges that the evidence base for this recommendation is poor. Thus, the aim of this review is to synthesise all available evidence on the efficacy, safety and acceptability of products containing phytoestrogens to assist women with vasomotor menopausal symptoms to reduce their symptoms by making good evidence\u2010based treatment decisions.",
        "summary": "There is no reliable randomized controlled trial (RCT) evidence to suggest that the use of phytoestrogens results in an effective reduction in the frequency or severity of vasomotor symptoms in peri\u2010 or post\u2010menopausal women. RCTs including more than 4000 participants compared dietary soy, soy extracts, genistein, promensil and other phytoestrogens with no treatment, placebo or hormone therapy. For the most part, phytoestrogens seemed to have no benefit in terms of vasomotor symptoms in peri\u2010 or post\u2010menopausal women. However, no pooling of data, and therefore no overall quantitative analysis, was possible for most of the comparisons, mainly due to substantial variation in the isoflavone concentrations and time of use and mix of constituents of each phytoestrogen intervention. Therefore, although there were relatively high numbers of women in total in assessments of several outcomes, individual trials were small and unlikely to detect clinically meaningful differences between groups if these were present. Four trials, not combined in a meta\u2010analysis, suggested that extracts with high (>30 mg/day) levels of genistein reduced the frequency of hot flushes. The only comparison for which studies were sufficiently similar for pooling was promensil compared with placebo; most analyses showed similar rates in the incidence, or the percentage reduction, of hot flushes in both groups, but all of the analyses had small numbers of women and were therefore likely to lack statistical power. There was no convincing RCT evidence to indicate that phytoestrogens were associated with estrogenic stimulation of the endometrium or the vagina, or other adverse effects, when used for up to two years, however, the studies were likely to be too small to detect clinically meaningful differences even if these were present."
    },
    "CD003313": {
        "query": "What are the benefits and harms of immunoglobulin for neonates with alloimmune hemolytic disease/jaundice?",
        "document": "Background Description of the condition The use of anti\u2010D immunoglobulin prophylaxis in D\u2010negative women has led to a marked decline in Rh hemolytic disease of the newborn (HDN) ( Urbaniak 2000 ). Sensitization can occur despite immunoprophylaxis, particularly if it is given too late or in insufficient dose. A proportion of HDN is caused by antibodies to antigens other than D and is, therefore, not preventable with anti\u2010D immunoglobulin. Fetal therapy has significantly improved outcome in Rh sensitized fetuses, but it does not comprehensively prevent need for neonatal treatment ( van Kamp 2004 ). Primary modes of postnatal therapy include phototherapy and exchange transfusion (ET) to reduce risk of mortality and kernicterus. Top\u2010up transfusions are used to treat early and late anemia. In contemporary perinatal centers, 15% to 40% of neonates admitted for Rh or ABO HDN require at least one ET ( Steiner 2007 ; Smits\u2010Wintjens 2011 ). The safety of ET has been reported for over 50 years. Published mortality rates vary from 0.53% to 4.7% per infant ( Boggs 1960 ; Panagopoulos 1969 ; Keenan 1985 ; Guaran 1992 ; Jackson 1997 ; Patra 2004 ; Badiee 2007 ). ET\u2010related death is more common in sick or premature infants than in healthy term infants ( Boggs 1960 ; Keenan 1985 ; Jackson 1997 ; Steiner 2007 ). Risks related to ET include adverse cardiorespiratory events; catheter\u2010related complications; those related to the use of blood products; metabolic derangements; and other serious complications such as pulmonary hemorrhage, necrotizing enterocolitis and bowel perforation. In the last two decades, ET\u2010related risks have been reported to be as high as 74%, although the incidence of severe adverse events is approximately 3\u201010% ( Ip 2004 ; Patra 2004 ; Badiee 2007 ; Steiner 2007 ). Because improved perinatal care has reduced the need for ET, the complication rate could increase as clinicians become less experienced with the procedure ( Steiner 2007 ). However, Steiner 2007 reported that over a 21\u2010year period, despite a sharp decline in the number of ETs performed, there was no increase in morbidity and mortality. Description of the intervention Intravenous immunoglobulin (IVIg) is an alternative therapy that may be effective in treating alloimmune HDN. In 1987, the first report of successful treatment of late anemia due to E\u2010incompatibility with IVIg was published ( Hara 1987 ). Subsequent case reports and case series reported success of IVIg treatment in neonates with both Rh or ABO incompatibility ( Kubo 1991 ; Sato 1991 ; Ergaz 1993 ). However, Hammerman 1996a found a reduced or no response to IVIg treatment in infants with ABO incompatibility who had early and severe hemolysis. Since the early 1990s, several quasi\u2010randomized or randomized controlled trials on the use of IVIg (including variations on timing of administration and dose) to reduce ET have been published ( Alpay 1999 ; Da\u011fo\u011flu 1995 ; Elalfy 2011 ; Miqdad 2004 ; Nasseri 2006 ; R\u00fcbo 1992 ; Santos 2013 ; Smits\u2010Wintjens 2011 ; Tanyer 2001 ; Atici 1996 ; Garcia 2004 ; Girish 2008 ; Hematyar 2011 ; Huang 2006 ; Liu 2016 ; Pishva 2000 ; R\u00fcbo 1996 ; Spinelli 2001 ; Voto 1995 ; Wang 2002 ). The potential benefits of IVIg over ET include that the treatment is less complicated and less labor intensive. In addition, IVIg could allow safe treatment of some infants in less sophisticated neonatal units, or avoid delaying treatment while transferring infants for ET. Comprehensive assessment of IVIg in premature infants, particularly in the treatment of sepsis, has shown that it is safe and well tolerated ( INIS Collaborative Group 2011 ). It is a well\u2010established therapy for alloimmune thrombocytopenia due to maternal and fetal human platelet antigen incompatibility ( Winkelhorst 2017 ). The risk of transmission of viral infection is extremely low ( Fischer 1988 ). Hemolysis and acute renal failure are uncommon complications of IVIg treatment ( Copelan 1986 ). One study showed an increased incidence of sepsis in premature infants receiving prophylactic IVIg ( Magny 1991 ). Since about 2010, several cases of necrotizing enterocolitis in infants with HDN treated with IVIg have been reported ( Figueras\u2010Aloy 2010 ; Corvaglia 2012 ; Yang 2016 + ). Other rare serious adverse effects of IVIg have been described in pediatric and adult cohorts, but not in newborns ( Kumar 2006 ). How the intervention might work IVIg might reduce the rate of hemolysis in alloimmune HDN by nonspecific blockade of Fc\u2010receptors on the macrophages that are thought to mediate the destruction of antibody\u2010coated red cells ( Urbaniak 1979 ). Ergaz 1995 demonstrated a decline in carboxyhemoglobin levels in four of five infants treated with IVIg for alloimmune HDN. Hammerman 1996b demonstrated a significant reduction in carboxyhemoglobin levels in 19 of 26 Coombs\u2010positive infants treated with IVIg. Carboxyhemoglobin levels are a sensitive index of hemolysis and hence these studies suggest that immunoglobulin could decrease hemolysis. IVIg is typically formulated in 6% to 12% solutions, so at doses of 0.5 g/kg to 1 g/kg the volume administered is 4 mL/kg to 16 mL/kg. It is possible that this is a sufficient fluid bolus to reduce bilirubin levels modestly through dilution, temporarily slowing their rate of rise and allowing more time for intensive phototherapy to have effect. Why it is important to do this review This is an update of a Cochrane Review first published in 2002. Although results of the previous review showed a significant reduction in the need for ET in infants treated with IVIg, the applicability of the results was limited because none of three included studies was at low risk of bias. Nevertheless, American Academy of Pediatrics (AAP) guidelines recommend the administration of 0.5 g/kg to 1 g/kg IVIg in alloimmune HDN if total serum bilirubin (TSB) is rising despite intensive phototherapy or if TSB level is within 34 \u00b5mol/L to 51 \u00b5mol/L (2 mg/dL to 3 mg/dL) of exchange level ( AAP 2004 ). As a result of these guidelines, despite the equivocal conclusions of the previous Cochrane Review, the use of IVIg in alloimmune HDN has become widespread in many countries. However, supplies of IVIg are limited and it does present some hazards. Therefore, use of IVIg should be restricted to treatment of conditions for which it is of confirmed benefit.",
        "summary": "Intravenous immunoglobulin (IVIg) may improve outcomes for neonates with alloimmune hemolytic disease without increasing adverse effects, but the evidence is only of very low to low certainty. Very low\u2010certainly evidence suggests that fewer newborn infants with alloimmune hemolytic disease/jaundice required exchange transfusion when treated with IVIg than placebo/no IVIg (on average, 115 vs 329 per 1000 infants); the number of exchange transfusions per infant was also lower with IVIg (on average, by 0.34 exchange transfusions). In addition, maximum total serum bilirubin may be lower (on average, by 25 \u00b5mol/L; very low\u2010certainty evidence) and duration of phototherapy shorter (on average, by 0.98 days) with IVIg. Researchers observed no clear differences between IVIg and placebo/no IVIg in the need for, or volume of, top\u2010up transfusions during or beyond the first week of life (very low\u2010 to low\u2010certainty evidence), nor in the incidence of longer\u2010term (> 1 year) neurological effects (based on 80 and 204 infants). Researchers observed no IVIg\u2010related adverse effects; adverse effects secondary to exchange transfusion use (hypoglycemia, hypocalcemia, sepsis, inspissated bile syndrome) were observed, but event rates were low. Neonatal mortality has not been studied."
    },
    "CD005268": {
        "query": "In people with type 2 diabetes mellitus, does individual patient education improve outcomes?",
        "document": "Background Description of the condition Diabetes mellitus is a disorder in the metabolism of blood glucose. It occurs when the body no longer responds effectively to endogenous insulin or when the body's production of insulin is inadequate. This leads to chronic hyperglycaemia (elevated levels of plasma glucose) accompanied by abnormal metabolism of carbohydrate, fat and protein. In the long term, people with diabetes are predisposed to complications which include retinopathy, nephropathy and neuropathy. The risk of cardiovascular disease is also substantially increased. For a detailed overview of diabetes mellitus, please see under 'Additional information' in the information on the Metabolic and Endocrine Disorders Group in The Cochrane Library (see 'About', 'Cochrane Review Group (CRGs)'). For an explanation of methodological terms, see the main glossary in The Cochrane Library . Type 2 diabetes is one the commonest chronic diseases globally and is closely entwined with the obesity epidemic. The International Diabetes Federation reported an estimate of 194 million people with diabetes in 2003 and predicted that this will increase to 333 million in 2025 ( Diabetes Atlas 2005 ) and similar projections have been made by the World Health Organization ( Wild 2004 ). Most of the increase in diabetes prevalence is predicted to occur in the developing countries where by 2025 more than 75% of the people with diabetes under the age of 65 will reside ( King 1998 ). Hence, in addition to the direct disease costs, diabetes will place an enormous additional burden on these countries through productivity losses. The financial cost of diabetes is immense and increasing. The national costs of diabetes for 2002 in the US is estimated at US$ 92 billion to US$ 132 billion (approx. 72 to 103 billion EUROS) in direct medical expenditure and US$ 40 billion (approx. 31 billion EUROS) for indirect costs and is estimated to increase to US $192 billion (approx. 150 billion EUROS) in 2020 ( ADA 2003 ). Complications are the main driver of all types of diabetes costs increasing the annual cost of diabetes in Australia from US$ 3220 (2523 EUROS) in people without complications to US$ 7715 (6044 EUROS) for people with both microvascular and macrovascular complications. European studies, ie CODE\u20102 ( Jonsson 2002 ) and the UK T2ARDIS study ( Holmes 2003 ) showed similar effects with quality of life glycaemic average scores also significantly reduced by complications. Description of the intervention These findings demonstrate the high financial burden associated with diabetes and its complications that may, a least in part, be attributable to deficiencies in self management. Education that increases patients' understanding of diabetes can prevent or delay complications and reduce the number and duration of hospitalisations, which in turn can improve quality of life glycaemic. Different methods of teaching people self\u2010management practices may affect long\u2010term compliance, relative to many other conditions, the impact of self\u2010management on diabetes outcomes is important not only to the individual but to the community in the form of health care costs ( Glasgow 1999 ). In the USA it has been found that medical non\u2010compliance in people with type 2 diabetes imposes a financial burden in the order of 100 billion dollars (approx. 78 billion EUROS) each year ( Vermeire 2003 ). Consequently, patient education is recognized globally as an essential component of diabetes management but its relationship to health outcomes and health service utilization is still not well understood ( Corabian 2001 ; Ellis 2004 ; Loveman 2008 ) and reports of its effectiveness are variable ( Loveman 2008 ). Nonetheless, due to the onerous requirements for self\u2010care that demands multiple daily decisions in order to balance diet, physical activity and medications, it is widely accepted that diabetes education is not only required in the first few months following diagnosis but is an important component of ongoing diabetes care ( Loveman 2003 ). The question, then, is not so much about whether or not people with diabetes should have diabetes education but about which methods and models of education produce the best effect on behaviour change, self management, and physical and psychological outcomes. An important Health Technology Assessment on patient education for type 2 diabetes by Corabian and Harstall ( Corabian 2001 ) points out that a particular difficulty in assessing educational interventions is the lack of well defined long\u2010term outcomes, and this is supported by Loveman et al ( Loveman 2008 ). However, since then the American Association of diabetes educators has published criteria for behaviour change as a result of education ( AADE 2003 ) and Australia has developed a national consensus on outcomes and indicators for diabetes patient education ( Eigenmann 2007 ). Patient empowerment, as championed by Funnell 2004 has been gathering popularity as an educational model since the 1980s and has been shown to produce benefits such as improved communication with providers, greater satisfaction with care, improvements in metabolic and psychological outcomes and quality of life. More recent education models such as Diabetes X\u2010PERT ( Deakin 2006 ) and DESMOND ( Davies 2008 ) are representative of current efforts to better define, structure and evaluate the effect of diabetes education. However, these reports, like the majority of research reports in the peer reviewed literature about diabetes patient education focus on group education. A systematic review by Norris and colleagues ( Norris 2001 ) found evidence that self management training in people with type 2 diabetes was effective. Despite inclusions of some studies assessing individual patient education, the majority of studies included in the review focused on group education. A Cochrane Review by Deakin and colleagues ( Deakin 2005 ) also concluded that adults with type 2 diabetes improved diabetes control and knowledge of diabetes after group\u2010based training programmes. Why it is important to do this review Individual education is a common form of patient education for people with diabetes but, to date, there has been no formal review of the effects of individual patient education on outcomes in people with type 2 diabetes. The availability of this information is critical to informing service and resource allocation decisions relating to individual patient education.",
        "summary": "It has been the generally held viewpoint of clinicians, as well as the recommended positioning of consensus guideline groups (e.g., American Diabetic Association), that diabetes education is an essential component of any comprehensive plan to manage diabetes, predicated on the belief that educational investment results in improved control of diabetes and better outcomes. Based on 4 studies in 632 participants, individual patient education did not improve glycemic control over a 12 to 18 month period compared with usual care (mean difference \u22120.08%, 95% CI \u22120.25% to 0.08%). However, people with HbA1c greater than 8% at baseline benefited (mean difference \u22120.31, 95% CI \u22120.54 to \u22120.09). This subgroup represents around 30\u221250% of people seen in the primary care setting in the US. Studies comparing individual with group education found that, at 6\u22129 months, individual education improved glycemic control with a mean difference between groups of 0.8% (95% CI 0.3 to 1.3) but this difference was not sustained at 12 to 18 months. It is disappointing that diabetes education has not been documented to improve outcomes relative to weight control, lipids, blood pressure, or other diabetic endpoints. The evidence is limited to smaller studies of short (2\u22124 hours) educational interventions, typically conducted for less than 1 year. One might question whether longer term follow\u2010up, during which patients become progressively more familiar with their diabetes, its management, and its consequences, would provide more meaningful insight into better outcomes of educational interventions. Hopefully, future long term studies will confirm the efficacy of diabetic education in areas other than glucose control outcomes."
    },
    "CD009578": {
        "query": "In women with gestational diabetes mellitus, do reminder systems increase uptake of testing for type 2 diabetes or impaired glucose tolerance?",
        "document": "Background Description of the condition Diabetes mellitus is a metabolic disorder resulting from a defect in insulin secretion, insulin action or both. A consequence of this is chronic hyperglycaemia (i.e. elevated levels of plasma glucose) with disturbances in carbohydrate, fat and protein metabolism. Long\u2010term complications of diabetes mellitus include retinopathy, nephropathy and neuropathy. The risk of cardiovascular disease and cancer is increased. Being pregnant is a state that creates a degree of metabolic stress, which can include an increase in insulin resistance ( Ratner 2007 ). For some women this results in glucose concentrations high enough for a diagnosis of gestational diabetes mellitus (GDM) to be made. Although these high glucose concentrations usually normalise immediately after birth, women who have experienced GDM are at increased risk of developing type 2 diabetes in the future ( Conway 1999 ; Hunt 2008 ; Retnakaran 2008 ; Retnakaran 2011 ; Schaefer\u2010Graf 2002 ). Both GDM and type 2 diabetes share the two main metabolic defects of insulin resistance and \u00df\u2010cell dysfunction ( Retnakaran 2008 ). In fact GDM could be regarded as \"type 2 diabetes unmasked by pregnancy\" ( Bottalico 2007 ). Approximately 7% of pregnancies in the USA are complicated by GDM ( Nicholson 2008 ), partly due to increasing rates of obesity ( Kim 2010 ). In Australia, the prevalence of GDM is 5% ( AIHW 2010 ). It is important to note that the prevalence of GDM is influenced by methods of detection and diagnosis, which differ across the world ( ACOG 2013 ; ADA 2013 ; Hoffman 1998 ). For example, following the recent Hyperglycemia and Adverse Pregnancy Outcomes (HAPO) study ( HAPO 2008 ), the recommendation to lower the diagnostic threshold for GDM will result in 18% of pregnant women being diagnosed with this condition ( Metzger 2010 ), nearly trebling the yield of many current methods of diagnosing GDM. Because of variations in diagnostic thresholds, a standard set of diagnostic criteria cannot yet be applied for identifying women with GDM. Women who have experienced GDM are over seven times more likely to develop type 2 diabetes than women with normal glycaemic concentrations in pregnancy ( Bellamy 2009 ). Cumulative incidence rates of type 2 diabetes range from 30% to 62% in the first five years after giving birth in a woman with previous GDM, and appear to plateau after 10 years ( Kim 2002 ). The risk of developing type 2 diabetes is proportional to the degree of hyperglycaemia during pregnancy ( Retnakaran 2008 ), with factors such as impaired glucose tolerance, needing insulin to manage GDM, prepregnancy obesity, high\u2010density lipoprotein\u2010cholesterol levels less than 50 mg/dL and age older than 35 years all being predictors of diabetes after GDM ( G\u00f6bl 2011 ; Nicholson 2008 ). The International Association of the Diabetes and Pregnancy Study Group has recently described a new category of \"overt diabetes in pregnancy\", in women with high results for glycated haemoglobin A1c (HbA1c), fasting glucose or oral glucose tolerance tests in early pregnancy \u2010 although this condition cannot be equated with underlying diabetes. A retrospective audit of women with overt diabetes in pregnancy has shown 21% to have type 2 diabetes and 38% to have impaired fasting glucose/impaired glucose tolerance in the early postpartum period ( Wong 2013 ). In addition to the increased risk of later type 2 diabetes, women diagnosed with GDM are also at increased risk of recurrent GDM in subsequent pregnancies. Rates of recurrence of GDM range from 30% to 84%, with some of these cases likely to be unrecognised (pregestational) type 2 diabetes ( Bottalico 2007 ; Getahun 2010 ; Kim 2007 ). Description of the intervention Many international professional and government clinical practice guidelines or consensus statements recommend that women who had GDM in their most recent pregnancy receive an oral glucose tolerance test (OGTT) between 6 to 12 weeks postpartum to detect type 2 diabetes ( ACOG 2013 ; Metzger 2007 ; RANZCOG 2011 ; Simmons 2002 ). Because of the high risk of future diabetes, these women are often advised to undergo retesting on a regular basis ( Metzger 2007 ; Metzger 2010 ; NICE 2008 ; RANZCOG 2011 ; Simmons 2002 ). There is a large gap between these recommendations for postpartum testing and practice. Even though a history of GDM provides a natural prompt to commence screening for type 2 diabetes ( Bellamy 2009 ), most women are not tested. Reported rates of testing vary from 5% to 60%, with probably only 20% to 40% of women with previous GDM having some form of postpartum glucose test ( Clark 2009 ; Conway 1999 ). In a large US study of nearly one million pregnant women using commercial diagnostic services, 19% (4486/23,299 women diagnosed with GDM) had a postpartum diabetes test within six months of giving birth. However, in this study overall uptake would have been somewhat less than 19% as only two\u2010thirds of the pregnant women in the study were screened for GDM ( Blatt 2011 ). Reasons given for not having a postpartum OGTT include: a perception that GDM resolves completely after pregnancy; the emotional stress and time demands of a new baby; the inconvenience of the test; fear of receiving a diagnosis of diabetes; and lack of continuity of postpartum care ( Bennett 2011 ; Hunt 2008 ; Keely 2010 ). Reminder systems have been shown to be effective in many areas of health care, including diabetes ( Weingarten 2002 ). In a systematic review of 54 studies addressing postpartum testing rates among women with a history of GDM, studies of proactive contact with women nearly doubled the testing rates reported in studies of usual care (from an average of 33% to 60%). Proactive contact in this review included phone calls, education programmes and postal reminders ( Carson 2013 ). Although not conducted specifically in pregnant or postpartum women with diabetes or a history of diabetes, other systematic reviews have demonstrated that clinician reminders can modestly increase rates of preventative care ( Dexheimer 2008 ) and healthcare performance in general ( Grimshaw 2006 ). Thus, reminder systems for women or health professionals (or both) may increase the uptake of postpartum glucose tests. Preventative care reminders have usually been in the form of mailed letters or direct phone calls, with email and mobile phone (SMS (short message service)) messaging now beginning to be used and to show some benefits ( Car 2012 ). A voluntary national gestational diabetes register, recently established in Australia ( http://www.ndss.com.au ), is issuing annual reminders to women who have experienced GDM and joined the scheme. An evaluation of its predecessor, the South Australian Gestational Diabetes Mellitus Recall Register, indicates the future potential of registration and follow\u2010up reminders to increase the uptake of glucose tests and therefore the early detection of type 2 diabetes ( Chittleborough 2010 ). While a reminder intervention is not envisaged to lead to adverse effects, there is the possibility that reminders may be regarded as intrusive by some people and may even be a source of anxiety. How the intervention might work The purpose of postpartum screening of women with previous GDM is to promptly identify those women who will subsequently develop type 2 diabetes. Early identification allows earlier management through preventative strategies such as diet modification, exercise and avoiding excessive weight gain ( Nield 2008 ; Norris 2005 ; Orozco 2008 ). Sometimes oral glucose\u2010lowering drugs or insulin may be added to such lifestyle changes. In a subgroup analysis of the Diabetes Prevention Program, both intensive lifestyle interventions and metformin were effective in delaying or preventing diabetes in women with impaired glucose tolerance and a history of GDM ( Ratner 2008 ). However, the beneficial effects of these preventive measures will not be realised unless women with previous GDM are screened postpartum, are offered appropriate management and follow up, and then agree to make lifestyle changes. Clinicians and women regard reminder systems for postpartum type 2 diabetes screening as important and useful ( Keely 2010 ), and so reminders are likely to be able to address some of the awareness and behavioural barriers that women face when making lifestyle changes after giving birth, leading to women with a history of GDM being able to avoid a diagnosis of type 2 diabetes in the future. Why it is important to do this review The incidence of GDM indicates the underlying frequency of type 2 diabetes, with both types of diabetes rising throughout the world ( Bellamy 2009 ). The early postpartum period is an important time in which to identify the risk of diabetes in women with a history of GDM or milder glucose intolerance in pregnancy ( Retnakaran 2008 ) and to translate postpartum testing into practice ( Oza\u2010Frank 2013 ). In fact, some researchers posit that prevention of subsequent type 2 diabetes may be the most compelling reason to diagnose GDM ( Keely 2012a ). For a majority of women with a history of GDM, the opportunity to prevent subsequent type 2 diabetes is currently missed, as is the chance to detect any problems and intervene to prevent future diabetic complications such as cardiovascular disease ( Kitzmiller 2007 ; Shah 2008 ) and future metabolic dysfunction ( Stuebe 2011 ), and also the chance to reduce the risk of diabetes in their children ( Clausen 2008 ; Dabelea 2011 ). Early detection may also reduce healthcare costs \u2010 in a Swedish longitudinal study, women diagnosed with diabetes after GDM had a more than 14\u2010fold likelihood of healthcare utilisation, with an annual healthcare cost 101% higher than in controls ( Anderberg 2012 ). This review evaluates the effects of reminder strategies to identify all possible women with previous GDM, follow them up, and offer them appropriate management and treatment. A Cochrane review assessing the effects of interventions to prevent type 2 diabetes in women with previous GDM is currently being prepared ( Wendland 2011 ).",
        "summary": "When a postal reminder for glucose tolerance testing was sent at around 3 months after giving birth to women who had attended an obstetrical unit for treatment of gestational diabetes mellitus, or to the woman and their doctor, low\u2010quality evidence showed that more women underwent an oral glucose tolerance test (on average 516 to 604 [depending on whether the reminder was sent to patient or doctor] versus 143 per 1000 women), or a fasting glucose test (on average 628 to 712 versus 400 per 1000 women), compared with women who received no such reminder. The analyses included small numbers of women (66 to 116), making it difficult to reliably detect differences between groups. Clinically important outcomes (proportion of women with post\u2010partum type 2 diabetes, impaired glucose tolerance, or impaired fasting glucose, quality of life, diabetes\u2010associated morbidity) were not reported in the trials."
    },
    "CD010463": {
        "query": "How do different chemotherapy regimens compare with each other for improving outcomes in elderly patients with advanced non\u2010small cell lung cancer?",
        "document": "Background Description of the condition Worldwide, lung cancer is the most common malignancy among men and the second most common among women, with an estimated 1.6 million new cases in 2008; it is responsible for most of the cancer\u2010related deaths reported in both sexes ( American Cancer Society 2011 ). It is estimated that in 2012, 56% of new cases were diagnosed at advanced stages of disease. Therefore, a large number of patients will be candidates for palliative chemotherapy. Approximately 50% of patients newly diagnosed with non\u2010small cell lung cancer (NSCLC) are older than 70 years of age at diagnosis ( Davidoff 2010 ). Despite this fact, these patients are underrepresented in randomized controlled trials (RCTs), resulting in lack of reliable information about treatment effectiveness and safety for patients in this age group ( Hutchins 1999 ; Talarico 2004 ). In clinical practice, this lack of information has led many clinicians to deliver suboptimal treatment based on the presumption of poor tolerance of treatment ( Quoix 2011a ). Recognition of this limitation has prompted investigators to design randomized studies specifically focused on this population; nonetheless, the best way of treating this important group of patients remains to be determined. Description of the intervention For patients with advanced NSCLC with a good performance status (PS), platinum regimens are considered standard first\u2010line treatment. However, debate about the most appropriate regimen for older patients is ongoing. Only recently, few RCTs allowed inclusion of elderly patients. A subgroup analysis of a meta\u2010analysis of individual participant data from 16 RCTs addressed the role of chemotherapy for this subgroup. Elderly individuals accounted for 26.9% of all participants, and analyses suggested similar benefit across younger and older participants, confirming the benefit of chemotherapy over best supportive care (BSC) ( NSCLC Collaborative Group 2010 ; NSCLC Meta\u2010Analyses Collaborative Group 2008 ). Nevertheless, concern about specific issues related to the older patient has led to trials specifically addressing the issue of chemotherapy in this population. One of the first RCTs to evaluate the role of chemotherapy in older patients was stopped early because of poor accrual; investigators randomly assigned 191 participants older than 70 years of age to vinorelbine monotherapy or BSC ( Gridelli 2001 ). This study showed better overall survival (OS) in the treatment arm than in the BSC arm. Since that time, other RCTs have sought the most appropriate regimen for this population by examining the role of different cytotoxic single agents and combined chemotherapy agents containing or not containing platinum. How the intervention might work Cytotoxic chemotherapy comprises a variety of drugs with different mechanisms of action, which are aimed at stopping cell division and consequently tumor growth. Cytotoxic chemotherapy has been selected as the main treatment for a variety of solid tumors, reducing risk of death and disease progression. However, it is also associated with numerous adverse events, which may be more common among older patients with significant co\u2010morbidities that affect their ability to tolerate and continue with treatment. Why it is important to do this review Today, no chemotherapy regimen is accepted as the standard of care for elderly patients. The best treatment approach for elderly patients with advanced NSCLC must be carefully balanced between efficacy and safety. The impact of more active regimens containing platinum compounds or newer drugs with better toxicity profiles remains to be defined with regard to benefits for survival and quality of life (QoL). A systematic review of RCTs for this group of patients is crucial and warranted.",
        "summary": "Survival and response rates are better when a platinum\u2010based chemotherapy regimen rather than a non\u2010platinum\u2010based regimen is given to adults with advanced non\u2010small cell lung cancer (NSCLC), but hematological adverse events may be increased. The evidence for combination non\u2010platinum\u2010based chemotherapy regimens compared with a single agent non\u2010platinum regimens is too low quality to draw conclusions. When platinum\u2010based chemotherapy was used compared with non\u2010platinum\u2010based therapy, moderate\u2010quality evidence showed that overall survival was better in good performance status (WHO or ECOG 0 to 2) adults with stage III or IV NSCLC) (the difference in median survival time ranged from \u20106.3 to +4.6 months across trials, with 9/14 trials reporting a longer median OS with platinum therapy). Progression\u2010free survival (median PFS times not reported; low\u2010quality evidence), 1\u2010year survival (on average 635 versus 714 died per 1000 people; moderate\u2010quality evidence) and objective response rate (342 versus 218 per 1000 people; moderate\u2010quality evidence) were also better with the platinum\u2010based chemotherapy. Results for quality of life were inconsistent across trials. In terms of adverse events, low\u2010quality evidence showed platinum\u2010based chemotherapy to be more commonly associated with grade 3/4 anemia (105 versus 41 per 1000 people) and thrombocytopenia (101 versus 28 per 1000 people). Rates of neutropenia and febrile neutropenia were similar for the two regimens. Platinum\u2010based chemotherapy also seemed to be more commonly associated with grade 3/4 fatigue (90 versus 58 per 1000 people), emesis (32 versus 9 per 1000 people) and peripheral neuropathy (31 versus 4 per 1000 people). When a combination of non\u2010platinum\u2010based chemotherapy agents was compared with a single agent non\u2010platinum regimen, low\u2010quality evidence suggested no apparent differences in overall survival, 1\u2010year survival), or progression\u2010free survival in adults with stage IIIB or IV NSCLC and good performance status (ECOG 0 to 2). More people receiving combination therapy seemed to have an objective response (on average 301 versus 168 per 1000 people). Results for quality of life were inconsistent across trials. Low to very low\u2010quality evidence suggested that there were no differences between the regimens in terms of hematological or non\u2010hematological adverse events."
    },
    "CD005305": {
        "query": "What are the effects of pulmonary rehabilitation after exacerbation in people with chronic obstructive pulmonary disease?",
        "document": "Background Clinical guidelines and documents of the American Thoracic Society (ATS) and the European Respiratory Society (ERS) include positive recommendations for pulmonary rehabilitation after chronic obstructive pulmonary disease (COPD) exacerbations based on earlier versions of this systematic review and its included trials ( BTS 2013 ; ERS ATS Statement 2013 ; GOLD 2016 ). However, recent studies indicate that post exacerbation rehabilitation may not always be effective. In addition, concerns have arisen that pulmonary rehabilitation may not be safe shortly after exacerbations of COPD. Therefore, our aim is to update our previous systematic review by assessing the effectiveness and safety of pulmonary rehabilitation after exacerbations of COPD. The protocol for this Cochrane review was based on a previously published non\u2010Cochrane systematic review ( Puhan 2005 ). Description of the condition Exacerbations and hospitalisations in patients with COPD represent a major health burden for both patients and healthcare systems in industrialised and developing countries ( Chan\u2010Yeung 2004 ; Kessler 2006 ; Seemungal 1998 ; Sin 2002 ; Sullivan 2000 ). Acute exacerbations are the most common reason for hospital admissions and death among patients with COPD ( Aaron 2014 ; Garcia\u2010Aymerich 2003 ; Mannino 2002 ; Piquet 2013 ; Soler\u2010Cataluna 2005 ). In addition, patients with COPD have reported reduced health\u2010related quality of life (HRQL) ( Kessler 2006 ; Schlenk 1998 ) compared with the healthy population, which is further impaired by acute and repeated exacerbations ( Seemungal 1998 ). Patients are at risk of early death and continued exacerbations requiring hospitalisation ( Aaron 2014 ; Piquet 2013 ; Soler\u2010Cataluna 2005 ). Mortality rates during the year following a hospitalisation are around 35% ( Almagro 2002 ; Connors 1996 ; Groenewegen 2003 ; Seneff 1995 ; Vitacca 2001 ), and rehospitalisation rates are around 60% ( Connors 1996 ; Cydulka 1997 ; Escarrabill 2014 ; Groenewegen 2003 ; Martin 1982 ). From the healthcare provider's perspective, COPD is resource\u2010consuming ( Ford 2015 ; Jansson 2013 ; Sullivan 2000 ). Acute exacerbations are the cost drivers for COPD care, accounting for more than 70% of COPD\u2010related costs incurred as the result of emergency visits and hospitalisations ( NHLBI 2001 ; Oostenbrink 2004 ; Sullivan 2000 ). Description of the intervention Position papers of the American College of Physicians, the American College of Chest Physicians, the Global Initiative for Chronic Obstructive Lung Disease (GOLD) and the National Institute for Health and Care Excellence (NICE) have provided recommendations on acute care and follow\u2010up management for acute exacerbations ( Amir 2011 ; GOLD 2016 ; NICE 2010 ). Pulmonary rehabilitation could play an important role in peri\u2010exacerbation management (management around the time of an exacerbation) because it combines several interventions that are known to improve health status and prognosis, such as physical exercise, smoking cessation, self\u2010management education, optimisation of medications and psychological and social support ( BTS 2013 ; ERS ATS Statement 2013 ; Maddocks 2015 ; Puhan 2014 ). A large body of evidence on patients with stable COPD shows that pulmonary rehabilitation improves exercise capacity and HRQL ( McCarthy 2015 ), and that it may be cost\u2010effective ( ERS ATS Statement 2013 ; Griffiths 2001 ). How the intervention might work A multi\u2010disciplinary approach to pulmonary rehabilitation addresses multiple risk factors for hospital readmission and determinants of poor exercise capacity and quality of life. This combined effect may accelerate recovery from exacerbations and lower the risk of hospital readmission by improving exercise capacity, alleviating symptoms and promoting better self\u2010management. Why it is important to do this review COPD exacerbations are a major burden for patients, caregivers and society. Evaluation of the effectiveness and safety of post exacerbation strategies such as pulmonary rehabilitation could substantially lower the disease burden.",
        "summary": "There is moderate to high quality evidence showing that pulmonary rehabilitation programs provide beneficial effects after exacerbation in people with chronic obstructive pulmonary disease (COPD). Subgroup analyses performed to assess extensive and less extensive rehabilitation programs separately suggest that benefits may occur consistently only in people undertaking extensive rehabilitation programs. Extensive rehabilitation, on average, consisted of at least 16 exercise training sessions, which included two to three exercise training sessions per week that involved at least endurance exercise (with or without strength training), as well as supervision of most exercise training sessions by physiotherapists or other trained health professionals. Extensive rehabilitation resulted in fewer hospital re\u2010admissions within nine months post discharge (on average, 397 vs 700 per 1000 people were admitted), lower mortality (72 vs 133), and improved health\u2010related quality of life (mean change in St George's Respiratory Questionnaire [SGRQ] total score 7.82 points). Trial results also show greater improvement from baseline in six\u2010minute walking distance with both extensive and less extensive rehabilitation and in shuttle walk test with extensive rehabilitation."
    },
    "CD012404": {
        "query": "How does automated monitoring compare with standard care for the early detection of sepsis in critically ill people?",
        "document": "Background Description of the condition Sepsis is a life\u2010threatening clinical syndrome. The criteria for the diagnosis of sepsis have evolved over time and are generally defined by international consensus groups ( ACCP/SCCM 1992 ; Levy 2003 ; Singer 2016 ). It is usually diagnosed when a patient has a suspected or documented infection, alongside systemic inflammatory response syndrome (SIRS). The criteria for diagnosing SIRS typically include the presence of two or more of the following abnormalities in the absence of other known causes, such as chemotherapy. Temperature greater than 38.3\u00b0C (hyperthermia) or less than 36.0\u00b0C (hypothermia) Heart rate greater than 90 beats per minute (tachycardia) Breathing greater than 20 breaths per minute (tachypnoea) or arterial carbon dioxide concentration (PaCO\u00b2) less than 32 mmHg (hyperventilation) Blood glucose greater than 7.7 mmol/L (hyperglycaemia) in the absence of diabetes mellitus New altered behaviour or mental state White blood cell count greater than 12,000 per microlitre (leukocytosis) or less than 4000 per microlitre (leukopenia) or normal white blood cell count with greater than 10% immature forms. Temperature greater than 38.3\u00b0C (hyperthermia) or less than 36.0\u00b0C (hypothermia) Heart rate greater than 90 beats per minute (tachycardia) Breathing greater than 20 breaths per minute (tachypnoea) or arterial carbon dioxide concentration (PaCO\u00b2) less than 32 mmHg (hyperventilation) Blood glucose greater than 7.7 mmol/L (hyperglycaemia) in the absence of diabetes mellitus New altered behaviour or mental state White blood cell count greater than 12,000 per microlitre (leukocytosis) or less than 4000 per microlitre (leukopenia) or normal white blood cell count with greater than 10% immature forms. If left untreated, sepsis can develop into severe sepsis (sepsis with organ dysfunction) or septic shock (severe sepsis with hypotension despite adequate fluid resuscitation). Mortality for this group of patients can be 40% or even higher depending on definitions used ( Szakmany 2018 ). Patients with sepsis often require admission to the intensive care unit (ICU). The incidence of sepsis in people admitted to ICU for other critical illnesses is also high (20% to 70% of people admitted to ICU in Europe, with considerable variance by country, Vincent 2006 ). Diagnosing sepsis is challenging and time consuming. It often requires the combination of information from several sources to be reviewed (e.g. patient history, laboratory data, and physiological data) at regular intervals ( Cohen 2015 ). Further, although many options are available to guide therapy ( Andriolo 2017 ), and many interventions have been tested ( Annane 2015 ; Borthwick 2017 ), early detection offers the prospect of a better therapeutic response. In addition, the complexity of diagnosis combined with the degree of illness results in a significant cost for treating sepsis in the ICU. For example, the cost of treating each patient with sepsis in the ICU was recently estimated as approximately EUR 29,000 in the Netherlands ( Koster\u2010Brouwer 2014 ), or GBP 20,000 in the UK ( UK Sepsis Trust 2013 ). Description of the intervention Automated monitoring systems provide a means of monitoring patient data continuously, and can facilitate the assembly of data from unconnected information systems ( Hooper 2012 ). These tools are variously referred to as alert systems, detection systems and monitoring systems ( Makam 2015 ). In essence, the systems process clinical data \u2010 that are routinely collected \u2010 to identify sepsis according to predetermined diagnostic thresholds, and include an electronic means of alerting staff. Although the algorithms (i.e. criteria) used to identify sepsis vary between the different automated systems ( Buck 2014 ; Nachimuthu 2012 ), their key feature is an ability to monitor one or more electronic systems (e.g. patient electronic health records) for potential indicators of sepsis. For example, a system may 'listen' for modified SIRS criteria ( Hooper 2012 ), although SIRS criteria have recently been deemed to have inadequate specificity and sensitivity for the detection of sepsis ( Singer 2016 ). Following detection of potential sepsis, the system should provide an automated notification (e.g. via email, phone message or pager) to the relevant physician or nurse, flagging the requirement for clinical evaluation and potential initiation of therapy ( Hooper 2012 ; Koenig 2011 ). The use of electronic early\u2010recognition tools has previously been validated in the critical care setting for detection of acute respiratory distress syndrome (ARDS) ( Koenig 2011 ). Potential adverse effects of automated systems might include the failure to detect sepsis and alarm fatigue (i.e. where frequent false alarms cause staff to ignore notification of potential sepsis). How the intervention might work Automated detection systems monitor patient data continuously to facilitate the early detection of sepsis in the ICU. The diagnosis of sepsis or septic shock is particularly time\u2010sensitive, as the length of time until initiation of appropriate antimicrobial therapy or fluid resuscitation is a critical determinant of survival in these patients ( Dellinger 2013 ; Kumar 2006 ; Rivers 2001 ; Yealy 2014 ). Therefore, guidelines recommend early fluid resuscitation of the septic patient within six hours of recognition of sepsis, and administration of broad\u2010spectrum antibiotics within one hour of the recognition of septic shock or severe sepsis without septic shock ( Dellinger 2013 ). Automated detection systems offer the possibility of monitoring patients in 'real time' ( Meurer 2009 ), and can alert the relevant physicians or nurses (e.g. by email or pager) to the need for timely clinical evaluation and potential initiation of treatment. Why it is important to do this review Although the rate of mortality from sepsis has improved ( Kaukonen 2014 ; McPherson 2013 ), national audits indicate that clinical standards relevant to the management of patients with sepsis are not being met, despite ongoing education programmes ( CEM 2012 ). The UK Parliamentary Ombudsman recently published a detailed report that identified common themes in 10 case studies of patients that died following sepsis ( Parliamentary Ombudsman 2013 ). Failings were identified throughout the care pathway, from carrying out a timely initial assessment and identifying the source of infection, to adequate monitoring and timely initiation of treatment ( Parliamentary Ombudsman 2013 ). Automated monitoring systems for the detection of sepsis may facilitate earlier detection and treatment of sepsis in the ICU, potentially increasing adherence to clinical standards and improving patient outcomes. Additionally, sepsis is the most expensive condition treated in hospitals, accounting for approximately 5% of total hospitalization costs and an overall annual cost of USD 20.3 billion in the USA ( Torio 2011 ), and more than GBP 2.5 billion in the UK ( UK Sepsis Trust 2013 ). Early detection of sepsis via automated systems and subsequent timely intervention may reduce treatment costs and overall resource use. The UK Sepsis Trust estimates that there are more than 100,000 hospitalizations per year for sepsis, and that achieving 80% delivery of basic standards of care could result in a potential cost saving of GBP 170 million per year, even after allowing for increased survival\u2010related costs ( UK Sepsis Trust 2013 ). Finally, it is now recognized that sepsis is associated with significant mortality, long\u2010term morbidity and a reduction in health\u2010related quality of life ( Winters 2010 ), thus reinforcing the importance of early effective treatment from both a patient and resource utilization perspective. In summary, there is clear rationale to synthesize the evidence relating to the use of automated systems for the detection of sepsis.",
        "summary": "Compared with standard care for people admitted to tertiary care or to a medical intensive care unit (ICU) who meet at least two systemic inflammatory response syndrome (SIRS) criteria, very low\u2010 to low\u2010certainty evidence suggests little to no benefit of automated monitoring (computerized monitoring and alerts sent to ICU team/clinician when SIRS criteria have been met) in terms of initiation of antibiotic therapy, mortality, or duration of ICU stay. No study reported time to initiation of fluid resuscitation, quality of life, or failure to detect sepsis in the ICU. Given the available RCT evidence, no conclusions can be drawn."
    },
    "CD011671": {
        "query": "Can hypothermic machine perfusion increase success of deceased donor kidney transplantation compared with static cold storage?",
        "document": "Background Description of the condition End\u2010stage kidney disease (ESKD) is defined as an irreversible decline in kidney function that is severe enough to be fatal without renal replacement therapy (RRT). ESKD is a major debilitating condition with a drastic effect on patients' quality of life, as well as being associated with significant morbidity and death. It is a condition with growing worldwide prevalence, affecting an estimated 3.2 million people ( Fresenius 2013 ). The maintenance treatment for such patients is regular RRT. The impact of dialysis on the ability of patients to lead normal lives is significant, requiring frequent hospital visits, as well as severely restricting travel. It is now widely accepted that kidney transplantation offers a survival advantage over all forms of RRT ( Wolfe 1999 ). In addition, there is also an economic benefit of transplantation when compared with the high cost of dialysis. It has been estimated that kidney transplantation costs GBP \u00a3241,000 less than dialysis over a 10\u2010year period for a single patient ( NHSBT 2009 ). An estimated 77,818 kidney transplants were carried out in 2012 ( GODT 2012 ). Potential recipients can be transplanted with a kidney graft (simply termed 'graft' for the remainder of the review) from a living or deceased donor. Deceased donors may be certified dead on the basis of brainstem death (donation after brainstem death; DBD) criteria or circulatory death (donation after circulatory death; DCD). However, kidneys from deceased donors have a higher incidence of delayed graft function (DGF) and primary non\u2010function (PNF) due to the trauma of brainstem death or circulatory arrest, as well as reperfusion injury when compared with live donor kidneys. The growing disparity between supply and demand has led to increasing use of DCD organs and marginal organs from donors outside traditional transplantation protocols. A well\u2010accepted definition of extended/expanded criteria donor (ECD) is age over 60 years or over 50 years with a history of hypertension, kidney impairment or cause of death secondary to stroke ( Port 2002 ). Most studies have shown that transplantation of organs from either DCD or ECD are associated with inferior short\u2010 and long\u2010term outcomes ( Glyda 2012 ; Hwang 2014 ; Metzger 2003 ; Pascual 2008 ). This has focused attention on organ preservation techniques and ways to recondition organs in the donor and ex vivo prior to transplantation to potentially improve outcomes for recipients. However, the significant increase in cost of machine perfusion (MP) means that its widespread use depends on the demonstration of superiority, over the relatively inexpensive static cold storage (SCS). Although it is also important to note that at least some of the additional cost may be offset by reduced hospitalisation, complications, or both. The use of MP brings with it further questions, such as what is the optimum perfusion temperature, preservation solution; pulsatile versus non\u2010pulsatile flow; and oxygenated versus non\u2010oxygenated perfusate. The main focus of this review will be to compare (sub)normothermic and hypothermic MP (HMP) versus SCS. Description of the intervention From the early days of organ transplantation, hypothermia was an effective means of preserving the organ in the absence of oxygenated circulation. Belzer 1968 successfully preserved human kidneys using HMP; although the machine was large, bulky and difficult to transport. Shortly thereafter, an electrolyte solution was developed that enabled preservation of a kidney for 24 hours in a container surrounded with ice, now termed SCS ( Collins 1969 ). Subsequently, various other preservation solutions have superseded Euro\u2010Collins, most notably University of Wisconsin (UW), histidine\u2010tryptophan\u2010ketoglutarate (HTK), and Marshall's hyperosmolar citrate. The preservation solution used has an effect on the incidence of DGF, which may affect long\u2010term outcomes. In a meta\u2010analysis both UW and HTK were found to have similar DGF incidence, when compared with older preservation solutions like Euro\u2010Collins ( O'Callaghan 2012 , Table 1 ). Solution name Energy substrate N + K + M 2+ Ca 2+ pH Buffer Osm Impermeant Euro\u2010Collins Glucose 10 108 0 0 7.4 Bicarbonate/phosphate 340 Glucose UW Adenosine 30 125 5 \u2010 7.4 Phosphate 325 Lactobionate /raffinose HTK Ketoglutarate 15 10 4 0.015 7.02 to 7.2 Histidine 310 Mannitol Belzer's Adenine 100 25 5 0.5 7.4 HEPES 320 Gluconate /ribose PBS140 \u2010 92 0 0 0 7.2 Phosphate 310 Sucrose Celsion\u00ae Glutamate 100 15 13 0.25 7.3 Histidine 320 Lactobionate /mannitol Marshall's hyperosmolar citrate Citrate 28 26 41 \u2010 7.1 Citrate 486 Mannitol HEPES \u2010 N\u20102\u2010hydroxyethylpiperazine\u2010N\u20102\u2010ethane sulfonic acid; HTK \u2010 histidine\u2010tryptophan\u2010ketoglutarate; Osm \u2010 osmolality; THAM \u2010 trometamol; tris\u2010hydroxymethyl aminomethane; UW \u2010 University of Wisconsin As organ preservation solutions have evolved so have extracorporeal MP technologies. There are now several commercially available HMP devices which are broadly similar with minor variations in perfusion temperature (4 o C to 10 o C), flow (pulsatile versus non\u2010pulsatile) and provision of oxygenation (oxygenated versus non\u2010oxygenated). The most popular machines currently available are the LifePort\u00ae (Organ Recovery Systems; Itasca, Illinois), the KidneyAssist\u00ae (OrganAssist; Gronigen, Netherlands) and the Waters RM3\u00ae system (Rochester, Minnesota). The Gambro MP devices (Gambro, Lund, Sweden) were previously available alternatives. Once the kidney has been removed from the donor, the kidney is cannulated and connected to a disposable circuit designed specifically for the device. The donor kidney is then continuously perfused typically at temperatures between 6 o C and 12 o C within the battery\u2010operated device, whilst the kidney is transported to a suitable recipient. The older Waters and Gambro pumps rely on continuous flow of cold perfusate to sustain hypothermia. This risks graft loss in the unlikely event of pump failure. In contrast, the newer LifePort perfusion device is able to revert to SCS in the event of pump failure, mitigating this risk. More recently Professor Nicholson in Leicester, United Kingdom, has pioneered a technique of normothermic machine perfusion (NMP) using modified cardiopulmonary bypass equipment. This preservation technique is static and can be used to complement either SCS or HMP; as the kidney still has to be transferred to the recipient centre ( Nicholson 2013 ). In the future commercially available transportable kidney normothermic perfusion machines may become available as there is now for the liver (OrganOx\u00ae metra\u2122 device). Whilst NMP uses a perfusion temperature of 35 o C to 37 o C, further studies may employ (sub)NMP; defined as 20 o C to 34 o C. How the intervention might work Hypothermia slows the metabolism of cells. In general, for every 10 o C drop, the metabolism rate halves ( Wilson 2006 ). SCS works by removing blood and clots from the kidney graft and replacing this with an acellular preservation solution in a hypothermic environment. Pulsatile preservation up\u2010regulates nitric oxide production by vascular endothelium ( Gallinat 2013 ), as well as clearing the microcirculation of debris and toxic metabolites. Proponents hypothesise that the ultimate result of MP is a reduced intra\u2010renal resistance at the time of in vivo reperfusion and better earlier transplant function. NMP or EVNP ( ex vivo normothermic perfusion; as it is also known) technology is in its infancy and the exact beneficial mechanism of action debated. In brief, whilst the recipient is undergoing anaesthesia and preliminary surgery, the kidney is prepared and connected to a modified cardiopulmonary bypass circuit using a red\u2010cell based perfusate ( Nicholson 2013 ). The perfusate lacks mediators of reperfusion injury like leukocytes, complement, platelets but includes vasodilators and heparin. Experimental work has shown that this combination improves early transplant function in a porcine model ( Bagul 2008 ). Why it is important to do this review Ischaemia reperfusion injury in a kidney transplant manifests as DGF with PNF if the injury is severe. In a recent review looking at ECD/DCD kidneys one year graft survival was only 73% and PNF rate of 12.5% in one subset that had been transplanted ( Kosmoliaptsis 2015 ). In addition, DGF leads to an increased requirement for RRT, prolonged hospitalisation and often more investigations \u2010 incurring significant extra financial costs. In our own institution these extra peritransplant financial costs for DCD recipients with DGF have been estimated to be GBP \u00a34500 per patient ( Wilson 2014 ). In cases of PNF the recipient requires a second operation to remove the kidney and returns to dialysis with an immune system sensitised and difficult to match for repeat transplantation. DGF is most commonly defined as the requirement for dialysis within the first week after implantation ( Mallon 2013 ), although common measures of kidney function estimation may be used such as estimated glomerular filtration rate (eGFR; Cockcroft\u2010Gault or MDRD). SCS is a simple method of storage, and is relatively cheap compared to MP. Robust evidence for the benefits of MP are required to justify these increased initial costs. This review will critically appraise and summate the current randomised controlled trial (RCT) literature to analyse the potential benefit of novel preservation technologies in kidney transplantation, both in terms of patient centred outcomes and the financial implications at a societal level.",
        "summary": "Hypothermic machine perfusion can increase the success of deceased donor kidney transplantation. Its use can be considered for grafts donated after both circulatory death (DCD) and brainstem death (DBD). High\u2010certainty evidence shows that hypothermic machine perfusion (HMP) decreased the rate of delayed graft function (DGF; requirement for dialysis in the first week after transplant) compared with static cold storage (SCS) (on average, 258 vs 333 per 1000 people). HMP lowered DGF rates for both DCD and DBD grafts. Based on two large RCTs (676 and 282 participants, respectively) at low risk of bias, HMP improved kidney graft survival at one year and at three years when compared with SCS. This was true for organs from both DCD and DBD donors. HMP made little or no difference in the incidence of primary non\u2010function (high\u2010certainty evidence). The comparative effects of HMP versus SCS on patient survival, incidence of acute rejection, and duration of DGF remain uncertain. This review did not assess adverse events."
    },
    "CD003807-0": {
        "query": "How do antifungal drugs compare with placebo for preventing oral candidiasis in adults and children undergoing cancer treatment?",
        "document": "Background Treatment of solid malignant tumours and the leukaemias with cytotoxic chemotherapy or radiotherapy or both is becoming increasingly more effective but it is associated with short and long term side effects. Among the clinically important acute side effects is the disruption in the function and integrity of the mouth. The consequences of this include severe ulceration (mucositis) and fungal infection of the mouth (oral candidiasis). These disease and treatment induced complications may also produce oral discomfort and pain, poor nutrition, delays in drug administration, increased hospital stays and costs and in some patients life threatening infection (septicaemia). These potential problems have prompted clinicians to use agents during cancer treatment to prevent such oral complications. Antifungal agents are often used during the treatment of cancer to prevent superficial infections including oral candidiasis. Prevention of superficial infection is considered important because of its possible role in the development of systemic fungal infection. The incidence of systemic fungal infection has increased with the development of increasingly effective cancer therapy causing greater mucosal damage and prolonged neutropenia ( De Pauw 1997 ). Systemic infection is difficult to diagnose early and consequently cure because it rapidly becomes well advanced and disseminated leading to considerable morbidity and mortality. Sometimes empirical antifungal treatment is given to patients without documented fungal infection but with persistent fever despite antibiotic treatment. A current Cochrane review concludes that the only prophylactic or empirical antifungal agent with documented evidence of reducing mortality in cancer patients with neutropenia is intravenous amphotericin B ( Gotzsche 2002 ). In this review studies concerned with the prevention and treatment of oral candidiasis were excluded. We consider it important to review the evidence for the prevention of oral candidiasis because of the effect a fungal infection in the mouth has on general well being and the possible related systemic consequences. This review is one in a series of four Cochrane reviews evaluating the evidence for the prevention and treatment of oral candidiasis and oral mucositis in patients treated for cancer ( Clarkson 2007 ; Worthington 2007 ; Worthington 2007a ).",
        "summary": "Moderate\u2010certainty evidence shows that adults and children undergoing any cancer treatment may benefit from the use of antifungal medication absorbed or partially absorbed from the gastrointestinal (GI) tract for prevention of oral candidiasis. RCT evidence in around 1200 people shows that antifungal drugs absorbed from the GI tract (such as fluconazole, ketoconazole, and itraconazole) are more effective than placebo for preventing oral candidiasis (on average, 27 vs 57 per 1000 people developed candidiasis; 7 RCTs; 1153 participants). This effect was consistent for antifungal drugs partially absorbed in the GI tract (such as miconazole and cotrimazole) compared with placebo (on average, 44 vs 267 per 1000 people; 4 RCTs; 292 participants). For other antifungal drugs not absorbed in the GI tract (such as amphotericin B, nystatin, and chlorhexidine), reviewers found little or no difference between antifungals and placebo in rates of candidiasis. For other outcomes evaluated, such as systemic fungal infection or toxicity (adverse events probably due to the drug or death), little or no difference was found between antifungal drugs and placebo, between antifungal drugs absorbed in the GI tract, or, when data were available, between partially absorbed and not absorbed antifungals and placebo. These results should be interpreted with caution. Dosage and length of prescription remain unclear. The benefit of antifungals may be greater in patients for whom risk of an oral fungal infection is high; however, balancing possible preventative effects and adverse effects is essential for final decision\u2010making. For a comparison of antifungal drugs absorbed and not absorbed from the GI tract, see CCA 2808 ."
    },
    "CD011382": {
        "query": "In people undergoing major upper gastrointestinal or liver surgery, how does adding enhanced recovery protocols to standard care compare with standard care alone?",
        "document": "Background Description of the condition Upper gastrointestinal disorders include diseases affecting the oesophagus, stomach, liver, gallbladder and pancreas. The definition of major upper gastrointestinal, liver and pancreatic surgery is variable. One definition that we have used in this review includes all upper gastrointestinal, liver and pancreatic surgeries that have been excluded from the British Association of Day Surgery Directory of Procedures ( BADS 2012 ), as well as surgeries such as transhiatal oesophagectomy, gastrectomy (irrespective of whether a total or subtotal distal gastrectomy is performed), liver resection (irrespective of the number of segments resected and the aetiology), pancreatic resection (irrespective of whether a pancreaticoduodenectomy or a distal pancreatectomy is performed, and irrespective of whether the pylorus is preserved), pancreatic drainage procedures (for chronic pancreatitis) and open pancreaticojejunostomy or pancreaticogastrostomy for pseudocyst. In upper gastrointestinal surgeries that are included in the British Association of Day Surgery Directory of Procedures ( BADS 2012 ), early mobilisation and discharge are recommended when possible, and patients are admitted because of coexisting medical illnesses, for administrative reasons, because of complications related to surgery, or because of intolerance to surgery. As will be evident from the description of the intervention, early mobilisation is one of the elements of an enhanced recovery protocol; therefore this review includes only upper gastrointestinal, liver and pancreatic surgeries that are not included in the British Association of Day Surgery Directory of Procedures ( BADS 2012 ). The number of major upper gastrointestinal, liver and pancreatic surgeries performed each year is difficult to estimate. In the UK alone, approximately 25,000 major upper gastrointestinal, liver and pancreatic surgeries are performed, costing approximately GBP 150 million each year ( Hospital Episode Statistics 2013 ). Description of the intervention 'Fast\u2010track surgery' or 'enhanced recovery protocol' or 'fast\u2010track rehabilitation' after surgery, again can be variably defined, and involves one or more of the following elements ( Kehlet 1997 ). Preoperative information and teaching. Decreased stress related to surgery. Pain relief. Exercise (early mobilisation). Enteral nutrition. Growth factors. Preoperative information and teaching. Decreased stress related to surgery. Pain relief. Exercise (early mobilisation). Enteral nutrition. Growth factors. How the intervention might work Enhanced recovery protocols aim to decrease pathophysiological changes after surgery, including surgical stress, pain, immunosuppression, nausea, vomiting and ileus, hypoxaemia, immobilisation leading to blood clots in the legs and malnutrition leading to catabolism and muscle wasting ( Kehlet 1997 ). Why it is important to do this review Implementation of enhanced recovery protocols involves considerable planning (including identifying the elements that need to be incorporated into the protocol for a particular surgery) and deployment (which involves educating and training the staff and encouraging staff members, including surgeons, to follow a unified protocol). Deployment may involve changing traditions and beliefs of surgeons and other clinicians who may be reluctant to change their traditional practices, or who may be concerned that the enhanced recovery protocol may be associated with patient risks and harms. It also incurs costs. Thus, implementation of enhanced recovery protocols involves considerable human and resource management. It is therefore essential to assess whether an enhanced recovery protocol is effective for major upper gastrointestinal, liver and pancreatic surgery. No Cochrane review on this topic has been conducted.",
        "summary": "Very low\u2010quality evidence failed to detect a benefit of adding an enhanced recovery protocol (education, alternative pain relief, early mobilization, early enteral nutrition, growth factors) to standard care for adults undergoing upper gastrointestinal surgery, in terms of mortality, health\u2010related quality of life, or hospital readmissions. Although the addition of an enhanced recovery protocol may have resulted in a shorter duration of hospital stay, the evidence was low quality. Very low\u2010 to low\u2010quality evidence suggested that fewer people experienced mild adverse effects with the enhanced recovery protocol, but no benefit was observed for serious adverse effects."
    },
    "CD006167": {
        "query": "Is amniotomy effective in shortening spontaneous labor and what are the effects on maternal and neonatal outcomes?",
        "document": "Background Intentional artificial rupture of the amniotic membranes during labour, sometimes called amniotomy or 'breaking of the waters', is one of the most commonly performed procedures in modern obstetric and midwifery practice. It was introduced in the mid\u2010eighteenth century, first being described in 1756 by an English obstetrician, Thomas Denman ( Calder 1999 ). Whilst he emphasised reliance on the natural process of labour, he acknowledged that rupture of the membranes might be necessary in order to induce or accelerate labour ( Dunn 1992 ). Since then, the popularity of amniotomy as a procedure has varied over time ( Busowski 1995 ), more recently becoming common practice in many maternity units throughout the UK and Ireland ( Downe 2001 ; Enkin 2000a ; O'Driscoll 1993 ) and in parts of the developing world ( Camey 1996 ; Chanrachakul 2001 ; Rana 2003 ). The primary aim of amniotomy is to speed up contractions and, therefore, shorten the length of labour. In order to carry out an amniotomy, the caregiver performs a vaginal examination to digitally identify the cervix and the amniotic membranes. The caregiver excludes the presence of blood vessels across the membranes (vasa praevia), and ensures the baby's head fits the pelvis well and is no higher than two stations above the ischial spines. The membranes are then punctured using a crotchet\u2010like, long\u2010handled hook (commonly referred to as an amnihook) and the membranes are torn apart digitally. The mechanism by which amniotomy speeds up labour remains unclear. It is thought that when the membranes are ruptured, the production and release of prostaglandins and oxytocin increases, resulting in stronger contractions and quicker cervical dilatation ( Busowski 1995 ). In the 1930s, Eastman suggested that the 'bag of water' surrounding the fetus played the principal role in the cervical dilatation and was therefore indispensable to normal labour ( Busowski 1995 ). Since then this concept of a 'protective bag' around the baby buffering and protecting the infant from the immense forces of uterine contractions, as well as aiding cervical dilatation, has been supported by many ( Caldeyro\u2010Barcia 1972 ; Robertson 1997 ). Vincent 2005 advocated that the bulging membranes at the vaginal introitus serve to pre\u2010stretch the perineum before the head has crowned. Pressure from intact membranes contributes to the ripening and effacement (softening and shortening) and dilatation of the cervix. The pressure exerted by the membranes stimulates oxytocin surges in much the same way as pressure from the fetal presenting part ( Vincent 2005 ). The membranes surrounding the fetus are composed of two layers: an inner amnion (nearest to the fetus) and an outer chorion (nearest to the lining of the pregnant womb, which is also known as the decidua). It is believed that softening and shortening of the cervix occurs in response to the prostaglandin PGE2, which is produced by both the amnion during pregnancy and also by the cervix itself at term. During pregnancy the chorion represents a protective barrier between the amnion and the cervix. The chorion produces an enzyme called prostaglandin dehydrogenase (PDHG), which breaks down PGE2; thus preventing the cervix from ripening, and avoiding an inappropriate and premature labour. There is a theory that in term pregnancies, the part of the chorion which is in direct contact with the opening of the cervix releases less PDHG. This allows the prostaglandins from the amnion to come into contact with the cervix, causing ripening and effacement ( Van Meir 1997 ). If amniotomy is performed, the influence of these prostaglandins on the cervix is therefore lost. This may explain in part why, if amniotomy is performed too early (that is, when the woman is less than 3 cm dilated), it can be counterproductive and slow the process of labour down. The converse has also been advocated: amniotomy use as a method of augmenting complicated and long labours ( Enkin 2000b ). Many caregivers promote amniotomy on the clinical assumption that it increases labour contractions and therefore improves labour progress ( Frigoletto 1995 ), especially in those women with prolonged labour ( Bohra 2003 ). Prolonged labour can be an important cause of maternal morbidity and contributes significantly to the half a million women who die annually as a result of childbirth ( WHO 2004 ). Haemorrhage and infection, which are strongly associated with long labours, are also leading causes of maternal death ( Neilson 2003 ). For this reason, amniotomy may be of particular importance for women in the developing world, who carry the greatest burden of morbidity and mortality associated with long labours. As well as employing amniotomy as a method of shortening labour, many caregivers deem it valuable in order to introduce internal fetal monitoring devices, such as fetal scalp electrode or an intrauterine pressure catheter. It also allows visualisation of the amniotic fluid to detect meconium\u2010stained liquor in order to identify factors, which may lead to fetal compromise ( Clements 2001 ). There is some suggestion that the quality of the amniotic fluid can only provide limited information, as meconium\u2010stained liquor may be seen in up to 20% of normal pregnancies at term ( Gibb 1992 ). In order to evaluate the use of amniotomy to accelerate spontaneous labour, it is important to identify what constitutes normal length of labour. Confirmation of the progress of labour is determined by the identification of increasing cervical dilatation and cervical effacement ( Enkin 2000a ; Neilson 2003 ). The definition provided by the World Health Organization for primiparous women is that more than 18 hours in labour is considered prolonged ( Kwast 1994 ). With the active management of labour protocol, introduced by O'Driscoll and Meagher over 30 years ago in Dublin, the use of amniotomy has been widely and readily accepted by some clinicians as part of a package ensuring that women are in labour for no longer than 12 hours ( O'Driscoll 1993 ). A study exploring the perceptions of duration of labour of traditional birth attendants in Mexico found that 29% of them thought labour of a primipara normally lasts 13 hours, and 74% of them said the labour of a multiparous woman could last between four and eight hours, but no longer that 10 hours ( Camey 1996 ). Another developing country (Thailand) classified normal labour would not exceed 12 hours ( Chanrachakul 2001 ). As the definition of normality appears to be vague, with resulting variation in practice, no consensus has yet been reached amongst midwives and obstetricians to provide a definition of normality. For example, there is little agreement concerning the 'normality' of a labouring primigravida who has made slow but steady progress for 20 hours in the absence of maternal and fetal compromise ( Neilson 2003 ). Very little is also known about how important length of labour is to most women ( Impey 1999 ). Reducing length of labour might not be a desired effect for all women. There are arguments that the length and progress of labour should not be based on the premise that all labours are the same, but by the woman and baby's wellbeing ( Jowitt 1993 ; Robertson 1997 ). Prolonged labour can ultimately be associated with delivery by caesarean section and low cord pH in the fetus. Amniotomy is employed with the assumption that shortening the length of labour is beneficial, with little apparent regard for any potential associated adverse effects. There is a lack of evidence to support or refute this assumption. Although several theoretical hazards exist as a consequence of amniotomy, few studies show any substantial risks. Possible complications include umbilical cord prolapse, cord compression and fetal heart rate decelerations, increased ascending infection rate, bleeding from fetal or placental vessels and discomfort of the actual procedure ( Busowski 1995 ). Data from studies suggest that early amniotomy increases the hourly rate of severe variable fetal heart rate decelerations without evidence of an adverse effect on neonatal outcome ( Fok 2005 ; Goffinet 1997 ). In areas of high HIV prevalence, it is considered prudent to leave the membranes intact for as long as possible to reduce perinatal transmission of HIV ( WHO 2006 ). Under normal conditions, the membranes remain intact until full dilatation in 70% of the cases ( Stewart 1995 ). As well as the physical risks associated with amniotomy, psychological effects need to be considered ( Clements 2001 ). The largest UK consumer\u2010directed research investigating women's attitudes surrounding the procedure of amniotomy identified that some women worried more about removing the protective bag of fluid cushioning the baby's head than the pain or duration of their labours ( NCT 1989 ). Some women complain that amniotomy causes them to lose control in labour ( Robinson 2000 ). However, others ( Impey 1999 ) have concluded that women prefer shorter labours and have little bias against the intervention (amniotomy) that helps achieve this. Readers may wish to refer to the following Cochrane systematic reviews for further information about artificial rupture of the membranes: 'Amniotomy alone for induction of labour' ( Bricker 2000 ), 'Amniotomy plus intravenous oxytocin for induction of labour' ( Howarth 2001 ), 'Oestrogens alone or with amniotomy for cervical ripening or induction of labour' ( Thomas 2001 ).",
        "summary": "The routine use of amniotomy to shorten spontaneous labor is not supported by the current randomized controlled trial evidence. However, many of the studies had weak methods and did not take account of the time when amniotomy was performed so the results should be interpreted with caution. In studies including around 1000 women amniotomy provided no statistically significant difference in the length of first stage of labor in primiparous or multiparous women compared with intention to preserve the membranes intact. In studies including at least at total 2000 women, there was also no statistically significant difference between amniotomy and no amniotomy in rates of cesarean section, maternal mortality, maternal infection and perinatal mortality. In studies including around 2500 primiparous women, there was also no statistically significant difference between groups in use of pain relief medication. One study addressing the use of pain relief medication in 533 multiparous women having amniotomy showed that 45 per 100 women (95% CI 36 to 57) with amniotomy compared with 31 per 100 women with no amniotomy will need pain medication, either epidural or other form of analgesia. The use of amniotomy reduced the number of neonates with Apgar less than seven at 5 minutes (1 neonate per 100 women (95% CI 0 to 1) with amniotomy compared with 2 neonates per 100 women with no amniotomy) and led a modest decrease in the length of the second stage of labor in primiparous women. The effect of amniotomy on maternal satisfaction with childbirth experience is uncertain; although the analysis did not find a statistically significant difference between groups, it was based on only 123 women and had high statistical heterogeneity."
    },
    "CD010770": {
        "query": "What are the effects of selective progesterone receptor modulators (SPRMs) for premenopausal women with uterine fibroids?",
        "document": "Background Description of the condition Uterine fibroids are common smooth muscle tumours arising from the uterus. They are also known as leiomyomata or myomas. The prevalence of these tumours depends on the population\u2019s ethnicity and the method of detection. More than 80% of black women and nearly 70% of white women will develop fibroids before the age of 50 years ( Baird 2003 ). These tumours, although benign, can cause significant distortion of the uterus and result in symptoms in up to 50% of women ( Baird 2003 ). Fibroids are frequently associated with abnormal uterine bleeding, bulk symptoms (pelvic pressure, urinary dysfunction, constipation, pain) and reproductive dysfunction (subfertility, miscarriage, pregnancy complications) ( Stovall 2001 ). In the United States alone, the direct cost of treatment for women with fibroids is estimated to be over four billion dollars annually ( Cardozo 2012 ). Fibroid\u2010related symptoms can be treated with surgery (hysterectomy, myomectomy, endometrial ablation, myolysis), minimally invasive procedures (uterine artery embolisation, magnetic resonance\u2010guided focused ultrasound) or medical therapies ( Wallach 2004 ). Despite these treatment options, hysterectomy is the second most frequently performed surgical procedure in the United States, with fibroids the most common indication ( Merrill 2008 ); this has contributed to significant surgical morbidity and escalating healthcare costs. Thus, focus on more conservative options is needed. Description of the intervention Currently, no pharmacological agents have received global approval specifically for long\u2010term treatment of individuals with uterine fibroids. The mainstay of medical management has comprised use of gonadotropin\u2010releasing hormone analogues (GnRHa) for preoperative optimisation seen as decreased blood loss, corrected anaemia and reduced fibroid volume ( Sabry 2012 ). Leuprolide acetate is one of the most frequently used GnRHa treatments for fibroids and is approved in the United States and Europe for this indication. Challenges associated with GnRHa therapy include decreased bone mineral density, development of vasomotor symptoms and an initial oestrogen flare that may exacerbate symptoms. Although medical therapies such as combined hormonal contraceptives, progestins, progestin\u2010releasing intrauterine systems and danazol may be used to decrease menstrual blood flow, their specific effects on fibroids and bulk symptoms are limited, and they often cause side effects that lead to discontinuation ( Ke 2009 ; Sangkomkamhang 2013 ; Van Voorhis 2009 ). Traditionally, oestrogen has been considered the most important hormone for stimulating fibroid growth. Recently, progesterone was found to be essential for the maintenance and growth of fibroids ( Bulun 2013 ). For this reason, selective progesterone receptor modulators (SPRMs) have shown promise for the treatment of women with uterine fibroids ( Chwalisz 2005 ). These molecules bind to the progesterone receptor and show varying levels of antagonistic activity. SPRMs were first discovered in 1980, and mifepristone, a powerful progesterone antagonist, was the pioneer drug. It has been used mainly for pregnancy termination but has also been evaluated as a therapeutic agent for fibroids. Meta\u2010analysis of three randomised trials showed that mifepristone is effective in reducing bleeding symptoms and improving fibroid\u2010related quality of life, with no effect on fibroid volume ( Tristan 2012 ). Other SPRMs were subsequently developed, each with different affinity for the progesterone receptor and showing varying degrees of antagonistic activity. The clinical activity of each SPRM class member reflects the subtlety of its spectrum of agonist and antagonist activity, along with tissue\u2010specific expression of progesterone receptor (PR) subtypes. How the intervention might work The \u2018progesterone hypothesis\u2019 suggests that progesterone acts as a key hormone in the development of fibroids by increasing mitotic rates and reducing apoptosis of fibroid smooth muscle cells ( Bulun 2013 ). Data also suggest that signalling occurs between oestrogen and progesterone receptors, whereby oestrogen induces increased expression of the progesterone receptor in fibroid cells ( Ishikawa 2010 ). The importance of progesterone in fibroid pathogenesis supports SPRMs as effective treatment for women with fibroids. Fibroid cells cultured with SPRMs demonstrate inhibited proliferation and increased apoptosis, without affecting normal myometrium ( Bouchard 2011 ). SPRMs can also downregulate the number of growth factors while reducing collagen synthesis in cultured fibroid cells ( Bouchard 2011 ). SPRMs act upon the uterine endometrium to provide relief of bleeding symptoms in women with fibroids ( Wagenfeld 2016 ). SPRMs are known to cause unique changes to the endometrium. Histological endometrial changes have been labelled as progesterone receptor modulator\u2010associated endometrial changes (PAECs) on the basis of international consensus ( Mutter 2008 ). These changes are benign and reversible. SPRMs may be used to treat women with fibroids in several clinical scenarios. Currently, the only SPRM approved for medical management of fibroids is ulipristal acetate (Esmya, Gedeon\u2010Richter, Europe, February 2012; Fibristal, Watson Laboratories Inc, Canada, July 2013). This drug was approved to treat bleeding symptoms while decreasing fibroid size for up to three months before surgery. Recently, it was approved in Europe and Canada for ongoing intermittent use. Long\u2010term use of SPRMs for fibroid\u2010related symptoms may decrease the need for surgical intervention and associated morbidity and costs. Long\u2010term medical therapy may be particularly beneficial for bridging perimenopausal women until menopause, when fibroids would then spontaneously decrease. Although pregnancy is contraindicated with SPRMs, evidence shows that the decrease in fibroid size is sustained after the medication has been discontinued ( Donnez 2012 ). This may cause fibroid\u2010related subfertility, for which medical management may reduce fibroid volume and facilitate pregnancy after discontinuation of SPRM treatment. Why it is important to do this review Despite the prevalence of uterine fibroids, only a few high\u2010quality studies have examined the effectiveness of medical therapies. With increasing demand for less invasive fibroid therapies, the benefits and risks of medical treatments must be critically evaluated. Furthermore, women are delaying childbearing, hence fertility\u2010sparing therapeutic options are needed. Biochemical and clinical evidence shows that SPRMs may decrease fibroid growth and ameliorate symptoms ( Chwalisz 2005 ). Although a Cochrane review on mifepristone has been completed ( Tristan 2012 ), the newer SPRMs require systematic evaluation of their benefits and harms.",
        "summary": "Selective progesterone receptor modulators (SPRMs) improve symptoms and quality of life for women with symptomatic uterine fibroids, although women may experience adverse endometrial changes. However, how SPRMs compare with leuprolide acetate is less clear, and no randomized controlled trials (RCTs) have compared SPRM versus surgery. Compared with placebo, moderate\u2010quality evidence shows that for adult women with symptomatic uterine fibroids, SPRMs (mifepristone, ulipristal acetate, and asoprisnil) improved symptoms (on average, by \u223c 20 points on a 100\u2010point scale) and quality of life (on average, by \u223c 23 points on a 100\u2010point scale), increased the number of women with amenorrhea at three to six months (on average, 477 vs 29 per 1000 women), and substantially reduced menstrual blood loss after three months of treatment. More women showed endometrial changes considered to be associated with SPRMs (on average, 351 vs 77 per 1000 women; low\u2010quality evidence), but researchers did not observe these only in women taking SPRMs. Results for pelvic pain were inconsistent across RCTs. When compared with leuprolide acetate, SPRMs showed no differences in symptom severity, quality of life, amenorrhea, menstrual blood loss, or pelvic pain (mifepristone and ulipristal acetate), but analyses included fewer than 300 women, making it more difficult to detect differences between active comparators (evidence of low to moderate quality). Percentage change in uterine volume was greater with SPRMs (on average, by \u223c 26%), and more women showed endometrial changes considered to be associated with SPRMs (on average, 585 vs 119 per 1000 women; moderate\u2010quality evidence), although researchers did not observe these changes only in women taking SPRMs. Researchers identified no RCTs that compared SPRM versus surgery (myomectomy or hysterectomy) or uterine embolization."
    },
    "CD005481": {
        "query": "What are the effects of co\u2010formulated abacavir\u2010lamivudine\u2010zidovudine for the initial treatment of HIV and AIDS?",
        "document": "Background The human immunodeficiency virus (HIV) pandemic poses one of the greatest challenges to global public health. In 2011, an estimated 34 million people were living with HIV and 1.7 million died of the acquired immunodeficiency syndrome (AIDS) (UNAIDS 2012). Prevention is commonly advocated to curb the spread of HIV infection, and although preventive methods have considerably slowed the spread of HIV in most parts of the world, people who are already infected need care and treatment. The goal of antiretroviral therapy is to achieve prolonged suppression of HIV replication. The ideal antiretroviral drugs should be effective in suppressing viral replication, affordable, available in simplified regimens, well tolerated, and have no dietary interactions. The use of monotherapy and dual therapy has often led to mutations and long\u2010term resistance ( Eron 1995 ; Pialoux 1998 ; Rutherford 2003 ), necessitating the development of combination therapy with three drugs taken separately ( Carpenter 2000 ; Hammer 2008 ). In well\u2010resourced countries ( Ledergerber 1999 ) and, recently, Brazil ( Hacker 2004 ; Teixeira 2004 ), antiretroviral therapy has contributed substantially towards delaying HIV progression to AIDS and death. However, these combinations are complex and difficult to take due to high pill burden, stringent intake schedules, and food and fluid restrictions They may also be associated with drug\u2010drug interactions and numerous side effects, including various lipid abnormalities ( Mehta 1997 ; Gifford 2000 ). This complexity also makes antiretroviral therapy less accessible to patients in most resource\u2010constrained regions of the world, which currently are hardest hit by the pandemic, such as sub\u2010Saharan Africa. This area is inhabited by approximately 10% of the world's population but is home to 60% of all people currently living with HIV ( UNAIDS 2012 ). Concern over toxicity, adherence, and drug\u2010drug interactions has led to the development of simpler antiretroviral regimens, including co\u2010formulated abacavir\u2010lamivudine\u2010zidovudine ( Anon 2000 ; Saez\u2010Llorens 2001 ). Three NRTIs simplify PI\u2010based therapy by easing dosing regimens (only one tablet twice daily) and avoiding lipid abnormalities ( Seaton 2003 ). Although treatment simplification could help patients maintain adherence, continued virologic suppression must be ensured. Therefore, clarification of the role of this simplified antiretroviral therapy on prolonged suppression of HIV replication is of considerable importance. Because all three antiretroviral drugs are of the same class, the use of co\u2010formulated abacavir\u2010lamivudine\u2010zidovudine (if proven to be effective) potentially preserves NNRTIs and PIs for later use, thereby avoiding resistance to all classes of antiretroviral agents at the same time, and allows for effective second\u2010line treatment regimens ( Staszewski 2001 ). There are concerns, however, about hypersensitivity reactions to abacavir ( Staszewski 1998 ). Cross resistance between drugs of the same class should also be considered. Also, entecavir used for hepatitis B virus (HBV) treatment, may select for M184V mutation which confers resistance to lamivudine in individuals co\u2010infected with HIV and HBV ( McMahon 2007 ). The aim of this review was to combine all high\u2010quality RCTs comparing co\u2010formulated abacavir\u2010lamivudine\u2010zidovudine with PI\u2010 or NNRTI\u2010based therapy to assess the antiviral potency and tolerability of the simplified triple nucleoside combination in initial therapy for HIV.",
        "summary": "Moderate\u2010quality evidence showed that, compared with alternate regimens, co\u2010formulated abacavir\u2010lamivudine\u2010zidovudine (Trizivir\u00ae) for initial treatment of HIV infection and AIDS led to a similar CD4 cell count and virologic failure rate (two successive HIV\u20101 RNA measurements of \u2265200 copies/mL after at least 16 weeks) after a mean of 48 weeks follow up. There was also no difference detected between the two treatment regimens in virologic suppression when measured as a viral load of HIV\u20101 RNA of <400 copies/mL (4 RCTs with 2247 participants) or <50 copies/mL (moderate quality evidence). Similarly, moderate\u2010quality evidence showed that there was no difference in hypersensitivity or severe adverse reactions between the two treatments. No trials reported clinical lipodystrophy manifestation or treatment adherence. It is also worth noting that the alternative regimens differed across the trials, and most included some, if not all, of the components of Trizivir\u00ae ."
    },
    "CD009881": {
        "query": "What are the benefits and harms of mifepristone in women with endometriosis?",
        "document": "Background Description of the condition Endometriosis is defined as the presence of endometrial tissue (glands and stroma) outside the uterine cavity. It is a condition that is oestrogen\u2010dependent and thus is seen primarily during the reproductive period ( Berek 2007 ; Giudice 2010 ). The prevalence of endometriosis ranges from 6% to 10% in women of reproductive age, from 50% to 60% in women and teenage girls with pelvic pain, and has been reported as up to 50% among women with infertility ( Cramer 2002 ; Eskenazi 1997 ; Goldstein 1980 ); endometriosis can also be found in asymptomatic women during surgical procedures such as laparoscopy or sterilisation. Endometriosis constitutes a substantial burden on the health\u2010related quality of life of women and on the finite healthcare resources of national health systems worldwide ( Vercellini 2014 ). Rokitansky reported this disease for the first time in 1860. Although considerable progress has been made, the pathogenesis remains unclear. The 'retrograde menstruation theory' proposed in the 1920s ( Sampson 1927 ) speculated that endometriosis derives from reflux of endometrial fragments regurgitated through the fallopian tubes during menstruation, with subsequent implantation on the peritoneum and the ovary. Hormonal imbalance involved in development of endometriosis includes increased oestrogen synthesis and metabolism and progesterone resistance ( Tosti 2016 ). Minimally invasive surgical treatments play an important role in the diagnosis and removal of endometriosis. Unfortunately, this approach is limited by recurrences of endometriosis. Goals of management are to provide pain relief, to limit recurrences, and to restore or preserve fertility when needed. Current medical treatment ( Brown 2014 ) has focused on blocking ovarian oestrogen secretion (with gonadotropin\u2010releasing hormone analogues or antagonists (GnRH\u2010a or GnRH antagonists)) or halting oestrogen\u2010induced growth of ectopic endometrium (with oral contraceptives and androgens). The downside of these approaches includes a significant decrease in bone mass, along with hot flushes, vaginal dryness, acne, and hirsutism, which impair quality of life for many women. Furthermore, the mean length of time before recurrence of pain after completion of medical therapy is between 6 and 18 months ( Mahutte 2003 ). The ideal medical therapy for endometriosis should relieve pain and inhibit endometrial proliferation while avoiding a hypo\u2010oestrogenic state. Description of the intervention Progesterone, acting via its cognate receptors, plays a central role in regulation of uterine function, making progesterone receptor an attractive therapeutic target. Progesterone receptor modulators (PRMs) represent a class of synthetic ligands that can exert agonist, antagonist, or mixed effects on various progesterone target tissues. Mifepristone (MFP, RU486) is a progesterone and glucocorticoid receptor antagonist that was first synthesised in 1980. Since that time, numerous related compounds of progesterone receptor ligands have been synthesised, exhibiting a spectrum of activity ranging from pure progesterone receptor antagonists (PRAs) to mixed agonists/antagonists. PRAs and partial agonist antagonists \u2010 selective progesterone receptor modulators (SPRMs) \u2010 belong to the large progesterone receptor ligand family. Owing to their antiproliferative effects in the endometrium, PRMs have been advocated for treatment of symptomatic endometriosis but may not provide appropriate treatment for asymptomatic endometriosis\u2010associated infertility. Unlike long\u2010acting GnRH\u2010a, PRMs are not associated with a decrease in bone mineral density, and their use is accompanied by an increase in oestradiol and progesterone receptors ( Neulen 1996 ), suggesting that the endometrial antiproliferative effect is due to progesterone antagonism. This offers a new alternative: suppression of endometrial proliferation in the midst of an oestrogenic environment. To date, compared with the widely used steroidal progesterone receptor agonists, only three PRMs have been approved by the FDA as emergency contraceptives: RU486, ulipristal acetate (UPA or CDB 2914), and asoprisnil (J867) ( Li 2016 ). Few studies of PRMs for endometriosis have been published, and some show clinical benefit ( Chwalisz 2004 ; Kettel 1996 ; Kettel 1998 ). In an uncontrolled study, mifepristone relieved pelvic pain without producing significant side effects ( Koide 1998 ). However, it is reported that mifepristone caused downregulation of progesterone receptors (PRs) and oestrogen receptors (ERs) and upregulation of androgen receptors (ARs) ( Narvekar 2004 ), and in low daily doses, it inhibited ovulation and induced amenorrhoea in most women ( Brown 2002 ). Administration of PRMs also has been found to be accompanied by morphological changes within the endometrium, described as PRM\u2010associated endometrial changes (PAECs) ( Williams 2007 ). These histological changes are recognised as a distinct histological entity, and the mechanisms by which they develop are poorly understood. The clinical effectiveness of PRMs remains to be evaluated. How the intervention might work Progesterone receptor antagonists block the action of progesterone. Therefore, it is not surprising that they have clinical application in the medical termination of pregnancy ( Brenner 2002 ; Brenner 2005 ). PRAs work as antiprogestins \u2010 substances that prevent cells from making or using progesterone. SPRMs represent a new class of progesterone receptor ligands that exert clinically relevant tissue\u2010selective progesterone agonist, antagonist, or partial (mixed) agonist/antagonist effects on various progesterone target tissues in an in vivo situation, depending on the biological action studied ( Spitz 2003 ). In human cell lines, mifepristone, asoprisnil, ulipristal acetate, lonaprisan, and telapristone acetate suppress endometrial proliferation, resulting in endometrial atrophy. Additional studies on animal models show that mifepristone, onapristone, and ZK136799 suppressed endometrial growth and reduced the production of prostaglandins with possible benefits for pain ( Bouchard 2011 ). As PRMs have a relatively minor effect on serum oestradiol and androgen levels, application of PRMs may have greater efficacy and flexibility than traditional treatments for endometriosis as the result of selective inhibition of endometrial proliferation without systemic effects of oestrogen deprivation. PRMs also have the potential to suppress endometrial prostaglandin and decrease menstruation; this mechanism is proposed to reduce endometriosis\u2010related pain ( Tosti 2016 ). Why it is important to do this review Until now, no systematic review has examined this topic. Doctors considering the use of PRMs in clinical practice are uncertain about the balance between benefit in terms of pain relief and unacceptable side effects. In this review, we evaluated the clinical effectiveness and safety of PRMs, primarily in terms of pain relief, and collated the best evidence for their use as single agents in women with endometriosis.",
        "summary": "Mifepristone may help to relieve symptoms of endometriosis, but its use may lead to increased side effects. Availability and use of mifepristone are restricted in the USA. For women with symptomatic endometriosis, moderate\u2010 to low\u2010quality evidence reveals that mifepristone (2.5, 5, or 10 mg daily for 6 months) decreased dysmenorrhea (on average, 51 vs 402 per 1000 women) and dyspareunia (on average, 85 vs 288 per 1000 women) at three months, compared with placebo. However, high\u2010quality evidence shows that mifepristone was associated with an increase in amenorrhea (884 vs 11 per 1000 women) and in hot flashes (243 vs 11 per 1000). Use of mifepristone is restricted and a black box warning has been issued in the USA because of its potential to terminate pregnancy."
    },
    "CD009231": {
        "query": "In terminally\u2010ill people, can offering home\u2010based end\u2010of\u2010life care improve outcomes?",
        "document": "Background Description of the condition Surveys of the preferences of the general public and terminally ill people report a growing consensus that, given adequate support, most people would prefer to receive end\u2010of\u2010life care at home ( Department of Health 2008 ; Gomes 2012 ; Higginson 2000 ). The preference of patients who do not have caregivers is less clear. While a policy supporting choice is broadly endorsed ( Agelopoulos 2009 ; Department of Health 2008 ), it brings with it conceptual and methodological difficulties for those evaluating the effectiveness of these types of services, and further challenges to those responsible for implementing these interventions, due to patient and caregiver preference changing over time. Description of the intervention End\u2010of\u2010life care at home is the provision of a service that provides active treatment for continuous periods of time by healthcare professionals in the patient's home for patients who would otherwise require hospital or hospice inpatient end\u2010of\u2010life care. How the intervention might work The rationale for providing end\u2010of\u2010life care at home is complex as it reflects the policy objective of providing patients and their families with a choice of where and when they want care. One difficulty underpinning the concept of choice in this context is that while more people want to die at home, they also recognise the practical and emotional difficulties of exercising this choice. For example, terminally ill people express concern about being a 'burden' to family and friends and worry about their families seeing them in distress or having to get involved with intimate aspects of care ( Gott 2004 ). Similarly, although caregivers of terminally ill people often prefer to care for their relatives at home ( Woodman 2015 ), continuity of care may be difficult to achieve, and yet is essential to fulfil the choice of dying at home ( Seamark 2014 ). Why it is important to do this review In some countries, namely the US and Canada, the number of people dying at home has increased ( Decker 2006 ; Wilson 2009 ), whereas in others, for example Italy and Japan, it has decreased. A retrospective cohort study of Taiwanese patients who died of cancer reported a decrease in the proportion of patients dying at home from 36% to 32%, due to access to treatments that were only available in hospital palliative\u2010care settings ( Tang 2010 ). Although recent data indicate a small increase in the number of people who have died at home in the UK, it was estimated that in 2008 18% of deaths were at home and 58% were in hospitals ( Department of Health 2008 ), and in 2013 22% of people died at home, 22% died in care homes, 6% in hospices, and 48% in hospital ( NEoLCIN 2014 ). The reduction in the proportion of people dying in hospital could be attributed, at least in part, to the improvements in care and services as a result of the 2008 National End of Life Care Strategy ( Department of Health 2008 ). Explanations for the large proportion of people still dying in hospital include poorly co\u2010ordinated services with variable provision, making it difficult for people to be transferred between settings ( National Audit Office 2008 ). Improved collaboration between health and social care, and acute and community services, could improve the quality of care, reduce emergency admissions, and allow more people to die in the place of their choosing ( National Audit Office 2008 ). Importantly, although between 30% and 90% of people die in their preferred death place, the likelihood of doing so decreases if they had chosen home ( Bell 2010 ). This is the fourth update of the original review.",
        "summary": "Many countries support a policy of providing people with a terminal illness the choice of dying at home. When adults with cancer, chronic obstructive pulmonary disease or heart failure at the end\u2010stage of life received specialized home\u2010based end\u2010of\u2010life care (multidisciplinary care, including specialist palliative\u2010care nurses, family physicians, palliative\u2010care consultants, physiotherapists, occupational therapists, nutritionists, and social care workers) more did indeed die at home compared with usual care (on average 591 versus 444 died at home per 1000 people). Usual care varied considerably across studies; two trials used home care but this was not specialized end\u2010of\u2010life care. Moderate\u2010quality evidence detected no differences between groups in terms of admission to hospital during 6 to 24\u2010months follow\u2010up or the number of inpatient days in two years. Results for control of symptoms, patient satisfaction, and caregiver burden were inconsistent. Delay in care and the number of participants who died in their preferred place of death were not reported in any trial."
    },
    "CD008093": {
        "query": "How does amiodarone compare with placebo for the prevention of sudden cardiac death in at\u2010risk adults?",
        "document": "Background Description of the condition From a clinical point of view, any unexpected death can be considered a 'sudden death', brought on by conditions as diverse as arrhythmias, aortic dissection, subarachnoid haemorrhage, acute myocardial infarction or massive pulmonary embolism. Traumatic death is usually excluded from this category. As there may be prognostic and therapeutic differences (i.e. subarachnoid haemorrhage or pulmonary embolism), researchers and clinicians recognise a distinct category known as 'sudden cardiac death' (SCD). A widely accepted definition is \"natural death due to cardiac causes, heralded by abrupt loss of consciousness within an hour of the onset of acute symptoms; pre\u2010existing heart disease may have been known to be present, but the time and mode of death are unexpected\" ( Myerburg 2004 ). SCD is one of the leading causes of cardiac death. Incidence increases with age and is three to four times more frequent in men than women at all ages ( Merghani 2013 ; MMWR 2002 ). Accurately estimating its real incidence is difficult, but according to data obtained from death certificates, SCD may cause 63.4% of total cardiac mortality in the United States ( MMWR 2002 ). This data probably overestimates SCD prevalence, as it is based only on clinical presentation ( MMWR 2002 ; Zheng 2001 ). Incidence rates, varying from 0.36 to 1.28/100,000 participants per year, have been reported by some emergency services, but these tend to underestimate the real incidence, as it only refers to participants who survive to the hospital ( Sara 2014 ). Incidence increases from 1/100,000 for those aged < 35 years to 100/100,000 in individuals aged \u2265 35 years old ( John 2012 ). A prospective observational study reported that 7% to 18% of overall mortality in the general population (of all ages) in the USA was due to SCD ( Stecker 2014 ). Globally, estimated incidence of SCD would then be approximately 4 to 5 million cases every year. However, this number may be inaccurate, as, on the one hand, SCD incidence rates in low\u2010 and middle\u2010income countries may not be equivalent to those in high\u2010income countries ( Vedanthan 2012 ), and on the other hand, incidence has declined over the past two decades, from 4.7 per 1000 person\u2010years in 1990\u20132000 to 2.1 per 1000 person\u2010years in 2001\u20132010 ( Niemeijer 2015 ). The main pre\u2010existing heart disease leading to SCD in high\u2010income countries is coronary heart disease (CHD); there is a general acceptance that SCD accounts for around 50% of all CHD\u2010related death and that the proportion of all SCDs resulting from CHD is around 80% ( Myerburg 2012 ). Other types of cardiopathy (e.g. hypertrophic cardiomyopathy, non\u2010ischaemic cardiomyopathy, arrhythmogenic right ventricular dysplasia) can also lead to SCD, while there is no structural abnormality in only 5% of cases ( Consensus 1997 ). The SCD event is most commonly caused by the sudden onset of monomorphic ventricular tachycardia (VT) that degenerates into ventricular fibrillation (VF), and less frequently by the abrupt onset of polymorphic VT/VF, bradyarrhythmias or heart blocks ( Priori 2001 ; Zipes 2006 ). However, the proportion of participants with pulseless electrical activity or asystole has increased over the past two decades ( Teodorescu 2010 ). More recently, research has identified diabetes mellitus as an independent risk factor for SCD ( Jouven 2005 ). The downward trend in SCD incidence might be due to better diagnosis and treatment of heart disease and most importantly primary prevention of SCD and cardiovascular disease in general through improved management of behaviours and other risk factors ( Niemeijer 2015 ). In this context, clinicians use electrophysiological (EP) testing with intracardiac recording and electrical stimulation at baseline, followed by administration of antiarrhythmic drugs for arrhythmia assessment and risk stratification. EP testing has been used to document the inducibility of VT, evaluate drug effects, assess the risks of recurrent VT or SCD, and assess the indications for implantable cardiac defibrillator (ICD) therapy. For example, in participants with CHD, asymptomatic non\u2010sustained VT and a left ventricular ejection fraction (LVEF) less than 40%, the inducibility of sustained VT ranges from 20% to 40% and confers worse prognosis, with an increased risk of SCD or death from other causes ( Buxton 2000 ). However, in participants with CHD and a lower LVEF (less than 30%), non\u2010inducibility doesn't necessarily portend a good prognosis ( Buxton 2002 ), and persistent inducibility while receiving antiarrhythmic drugs confers an even worse prognosis ( Wilber 1990 ). Description of the intervention Amiodarone, one of the main class III antiarrhythmics, is a benzofuran derivative approved by the US Food and Drug Administration (FDA) for the treatment of patients with life\u2010threatening ventricular tachyarrhythmias when other drugs are ineffective or not tolerated ( FDA 2013 ). Researchers have proposed the drug as an alternative to ICD, categorising it as having a 2a level of evidence (weight of evidence is in favour of usefulness/efficacy) for prophylaxis of SCD in participants with CHD and left ventricular (LV) dysfunction ( Zipes 2006 ). The onset of action after intravenous administration is generally within one to two hours, but after oral administration, the onset of action may require anywhere from two to three days and often from one to three weeks. On occasion, it may take even longer, to the point that achieving a steady state without a loading dose takes about 265 days ( Braunwald 2001 ). However, research has also associated the use of amiodarone with toxicity involving the lungs, thyroid gland, liver, eyes, skin and nerves ( Connolly 1997 ). Pulmonary toxicity is the drug's most serious potential adverse effect, and some series have described its frequency as high as 17%, although the incidence when compared with placebo is less than 1% ( Pollak 1999 ). Thyroid toxicity is the most common complication requiring intervention, occurring in up to 10% of participants receiving long\u2010term amiodarone therapy. Minor adverse effects are nausea, anorexia, photosensitivity, and a blue discolouration of the skin ( Siddoway 2003 ). The frequency of most adverse effects is related to total amiodarone exposure ( Siddoway 2003 ), but amiodarone is slowly, variably and incompletely absorbed, which makes adverse events unpredictable. Extensive hepatic metabolism occurs with desethylamiodarone as a major metabolite, both extensively accumulating in the liver, lung, fat, 'blue' skin, and other tissues. How the intervention might work Generally speaking, class III antiarrhythmic drugs act by prolonging the action potential\u00b4s duration of the myocardial cell by lengthening the repolarisation phase and thus the effective refractory period. This prolongation is believed to facilitate termination and prevention of both ventricular re\u2010entry arrhythmias by producing block within re\u2010entry circuits, and thus providing both an elevation of the ventricular fibrillation threshold and a reduction of the ventricular defibrillation threshold ( Brendorp 2002 ). As a class III antiarrhythmic drug, amiodarone prolongs the QT interval, slows the heart rate and atrioventricular nodal conduction (via calcium channel and beta\u2010receptor blockade), prolongs refractoriness (via potassium and sodium channel blockade), and slows intracardiac conduction (via sodium channel blockade) ( Siddoway 2003 ). By blocking the potassium repolarisation currents, it can inhibit or terminate ventricular arrhythmias by increasing the wavelength for reentry ( Zipes 2006 ). Why it is important to do this review According to current evidence, ICD therapy, when compared with antiarrhythmic drugs, reduces mortality in high risk participants with reduced LVEF in both CHD and non\u2010ischaemic cardiopathy, for both primary and secondary prevention ( AVID 1997 ; Connolly 2000 ; Desai 2004 ; Kuck 2000 ). While ICD therapy may improve survival in selected patient populations, it may diminish patients' quality of life ( Gehi 2006 ). In a study comparing ICD versus no ICD in participants who underwent coronary artery bypass graft surgery, the use of ICD was associated with lower levels of psychological well\u2010being and reduced physical and emotional role functioning ( Namerow 1999 ). On the other hand, a recent analysis by Marks et al. from the Sudden Cardiac Death \u2010 Heart Failure Trial (SCD\u2010HeFT) showed that subjective measures of physical function did not differ significantly between the ICD and placebo groups at any time point, but there was a short\u2010term increase in psychological well\u2010being among participants with ICD therapy throughout the first year after implantation, a benefit that did not persist at 30 months ( Bardy 2005\u2013SCD\u2010HeFT ). The occurrence of ICD shocks reduced the quality of life, but only if quality of life was measured within one to two months after the shock ( Bardy 2005\u2013SCD\u2010HeFT ). However, the elevated up\u2010front costs of ICD therapy (between EUR 11,000 and 19,000) impede its ready availability in the health systems of low\u2010 and middle\u2010income countries, even though costs tend to diminish along patients' longevity ( Biffi 2011 ). Regarding amiodarone, evidence from trials has been inconsistent, with some studies showing a moderate effect and others no effect at all ( Bardy 2005\u2013SCD\u2010HeFT ; Heidenreich 2002 ; Strickberger 2003 ). Indirect evidence of effect comes from a recent systematic review concluding that ICD discharges were reduced in participants with ICD plus amiodarone compared to participants with ICD alone. Assuming ICD discharges follow ventricular arrhythmias, one could infer that amiodarone reduces the number of arrhythmic episodes ( Ferreira 2007 ), provided the ICDs were programmed with similar arrhythmia detection times, as different detection times could mean different thresholds for considering any disturbance to be an arrhythmic episode ( Scott 2014 ). On the other hand, data collected since the 1980s has convincingly proven that beta\u2010blocking treatment is associated with an improved clinical outcome in several patient groups. The efficacy of this treatment in people with post\u2010myocardial infarction (MI) relates to a drug\u2010associated reduction in all\u2010cause mortality and is not necessarily related to the time after the acute event, when therapy starts ( Yusuf 1985 ). People with a history of congestive heart failure (CHF) or depressed left ventricular function tend to experience the greatest benefits in mortality reduction. Data suggests that in participants post\u2010MI, potasium channel blockers such as dofetilide or d\u2010sotalol have neutral or even harmful effects regarding all\u2010cause mortality ( K\u00f8ber 2000 ; Torp\u2010Pedersen 1999 ; Waldo 1996 ), and, for example, both of these drugs result in a higher rate of Torsade de Pointes than amiodarone ( Brendorp 2002 ). However, calcium\u2010channel blockers such as verapamil have shown favourable effects, if only in people without heart failure ( DAVIT II 1990 ). It is also important to note that chronic treatment with antiarrhythmic drugs is associated with severe adverse effects, including the potential induction of life\u2010threatening arrhythmias (e.g. increased mortality is associated with the long\u2010term use of quinidine; Coplen 1990 ). A previous systematic review of randomised controlled trials, which evaluated amiodarone versus other antiarrhythmics or placebo for the prevention of SCD in participants with or without ICD, concluded that it significantly reduced SCD and cardiac mortality, but not all\u2010cause mortality. However, the authors did not carry out a separate analysis for participants with or without ICD, and the review only evaluated amiodarone for primary prevention ( Piccini 2009 ). If amiodarone proves to be beneficial in SCD prevention, it would constitute a valid alternative in situations where economic constraints limit the widespread use of ICD.",
        "summary": "Very low\u2010 to low\u2010quality evidence suggests that amiodarone may reduce cardiac\u2010related deaths when used for primary prevention, but this may be at an increased risk of thyroid and pulmonary adverse events. There is some question over the appropriateness of assessing primary prevention populations for a placebo comparison; amiodarone is not a first\u2010line treatment due to its adverse effects, and would most likely only be prescribed if other treatments, such as beta\u2010blockers, were not tolerated or had failed. When amiodarone was compared with placebo in people with a range of cardiovascular diseases, evidence suggests that sudden cardiac death was reduced with amiodarone when used for primary prevention (on average 70 versus 91 per 1000 people; low\u2010quality), but not secondary prevention (very low quality). The secondary prevention trials included only 440 participants, and therefore, given the extremely low event rates, would likely have been too small to detect a difference between groups if one was present. Overall cardiac mortality was also reduced with amiodarone in the primary prevention trials (103 versus 120 per 1000 people; 17 RCTs, 8383 participants); this outcome was not reported in the secondary prevention trials. There was no apparent positive or negative effect of amiodarone on quality of life at 30 months. In terms of adverse events, hyperthyroidism (22 versus 5 per 1000 people), hypothyroidism (extremely low event rate) and pulmonary events (20 versus 12 per 1000 people) were all more commonly associated with amiodarone when a placebo was used as the comparator. When the comparator was no treatment (subject to performance and possibly detection bias), no differences were detected between groups. More people withdrew from treatment due to adverse events with amiodarone (324 versus 223 per 1000 people). Most of the trials were conducted in people who were being treated for the primary prevention of cardiovascular events. However, even within these trials, the populations were extremely heterogeneous. There is also some question over the appropriateness of assessing primary prevention populations for a placebo comparison; amiodarone is not a first\u2010line treatment due to its adverse effects, and would most likely only be prescribed if other treatments, such as beta\u2010blockers, were not tolerated or had failed. These people were not the focus of the trials, and the benefits and harms of amiodarone in this subgroup of refractory patients are therefore unclear. There was also a variety of amiodarone regimens used across the trials. This will make it difficult to apply the pooled results to specific population/treatment regimen scenarios."
    },
    "CD008929": {
        "query": "What are the effects of interventions for people with skeletal muscle spasticity following traumatic brain injury?",
        "document": "Background Traumatic brain injury (TBI) is the result of an external force to the head, that can lead to permanent damage to the brain. There are many causes of TBI including motor vehicle accidents, falls, violent assaults or blast injuries ( Maas 2008 ). In 2005, The US Centre for Disease Control and Prevention estimated at least 3.17 million Americans, approximately 1.1% of the US population, are living with long\u2010term disability as a result of TBI ( Summers 2009 ). In Europe, the incidence of TBI in studies published between 1983 and 2013 has been estimated to be between 47.3 to 849 per 100,000 population per year ( Brazinova 2016 ). There are limited data available for low\u2010 to middle\u2010income countries. The impact of TBI to a person can be far\u2010reaching and may result in ongoing physical, cognitive and behavioural issues ( Khan 2003 ). Skeletal muscle spasticity is one of the major physical complications following TBI ( Brashear 2016 ). Description of the condition Spasticity is defined as an ongoing contraction of a muscle caused by an increase in muscle tone and deep tendon reflexes that is partly due to a reduction of the skeletal stretch reflex threshold ( Lance 1980 ). It is often described as muscle overactivity. Spasticity occurs due to damage of upper motor neurons (UMN) of the corticoreticular pathways in the brain cortex or internal capsule, or damage to the UMNs in the reticulospinal or vestibulospinal tracts in the spinal cord ( Pandyan 2005 ). Spasticity may occur sporadically or continuously, for periods of short and long duration. Spasticity tends to affect the antigravity muscle groups in the upper and lower limbs ( Nair 2014 ). In the upper limbs, this can commonly include the shoulder adductors, elbow, wrist and finger flexors, forearm pronators and thumb adductors. In the lower limbs, spasticity often affects the hip adductors, knee flexors, ankle plantarflexors and invertors, and big toe extensors ( Nair 2014 ). Spasticity can also affect muscles in the neck. There are limited epidemiological data regarding the prevalence of spasticity following TBI ( Martin 2014 ; McGuire 2016 ). In one systematic review of the epidemiology of lower limb spasticity, Martin 2014 identified only one study ( Singer 2004 ), conducted in 105 people with TBI that found the prevalence of ankle spasticity was 13%. Similarly, McGuire 2016 was only able to find one study of spasticity prevalence following TBI ( Wedekind 2005 ), which was a study of 32 people with TBI, in which the prevalence of spasticity (location unclear) was 32%. While both the definition and measurement of spasticity is inconsistent, and often poorly defined ( Malhotra 2009 ), McGuire 2016 suggests that by extrapolating data from studies in other populations (including people with stroke, spinal cord injury and multiple sclerosis), 'problematic' spasticity may occur in between 30% and 50% of people with TBI. Spasticity can lead to a range of musculoskeletal issues such as muscle contracture, involuntary and uncontrollable shaking, joint stiffness, reduced range of movement, broken skin and pain ( Ada 2006a ; Ada 2006b ). The debilitating nature of the condition can directly impact a person's ability to carry out normal activities of daily living (ADL), such as self\u2010care and household tasks, and is likely to lead to dependency on carers or family members for assistance. Participation in daily life and opportunities for community integration can prove difficult and may ultimately impact the person's quality of life ( Kwakkel 1999 ). Management of spasticity in people with TBI varies from other clinical populations primarily due to behavioural and cognitive issues that affect their ability to participate in, or tolerate, treatment (e.g. their ability to follow instructions, monitor use of a spastic limb or tolerate a cast). These factors are likely to impact on whether or not a treatment is effective in this population and limit the applicability of findings from other populations where behavioural and cognitive issues are not a primary concern ( Manchester 1997 ; Wood 1999 ). Furthermore, some studies have shown that mobility limitations can be improved over time and that the presence, distribution and severity of spasticity may not necessarily be the best determinant in recovery and mobility outcomes ( Williams 2015a ; Williams 2015b ). The measurement of spasticity in clinical practice, and in research, is challenging ( Malhotra 2009 ). The most common scales to measure spasticity are the (Modified) Ashworth Scale ( Pandyan 1999 ) and the (Modified) Tardieu Scale ( Haugh 2006 ). The (Modified) Ashworth Scale is commonly used by clinicians as it is the easier scale to complete ( Pandyan 1999 ), however it only measures the resistance in a muscle, which may or may not be caused by spasticity ( Patrick 2006 ). This is in contrast to the (Modified) Tardieu Scale which measures spasticity by the spasticity catch angle as well as resistance in the muscle ( Haugh 2006 ). The Tardieu has also demonstrated greater test\u2010retest and inter\u2010rater reliability compared with the Modified Ashworth Scale ( Mehrholz 2005 ). The (Modified) Tardieu includes a continuous and nominal (or sometimes considered ordinal ( Haugh 2006 )) component, and the (Modified) Ashworth is ordinal, but both are commonly treated as continuous scales by trialists. While there does not appear to be a clear consensus about the most appropriate way to analyse these scales, we note that for another five\u2010point ordinal scale used in TBI (the Glasgow Outcome Scale ( Jennett 1975 )), a sliding dichotomy or proportional odds methodology is recommended for analysis ( Maas 2010 ). Description of the intervention Interventions for managing skeletal muscle spasticity can be broadly categorised as either pharmacological or non\u2010pharmacological. Examples of pharmacological interventions include baclofen ( Becker 1997 ), botulinum toxin A, clonidine, dantrolene sodium, tizanidine and phenol injection ( Meythaler 2001a ; Yelnik 2009 ). Examples of non\u2010pharmacological interventions include casting, splinting, stretching, strengthening, transcutaneous electric nerve stimulation (TENS) ( Aydin 2005 ), Bobath treatment, weight bearing gait training and seating. In practice, a combination of both pharmacological and non\u2010pharmacological interventions is used to manage spasticity. Interventions can be either focal or systemic in their action. Focal interventions involve treatment of one or two muscle groups whereas systemic interventions are used to treat generalised spasticity. How the intervention might work Interventions used to manage spasticity all aim to reduce overactivity within the muscle so that it can be lengthened ( Esquenazi 2006 ). Pharmacological interventions can act locally at the muscle or systemically through the central nervous system. For example, botulinum toxin A and phenol are both injected locally at the site of the spastic muscle whereas other interventions such as tizanidine and clonidine are administered orally. Oral medications act systemically and can induce unwanted adverse effects, such as drowsiness. For those with severe spasticity in multiple areas, adverse effects associated with oral medications can sometimes outweigh the potential benefits of reduced spasticity. An alternative approach, as seen with baclofen, is to administer the treatment through a pump into the space around the spinal cord, thus reducing the impact of adverse effects whilst maintaining improved outcomes for spasticity ( Becker 1997 ). Non\u2010pharmacological interventions involve the use of physical modalities such as stretching and strengthening to promote elongation and control of spastic muscles, or external devices such as casting or splinting to modify and maintain correct positioning of a spastic muscle. Table 1 provides a summary of the range of different interventions and their mode of action. Pharmacological intervention Mode of action Non\u2010pharmacological intervention Mode of action Baclofen Administered orally or via intrathecal pump to limit the release of excitatory neurotransmitters in the spinal cord. Casting Applied directly to the limb to maintain the muscle in an extended position. Botulinum toxin A A neurotoxin injected directly into the muscle to block the release of the neurotransmitter acetylcholine. Splinting Thermoplastic or fabric material that is customised to provide support to a person's limb and maintain the limb in the corrected position. Clonidine Administered orally or by transdermal patch to act on the central nervous system by reducing the excitability of alpha motor neurons. Seating Custom made seating for people to provide maximal support and reduce the impact of spasticity. Dantrolene sodium Administered orally to reduce the excitation\u2010contraction coupling within the skeletal muscle and decrease the strength of muscle contraction. Stretching Promotes elongation of a muscle for varying lengths of time causing viscous deformation changes. Phenol Injected into specific nerves to induce neurolysis to permanently block nerve transmission. Transcutaneous electric nerve stimulation Portable electric stimulator placed on the skin over a spastic muscle to reduce pain. Tizanidine Administered orally to act on the central nervous system and reduce the excitability of alpha motor neurons. Surgery Surgical techniques primarily aim to alter the structure of a muscle or nerve or relocate a tendon to change its function. Pharmacological intervention Mode of action Non\u2010pharmacological intervention Mode of action Baclofen Administered orally or via intrathecal pump to limit the release of excitatory neurotransmitters in the spinal cord. Casting Applied directly to the limb to maintain the muscle in an extended position. Botulinum toxin A A neurotoxin injected directly into the muscle to block the release of the neurotransmitter acetylcholine. Splinting Thermoplastic or fabric material that is customised to provide support to a person's limb and maintain the limb in the corrected position. Clonidine Administered orally or by transdermal patch to act on the central nervous system by reducing the excitability of alpha motor neurons. Seating Custom made seating for people to provide maximal support and reduce the impact of spasticity. Dantrolene sodium Administered orally to reduce the excitation\u2010contraction coupling within the skeletal muscle and decrease the strength of muscle contraction. Stretching Promotes elongation of a muscle for varying lengths of time causing viscous deformation changes. Phenol Injected into specific nerves to induce neurolysis to permanently block nerve transmission. Transcutaneous electric nerve stimulation Portable electric stimulator placed on the skin over a spastic muscle to reduce pain. Tizanidine Administered orally to act on the central nervous system and reduce the excitability of alpha motor neurons. Surgery Surgical techniques primarily aim to alter the structure of a muscle or nerve or relocate a tendon to change its function. Why it is important to do this review A range of interventions are currently used to manage skeletal muscle spasticity for people with TBI. Clinical management often involves a combination of pharmacological and non\u2010pharmacological interventions. Management of spasticity following TBI varies from other clinical populations because of the added complexity of behavioural and cognitive issues associated with TBI. Furthermore, the current management for spasticity in other conditions may not be applicable to TBI as a result of global UMN damage that can occur such as axonal shearing, haemorrhage and hypoxia. A comprehensive systematic review of interventions assessed in the TBI population is needed to identify those likely to have the greatest impact for managing spasticity as well as determining whether the severity of TBI or the timing of an intervention is relevant to the outcome. Reviews of interventions for spasticity in people with TBI are focused solely on the effect of one intervention (botulinum toxin A), and include people with spasticity due to other conditions, predominantly stroke ( Baker 2015 ; Dashtipour 2016 ; Dong 2017 ). None of these reviews allow conclusions to be drawn specific to people with TBI. Systematic reviews that consider the effects of a broader range of interventions for managing spasticity have done so in populations other than TBI such as stroke ( Taricco 2000 ) and spinal cord injury ( Demetrios 2013 ; Hsieh 2012 ). To our knowledge, there are no systematic reviews that consider all potential interventions used to manage skeletal muscle spasticity specifically for people with TBI.",
        "summary": "A total of eight RCTs compared baclofen with placebo; botulinum toxin A with placebo; tizanidine with placebo; physiotherapy with casting (with or without botulinum toxin A); pseudoelastic orthosis with static splint; or tilt table standing and electrical stimulation plus ankle splinting with tilt table standing alone, but all trials were too small to provide clinically meaningful results (between 6 and 36 participants)."
    },
    "CD011384": {
        "query": "What are the effects of lexipafant and gabexate\u2010containing regimens for adults with acute pancreatitis?",
        "document": "Background Description of the condition The pancreas is an abdominal organ that secretes several digestive enzymes into the pancreatic ductal system before it empties into the small bowel. The pancreas also lodges the Islets of Langerhans, which secrete several hormones including insulin ( NCBI 2014 ). Acute pancreatitis is a sudden inflammatory process in the pancreas, with variable involvement of nearby organs or other organ systems ( Bradley 1993 ). The annual incidence of acute pancreatitis ranges from 5 to 30 per 100,000 population ( Roberts 2013 ; Yadav 2006 ). There has been an increase in the incidence of acute pancreatitis in the last 10 to 20 years in the UK and USA ( Roberts 2013 ; Yang 2008 ). Acute pancreatitis is the commonest gastrointestinal (digestive tract) cause of hospital admission in the USA ( Peery 2012 ), and gallstones and alcohol are the two main causes. Approximately, 50% to 70% of acute pancreatitis is caused by gallstones ( Roberts 2013 ; Yadav 2006 ); these slip into the common bile duct and obstruct the ampulla of Vater (a common channel formed by the union of common bile duct and pancreatic duct), resulting in obstruction to the flow of pancreatic enzymes and leading to activation of trypsinogen within the pancreas and acute pancreatitis ( Sah 2013 ). Advanced age, male sex, and lower socioeconomic class are associated with higher incidence of acute pancreatitis ( Roberts 2013 ). Clinicians generally diagnose acute pancreatitis when at least two of the following three features are present ( Banks 2013 ). Acute onset of a persistent, severe, epigastric pain, often radiating to the back. Serum lipase activity (or amylase activity) at least three times greater than the upper limit of normal. Characteristic findings of acute pancreatitis on contrast\u2010enhanced computed tomography (CECT) and less commonly magnetic resonance imaging (MRI) or transabdominal ultrasonography. Acute onset of a persistent, severe, epigastric pain, often radiating to the back. Serum lipase activity (or amylase activity) at least three times greater than the upper limit of normal. Characteristic findings of acute pancreatitis on contrast\u2010enhanced computed tomography (CECT) and less commonly magnetic resonance imaging (MRI) or transabdominal ultrasonography. Depending upon the type of inflammation, acute pancreatitis can be classified into interstitial oedematous pancreatitis (diffuse (widespread) or occasionally localised enlargement of the pancreas due to inflammatory oedema as seen on CECT) or necrotising pancreatitis (necrosis involving either the pancreas, peripancreatic tissues, or both) ( Banks 2013 ). Approximately 90% to 95% of people with acute pancreatitis have interstitial oedematous pancreatitis, while the remainder have necrotising pancreatitis ( Banks 2013 ). Necrotising pancreatitis may be sterile or infected ( Banks 2013 ). Various theories exist as to how pancreatic and peripancreatic tissues get infected. These include spread from blood circulation, lymphatics, bile, and the small bowel (duodenum) through the pancreatic duct, as well as movement (translocation) through the large bowel wall ( Schmid 1999 ). Local complications of acute pancreatitis include acute peripancreatic fluid collection, pancreatic pseudocyst, acute necrotic collection, and walled\u2010off necrosis ( Banks 2013 ). The systemic complications of acute pancreatitis include worsening of pre\u2010existing illnesses such as heart or chronic lung disease ( Banks 2013 ). The mortality rates following an attack of acute pancreatitis are between 6% and 20% ( Roberts 2013 ; Yadav 2006 ), according to severity. Acute pancreatitis can be classified as mild, moderate, or severe, depending on the presence of local or systemic complications, transient organ failure involving one of more of lungs, kidneys, and cardiovascular system (heart and blood vessels) lasting up to 48 hours, or persistent failure of these organs lasting beyond 48 hours. Mild pancreatitis has the best prognosis, and there are no local or systemic complications or organ failure. In moderately severe acute pancreatitis, there may be local or systemic complications or transient organ failure. Severe acute pancreatitis carries the worst prognosis in terms of mortality, and there is persistent organ failure ( Banks 2013 ). The clinical manifestation of acute pancreatitis is believed to be caused by activation of inflammatory pathways either directly by the pathologic insult or indirectly by activation of trypsinogen (an enzyme that digests protein or a protease), resulting in formation of trypsin, a protease that can break down the pancreas ( Sah 2013 ). This activation of inflammatory pathways manifests clinically as systemic inflammatory response syndrome (SIRS) ( Banks 2013 ; Sah 2013 ; Tenner 2013 ). Systemic inflammatory response syndrome is characterised by two or more of the following criteria ( Bone 1992 ). Temperature of less than 36\u00b0C or more than 38\u00b0C. Heart rate less than 90 beats/minute. Respiratory rate more than 20/min or PCO\u2082 less than 32 mm Hg. White blood cell count more than 12,000/mm\u00b3, less than 4000/mm\u00b3, or more than 10% immature (band) forms. Temperature of less than 36\u00b0C or more than 38\u00b0C. Heart rate less than 90 beats/minute. Respiratory rate more than 20/min or PCO\u2082 less than 32 mm Hg. White blood cell count more than 12,000/mm\u00b3, less than 4000/mm\u00b3, or more than 10% immature (band) forms. See Appendix 1 for a glossary of terms. Description of the intervention The main purpose of treatment is to decrease the mortality and morbidity associated with acute pancreatitis. The various pharmacological interventions that have been evaluated in the treatment of acute pancreatitis include agents such as somatostatin or octreotide that decrease pancreatic secretions; protease inhibitors such as gabexate mesilate, aprotinin, ulinastatin, and nafamostat; antioxidants such as vitamin C and selenium; platelet activating factor such as lexipafant; other agents that modulate the inflammatory pathway such as steroids and tumour necrosis factor\u2010alpha (TNF\u2010\u03b1) antibody; probiotics; and antibiotics ( Bang 2008 ; Neumann 2011 ; Rada 2011 ; Yang 2011 ). We included any pharmacological intervention aimed at the treatment of acute pancreatitis. We did not cover endoscopic sphincterotomy for the treatment of common bile duct stones ( Ayub 2010 ), nor did we focus on endoscopic, radiology\u2010guided percutaneous treatments or surgical treatments for treatment of complications of acute pancreatitis ( Tenner 2013 ). Furthermore, we did not cover the use of non\u2010steroidal anti\u2010inflammatory drugs (NSAIDs) or other drugs such as somatostatin analogues for preventing postendoscopic retrograde cholangiopancreatography (post\u2010ECRP)\u2010induced pancreatitis ( Elmunzer 2012 ; Zhang 2009 ). How the intervention might work Somatostatin and its analogues decrease pancreatic secretion ( Bang 2008 ). Since autodigestion (breakdown of pancreas) due to trypsinogen activation is one of the mechanisms believed to cause acute pancreatitis, decreasing pancreatic secretion can decrease the amount of trypsinogen. Inhibition of trypsin by protease inhibitors may result in decreased damage to the pancreas ( Neumann 2011 ). Antioxidants, platelet\u2010activating factor inhibitors, steroids, and TNF\u2010\u03b1 antibody are all aimed at decreasing the inflammatory response or at mitigating the damage resulting from the inflammatory response ( Bang 2008 ). Probiotics decrease the bacterial colonisation of the gut, and antibiotics have antibacterial actions ( Bang 2008 ). Why it is important to do this review Despite various pharmacological interventions being evaluated in acute pancreatitis, none is currently recommended in the treatment of acute pancreatitis, with the exception of antibiotics in infected necrotising pancreatitis ( Tenner 2013 ). Systematic reviews and meta\u2010analyses increase the precision of the treatment effects (i.e. they provide a narrower range of the average treatment effect) ( Higgins 2011 ), and so decrease the risk of a type II error (concluding that there is no difference between treatments when there is actually a difference). Systematic reviews also help in identifying the differences in the treatment effects between studies and allow exploration of the reasons behind these differences. Many studies have compared these interventions with placebo or with no treatment. It is therefore not possible to obtain accurate information on how one treatment compares with another treatment. Multiple treatment comparisons or a network meta\u2010analysis allow comparison of several treatments simultaneously and provide information on the relative effect of one treatment versus another, even when there is no direct comparison. There is no Cochrane Review or network meta\u2010analysis on this topic. So, we planned to perform a network meta\u2010analysis if the type of participants were included across all the comparisons. This systematic review will identify the relative effects of different treatments and identify any research gaps.",
        "summary": "Randomized controlled trials found little or no evidence to suggest that lexipafant or gabexate\u2010containing treatments are effective or unsafe for adults with acute pancreatitis; evidence is of low quality, and no firm conclusions can be drawn. Evidence of very low quality suggests no effect of lexipafant or gabexate\u2010containing treatment on short\u2010term mortality (up to three months) versus placebo or alternative treatment in adults with acute pancreatitis. Randomized controlled trials (RCTs) also found no clear effects of lexipafant or gabexate\u2010containing treatment on length of intensive care unit (ICU) or hospital stay versus placebo, but one RCT showed a reduction in length of hospital stay with gabexate\u2010containing regimens versus non\u2010gabexate\u2010containing somatostatin\u2010based regimens (on average, by approximately 5 to 13 days, depending on the specific comparison). Three RCTs with 426 participants reported that fewer additional invasive interventions were required with gabexate than with placebo (on average, 148 vs 231 per 1000 people); RCTs reported no such benefit of gabexate\u2010containing treatments over aprotinin\u2010containing ones. In terms of adverse events associated with lexipafant, a single RCT showed no clear difference between lexipafant and placebo in the number of people experiencing an adverse event. However, one RCT (290 participants) suggested that the overall number of adverse events experienced by people taking lexipafant was lower than for those taking placebo. This RCT also reported fewer serious adverse events with lexipafant than with placebo (on average, 292 vs 437 per 1000 people). Very low\u2010quality evidence suggests no difference in organ failure between lexipafant and placebo. In terms of adverse events, fewer people experienced an adverse event with gabexate\u2010based regimens than with aprotinin (on average, 127 vs 264 per 1000 people); RCTs detected no clear differences between groups for any other comparison. Regarding serious adverse events, organ failure, and sepsis, RCTs noted no clear differences between gabexate\u2010 and non\u2010gabexate\u2010containing treatments. No RCTs assessed long\u2010term mortality, quality of life, or time to return to normal activity. The severity of pancreatitis, as well as the timing of the start of pharmacological treatment in the course of acute pancreatitis, might influence treatment effects. The included studies were highly heterogeneous in this regard, making interpretation of trial results difficult."
    },
    "CD009624": {
        "query": "In adults with chemotherapy\u2010induced anemia, how does adding iron supplementation to erythropoiesis\u2010stimulating agents (ESAs) compare with ESAs alone?",
        "document": "Background The majority of cancer patients undergoing chemotherapy develop chemotherapy\u2010induced anemia (CIA) ( Kitano 2007 ; Knight 2004 ; Leonard 2005 ; Ludwig 2004 ; Pujade\u2010Lauraine ). Approximately 83% of people receiving chemotherapy develop CIA ( Barrett\u2010Lee 2006 ). In people undergoing myelosuppressive chemotherapy or radiation therapy, or both, the incidence is as high as 70% to 90%, and it is about 60% in people with solid tumors and lymphomas ( Schwartz 2007 ). The majority of people with CIA suffer from fatigue, weakness, and dyspnea, leading to decreased quality of life and performance status ( Littlewood 2001 ; Mancuso 2006 ; Stasi 2003 ). The overall goal of treatment in people with CIA is reduction in transfusion requirements and maximization of quality of life ( Rizzo 2008 ; Rizzo 2010 ). The National Comprehensive Cancer Network (NCCN) guidelines, NCCN 2009 , and the European Organisation of Research and Treatment of Cancer (EORTC) guidelines, Aapro 2008 , recommend red blood cell (RBC) transfusion as an effective strategy to manage CIA because it leads to replacement of depleted hemoglobin (Hb). However, research has shown the effect of RBC transfusion to be temporary and possibly associated with serious thromboembolic events and increased mortality ( Khorana 2008 ; Mercadante 2009 ). An alternative to RBC transfusion in treating CIA in cancer patients involves the use of erythropoiesis\u2010stimulating agents (ESAs). ESAs are man\u2010made proteins that stimulate the production of RBCs in bone marrow when the oxygen level in the blood goes down. ESAs increase Hb levels, reduce transfusion requirements, and improve quality of life ( Demetri 1998 ; Glaspy 1997 ; Littlewood 2001 ; Rizzo 2002 ). However, a recent meta\u2010analysis employing published and unpublished/unreported data from randomized controlled trials (RCTs) showed found no evidence for a clinically relevant improvement of fatigue\u2010related symptoms and only small benefits for anemia\u2010related symptoms in cancer patients receiving ESAs compared to controls ( Bohlius 2014 ). Moreover, evidence from several studies indicates that ESA therapy is also associated with increased risk of thromboembolic events ( Glaspy 2010 ; Rizzo 2008 ). A systematic review of 51 phase III RCTs examining the use of ESAs in the treatment of CIA showed a relative increase of 57% in the risk of blood clots (venous thromboembolism) and a relative increase of 10% in the risk of mortality among participants ( Bennett 2008 ). An individual participant data meta\u2010analysis (53 RCTs, 13,933 participants) examining the effects of two types of ESAs (epoetin and darbepoetin) on the survival of cancer patients showed that ESAs increased overall mortality by 17% in all participants compared to control groups, and by 10% in participants undergoing chemotherapy compared to control groups ( Bohlius 2009 ). For patients undergoing chemotherapy who have a Hb less than 10 g/dL, American Society of Hematology (ASH)/American Society of Clinical Oncology (ASCO) recommend that clinicians should discuss the potential harms (for example increased incidence of thromboembolic events and reduced survival) and benefits (for example decreased RBC transfusions) of ESAs with patients ( Bohlius 2009 ; Bohlius 2014 ; Tonia 2012 ), so that patients' preferences for demonstrated risk guide decisions on CIA treatment ( Rizzo 2010 ). In fact, NCCN discourages the use of ESAs with a curative intent for people undergoing chemotherapy ( NCCN 2010 ). Due to the potential harms associated with ESA treatment, iron has been proposed as an adjunct to ESAs in the management of CIA. Cancer patients suffering from CIA who are treated with ESAs alone are likely to experience increased erythron iron requirements exceeding the available supply (that is functional iron deficiency (FID)) and production of iron\u2010poor erythrocytes in the bone marrow ( Eschbach 2005 ). Co\u2010administration of iron prevents FID and may require a reduced dose of ESAs to attain target Hb levels ( Auerbach 2008a ). However, iron therapy is not without risks. For example, oral iron can cause diarrhea, constipation, stomach upsets, and allergic reactions such as rash, itching, and swelling of face/tongue/throat. High\u2010molecular weight iron dextran is associated with a much higher adverse event rate than the low\u2010molecular weight iron dextran ( Fletes 2001 ; Mamula 2002 ). However, newer preparations of intravenous (IV) iron including low\u2010molecular weight iron dextran, iron sucrose, and ferric gluconate are associated with few adverse events ( Chertow 2004 ; Chertow 2006 ). A number of RCTs have been conducted to assess the efficacy of iron supplementation to ESAs versus ESAs alone for the management of CIA. However, evidence related to efficacy of iron in combination with ESAs compared with ESAs alone in people with CIA is conflicting. Whereas some trials have shown that the use of iron as adjunct to ESAs compared with ESAs alone is associated with improved response to ESAs, increased Hb levels, greater hematopoietic response, and improved health\u2010related quality of life in cancer patients ( Bastit 2008 ; Bellet 2007 ; Hedenus 2007 ; Pedrazzoli 2008 ), others have shown that IV iron had no differential impact on Hb levels, blood transfusions, ESA usage, or patient quality of life compared with oral supplementation or placebo ( Steensma 2011a ). Additionally, studies supporting use of iron supplementation have not definitively addressed the optimal dosage or type and route of administration of iron. The lack of definitive evidence regarding benefits and harms of iron supplementation to ESAs in people with CIA calls for a comprehensive systematic assessment of the effects of iron supplementation to ESAs. Description of the condition Anemia refers to a reduction in the number of RBC counts or hemoglobin (a protein inside the RBCs that contains iron and transports oxygen to different body systems), resulting in a decreased ability of the blood to carry oxygen to body tissues. According to the World Health Organization (WHO), a man with Hb level less than 13 g/dL or a woman with Hb level less than 12 g/dL is considered anemic. People with cancer, especially those undergoing chemotherapy, are susceptible to anemia because they have low erythropoietin levels. CIA occurs when chemotherapy agents attack rapidly diving cells including RBCs, thus preventing them from dividing. Besides disrupting erythropoiesis (the production of red blood cells), chemotherapy may cause mouth sores, taste changes, and nausea, thus reducing intake of nutrients necessary for RBC production. CIA is associated with a reduction in the production of RBCs in the bone marrow, a decrease in erythropoietin, and inadequate iron release. One of the most severe clinical manifestations of CIA is fatigue, experienced by 63% of anemic cancer patients following chemotherapy ( Gabrilove 2007 ). Other symptoms may include insomnia, anorexia, and depression ( van Weert 2006 ); peripheral edema, sustained tachycardia, tachypnea, chest pain, dyspnea on exertion, and orthostatic lightheadedness ( NCCN 2009 ). Description of the intervention A number of RCTs have shown that the use of iron as an adjunct to ESAs may increase the rate at which patients respond to ESA therapy and shorten the length of ESA administration ( Auerbach 2004a ; Bellet 2007 ; Hedenus 2007 ; Henry 2007a ). Iron may be administered either orally or intravenously (IV). People with CIA who are treated with IV iron as opposed to oral iron have experienced a significantly greater Hb response, in Auerbach 2004a and Henry 2007a , and significant reduction in RBC transfusion and lag time to response ( Bastit 2008 ). However, IV iron is more expensive ( Shord 2008 ). Adverse events including allergic and anaphylactoid reactions are associated with iron dextran treatment ( Bailie 2005 ; Shander 2010 ). Examples of oral iron salts currently approved by the US Food and Drug Administration for use in management of CIA include ferrous sulfate, ferrous gluconate, and ferrous fumarate, whereas IV formulations include iron dextran (approved in 1991), iron ferric gluconate (approved in 1999), iron sucrose (approved in 2000), and ferumoxytol (approved in 2009) ( Shander 2010 ). Doses of iron used in recent RCTs include iron dextran total dose infusion or 100 mg bolus injections ( Auerbach 2004a ), ferric gluconate 125 mg once a week for eight weeks ( Henry 2007a ), iron sucrose 100 mg once a week for week one to six and 100 mg every two weeks for week eight to 14 ( Hedenus 2007 ), ferric gluconate or iron sucrose 200 mg every three weeks ( Bastit 2008 ), ferric gluconate 125 mg for six weeks ( Pedrazzoli 2008 ), and iron dextran 400 mg every three weeks ( Auerbach 2008a ). How the intervention might work Erythropoietin is the hormone that facilitates the production of erythrocytes in the bone marrow. Inadequate quantities of iron or erythropoietin, or both result in anemia. Although ESAs have been used to treat CIA in cancer patients, without iron supplementation these patients are likely to experience FID and production of iron\u2010poor erythrocytes in the bone marrow ( Eschbach 2005 ). However, co\u2010administration of iron prevents FID and may require a reduced dose of ESAs to attain target Hb levels ( Auerbach 2008a ). Why it is important to do this review Currently, ESAs are often used to manage CIA. However, about half of patients fail to show an increase in baseline Hb, a reduction in transfusions, or an improvement in function following treatment with ESAs ( Birgegard 2006 ; Henry 1995 ; Razzouk 2006 ). Moreover, the use of ESAs is further restricted due to the associated adverse thromboembolic events. Hence, the use of iron as an adjunct to ESAs has been suggested as a way of circumventing issues related to the use of ESAs alone. However, the findings from RCTs addressing benefits and harms of iron in the management of CIA are conflicting. The findings will provide answers regarding the impact of iron supplementation to ESAs on various outcomes such as hematopoietic response, time to hematopoietic response, and mean change in Hb in people with CIA. The results will also improve our understanding of optimal dose, length of therapy, and route of administration of iron in the management of CIA. This review may not help physicians to make decisions about using iron to manage patients with CIA. It will assist them in decision making regarding use of iron in patients with CIA receiving ESAs.",
        "summary": "In adults with chemotherapy\u2010induced anemia, adding iron to erythropoiesis\u2010stimulating agents (ESAs) may improve outcomes but impact on survival is not known. High\u2010quality evidence shows that supplementation of iron to ESAs results in higher hematopoietic response compared with no iron. Fewer people receiving ESA plus iron required a red cell blood transfusion (moderate\u2010quality evidence), time to hematopoietic response was similar in both groups and hemoglobin level was higher with ESA plus iron compared with no iron (low\u2010quality evidence for both outcomes). High\u2010quality evidence also shows similar quality of life in both groups. Adverse effects were more common with ESA plus iron, but there was no evidence of a difference between groups with regard to thromboembolic events and no incidence of treatment\u2010related deaths was seen. Overall survival was not assessed in any of the trials. In terms of adverse effects, no information on infection was available and subgroup analyses by type of chemotherapy, iron pre\u2010therapy, duration of follow\u2010up or ESA dose could not be performed."
    },
    "CD012059": {
        "query": "What are the benefits and harms of desmopressin in men with nocturia?",
        "document": "Background Description of the condition Nocturia is defined as waking at night one or more times to pass urine with each void being preceded and followed by sleep. This is distinct from nocturnal enuresis, which is defined as voiding during sleep ( Van Kerrebroeck 2002 ). Although somewhat controversial, clinically significant nocturia is defined as one or more voids per night ( Abrams 2002 ). Nocturia is one of the most bothersome lower urinary tract symptoms (LUTS), with a correlation between degree of severity and increased bother ( Bliwise 2009 ). Nocturia occurs when nocturnal urine volume exceeds the maximal voiding volume, which is reflective of the functional bladder capacity ( Bosch 2013 ; Cornu 2012 ). The maximal voiding volume may be different at night than it is during the day. Nocturia has both urologic and non\u2010urologic causes that are behavioural, physiological, or pathologic in nature. Overall, the causes fall into four main categories: reduced bladder capacity secondary to anatomical or functional factors, which may or may not only occur at night; overall increase in urine production (24\u2010hour polyuria); nocturnal polyuria; or any primary or secondary sleep disorder ( Bosch 2013 ; Cornu 2012 ). Benign prostatic hyperplasia (BPH) contributes to, but is not the sole cause of, LUTS, including nocturia. Benign prostatic hyperplasia can cause bladder outlet obstruction, which may induce secondary bladder overactivity and reduction in functional bladder capacity, resulting in nocturia ( Yoshimura 2003 ). Men may also experience nocturia with or without detrusor overactivity or postvoid residual urine ( Berges 2014 ). Nocturia can affect any population at any age, however its incidence generally increases with age. Nocturia defined as one or more micturitions at night affects 20.4% to 43.9% of men aged 20 to 40 years and 68.9% to 93% in men older than 70 years ( Bosch 2013 ). Risk factors for nocturia are dependent upon the underlying causes, which can be overlapping. General risk factors include advancing age, higher body mass index, alcohol consumption, smoking, hypertension, cardiovascular disease, cerebrovascular disease, diabetes mellitus, and metabolic syndrome ( Yoshimura 2003 ). Men with LUTS suggestive of BPH are more susceptible to nocturia than are men without LUTS. Advancing age, smaller functional bladder capacity, increased nocturnal urine volume, and severity of urgency are predictors of frequent nocturnal voiding ( Yoshimura 2003 ). Overall, age is considered the most important risk factor ( Yoshimura 2012 ). Initial assessment of nocturia includes patient history, review of current medications, physical exam including a digital rectal examination in men, urinalysis, measurement of postvoid residual volumes, and validated symptom questionnaires such as the Nocturia\u2010Quality of Life questionnaire (N\u2010QoL) and the International Prostate Symptom Score (IPSS) ( Abraham 2004 ). Other tests that may be necessary include uroflowmetry, urodynamic evaluation of bladder function, cystoscopy, bladder biopsies, sleep laboratory evaluation, and advanced cardiology tests ( Oelke 2014a ). A potential dilemma in diagnosing nocturia in men is that it is often not the only symptom mentioned during medical consultations, and it is rarely the symptom that prompted seeking medical evaluation. Its evaluation is therefore often limited to patient\u2010reported numbers of voids per night as an item on the IPSS and subjective history. The International Continence Society encourages the use of the frequency volume chart, an important diagnostic tool that involves the person recording the time and volume of all voids as well as incontinence episodes and the number of incontinence pads used over a specific time, which is usually seven days ( Van Kerrebroeck 2002 ). Treatment is dependent upon the cause of nocturia as well as the person's individual treatment goals. Goals that are cited as being important include decreasing the number of nocturnal voiding episodes, prolongation of undisturbed sleep to greater than four hours, and improvement of sleep quality ( Oelke 2014a ). Treatment for nocturia can be divided into behavioural modification, medical management, and surgical modalities. Although treatment should be centred on the cause of nocturia, behavioural modifications appear to be universally beneficial regardless of underlying aetiology. Lifestyle modification is therefore often used as first\u2010line therapy ( Van Kerrebroeck 2010 ). Behavioural treatments include restriction of fluids before going to sleep, avoiding caffeinated or alcoholic beverages in the evening, pre\u2010emptive voiding immediately before going to bed, strategic medication timing, leg elevation in the case of lower extremity oedema, moderate physical exercise, and keeping warm in bed. Lifestyle modifications have been associated with up to 50% of people reporting improvement in nocturia ( Soda 2010 ). Medical management includes therapy with a single drug or a combination of drugs from different classes. Nocturia has historically been considered a symptom secondary to underlying bladder or prostate dysfunction, therefore therapies have been targeted towards these conditions. Current therapies include antimuscarinics, alpha\u20101 adrenergic receptor blockers, 5\u2010alpha\u2010reductase inhibitors, phosphodiesterase type 5 inhibitors, anti\u2010inflammatory drugs, and desmopressin. The use of alpha\u20101 adrenergic receptor blockers and 5\u2010alpha\u2010reductase inhibitors are in the context of BPH management, with only a few studies focusing on nocturia as a primary outcome. Overall, the evidence supporting the efficacy of alpha\u20101 adrenergic receptor blockers and 5\u2010alpha\u2010reductase inhibitors in treating nocturia is low with inconsistent evidence that at best accounts for minor improvements ( Schneider 2009 ). In one large study analysing 3047 men randomised to one of four groups (doxazosin, finasteride, doxazosin plus finasteride, and placebo), the doxazosin and doxazosin plus finasteride groups showed statistically significant mean reductions in the number of nocturia episodes by 0.77 for doxazosin and 0.80 for doxazosin plus finasteride at one year ( Johnson 2007 ). The placebo group had a decrease in the mean number of nocturia episodes by 0.61 and the finasteride group had a decrease of 0.60 at one year. These results were modest, and whether they are clinically significant is highly questionable. Tadalafil is a phosphodiesterase type 5 inhibitor that also has approval for treatment of LUTS/BPH. Integrated data from four randomised, placebo\u2010controlled studies investigating the use of daily tadalafil for nocturia found a mean decrease in the number of episodes of 0.4 in the placebo group and 0.5 in the tadalafil group ( Oelke 2014b ). While the difference between groups was statistically significant, the difference was very small and unlikely to be indicative of a clinically meaningful improvement. Most studies on antimuscarinics such as solifenacin and tolterodine are in the context of overactive bladder management, and evidence that they are effective for the specific management of nocturia is limited. The clinical impact of these agents is also questionable, as they result in a reduction of half a void or less per night ( Smith 2011 ). The population that may benefit most from these agents are people with severe overactive bladder who have frequent nighttime awakenings associated with urgency. Few studies have been done on anti\u2010inflammatory drugs for nocturia. One study evaluating celecoxib in men with BPH with refractory nocturia showed promising results ( Falahatkar 2008 ). Men in the active treatment group had a statistically significant decrease in nocturnal frequency from (mean \u00b1 standard deviation (SD)) 5.17 \u00b1 2.1 episodes to 2.5 \u00b1 1.9 episodes, a dramatic response that is better than any other treatment modality. However, this study included only 80 men and was for a period of only four weeks. Without more reports with longer follow\u2010up and larger study populations, the evidence to support the use of anti\u2010inflammatory drugs for nocturia in men remains unclear. Surgical treatment may be appropriate for men with nocturia due to LUTS related to benign prostatic obstruction, although no surgical therapy is specifically indicated for nocturia ( Cornu 2012 ). Surgical options to reduce prostatic obstruction include transurethral resection of the prostate, transurethral incision of the prostate, transurethral microwave therapy of the prostate, and prostatectomy. Transurethral resection of the prostate has been associated with a one\u2010point reduction in nocturia, which correlates to one less nightly episode ( Wada 2014 ; Yoshimura 2003 ). Prostatectomy has been associated with a decrease in nighttime voiding frequency of 0.8 (mean \u00b1 SD): from 3.4 \u00b1 1.2 episodes to 2.6 \u00b1 0.99 episodes ( Margel 2007 ). Other interventions such as botulinum toxin detrusor injection, sacral neuromodulation, or tibial electric nerve stimulation are used in the context of overactive bladder where nocturia is regarded a secondary outcome ( Cornu 2012 ). Description of the intervention Desmopressin is the synthetic analogue of the human hormone vasopressin, which has been used clinically in a variety of formulations since 1974. Vasopressin, also known as antidiuretic hormone, is produced by the posterior pituitary gland, and its role is to maintain serum osmolality and volume via modulation of free water excretion. Vasopressin is released in states of hyperosmolality and hypovolaemia, which is detected by chemoreceptors and baroreceptors located in the hypothalamus and carotid sinus, respectively. It acts on the V2 receptors in the distal collecting tubules, which subsequently results in translocation of aquaporin channels associated with cytosolic vesicles to the apical membrane of collecting duct cells. Free water is then passively reabsorbed from the nephron back into the systemic circulation via basolateral membrane channels. Vasopressin also plays a minor role in increasing systemic vascular resistance and increasing urea reabsorption in the medullary collecting tubule ( Shoskes 2011 ). It is the most frequently tested medication for the specific treatment of nocturia, but has traditionally been used to treat central diabetes insipidus, bleeding disorders such as Von Willebrand disease, and primary nocturnal enuresis. The US Food and Drug Administration (FDA) approved desmopressin acetate nasal spray with the trademark name of NOCTIVA in March 2017 for nocturia due to nocturnal polyuria in adults who awaken at least twice per night to void ( Serenity Pharmaceuticals 2017 ). The most frequently encountered adverse events include headache, hyponatraemia, insomnia, dry mouth, hypertension, abdominal pain, peripheral oedema, and nausea ( Friedman 2013 ). How the intervention might work Desmopressin is effective in the treatment of polyuric states, and nocturnal polyuria is very common in people with nocturia (as high as 82.9%) ( Chang 2006 ). Desmopressin may help alleviate nocturnal voiding by inducing antidiuresis resulting in increased urine osmolality and decreased urine output. Why it is important to do this review Nocturia may decrease quality of life and is associated with a high degree of bother. Reduced sleep due to nocturia has been associated with diurnal fatigue, decreased concentration, lower performance at work, and accidents from cognitive and motor impairment ( Asplund 2005 ; Chartier\u2010Kastler 2006 ). In the elderly population, nocturia is associated with an increased risk of bone fractures due to nighttime falls ( Nakagawa 2010 ). Although various treatment modalities have been used to treat LUTS successfully, nocturia remains one of the most elusive and problematic symptoms. There are also inherent risks in recommending a medication to treat a symptom if the underlying disease process is not thoughtfully considered. Desmopressin can also cause hyponatraemia, which is often asymptomatic but in limited cases can have deleterious effects. With regard to assessing efficacy, a common outcome measure is the decrease in the number of nocturnal voids, however there is no existing consensus on its clinical significance. To better assess clinically meaningful efficacy it is therefore crucial to critically analyse and synthesise the symptoms, quality of life measures, and sleep parameters in existing clinical trials ( Cornu 2012 ). One systematic review on this topic exists ( Ebell 2014 ). However, no review has used GRADE to assess the quality of evidence supporting the use of desmopressin for the treatment of nocturia in men. Given the modest efficacy and questionable clinical significance of the multiple medical and surgical treatments for nocturia in men, such an investigation into the effects of desmopressin based on the totality of available evidence as summarised in a rigorous systematic review is critically important. In an era of rising health costs and an increased emphasis on evidence\u2010based medicine, the findings of this review will be especially relevant to policymakers and healthcare providers. Furthermore, as a drug can gain regulatory approval based on statistical significance, it is important to know that this does not automatically mean that the drug has a clinically significant impact. This is of particular concern when the effect size is small or when the potential adverse events are considerable, or both ( Fralick 2017 ).",
        "summary": "Desmopressin appears effective in reducing the number of nocturnal voids and improving sleep patterns in men with nocturia and seems to compare favorably with other medicines used for urinary problems. However, much of the evidence is of low quality, making it difficult to draw firm conclusions. When men with nocturia were treated with desmopressin, they reported fewer nocturnal voids after 3 to 12 months (on average, 0.85 fewer nocturnal voids) and a longer first sleep episode (on average, by 55 minutes at 1 to 3 months and by 18 minutes at 3 to 12 months) than with placebo, with no clear increase in the incidence of adverse events. Although the quality of evidence for the first sleep episode at 1 to 3 months was considered moderate, the quality of evidence for all other outcomes in the comparison with placebo was very low to low. Desmopressin also seemed to perform well when combined with an alpha\u2010blocker and compared with an alpha\u2010blocker alone, with fewer nocturnal voids (on average, 0.47 fewer nocturnal voids), small improvement in quality of life (on average, by 0.3 points and considered possibly unimportant by the reviewers) (both moderate\u2010quality evidence), and no clear differences in adverse events (low\u2010quality evidence). When desmopressin plus an alpha\u2010blocker was compared with an anticholinergic plus an alpha\u2010blocker, evidence shows no clear difference in the number of nocturnal voids reported, but desmopressin plus an alpha\u2010blocker seemed to have a better adverse event profile (10 vs 45 people with minor adverse events per 1000 people; low\u2010quality evidence), and fewer people withdrew from desmopressin plus alpha\u2010blocker treatment owing to adverse events when compared with people given an alpha\u2010blocker plus anticholinergics (on average, 10 vs 45 per 1000 people; low\u2010quality evidence)."
    },
    "CD000028": {
        "query": "How does pharmacotherapy affect outcomes in people aged 60 years or older with hypertension?",
        "document": "Background Description of the condition Blood pressure increases with age, and the rate of rise is greater over the age of 60. As a result, the number of people with elevated blood pressure (known as 'hypertension') increases with age. Systolic blood pressure is more strongly associated with cardiovascular disease than is diastolic blood pressure, particularly in older people. Isolated systolic hypertension occurs more commonly in older people. Older people also accumulate higher rates of other risk factors for cardiovascular disease such as obesity, left ventricular hypertrophy, sedentary lifestyle, hyperlipidaemia, and diabetes. Hypertension is a major risk factor for cardiovascular disease in older adults. Hypertension is present in 69% of patients with a first myocardial infarction; in 77% of those with a first stroke; in 74% of those with congestive heart failure; and in 60% of those with peripheral arterial disease ( Aronow 2015 ). Uncontrolled high blood pressure can lead to heart attack, stroke, aneurysm (life\u2010threatening if ruptured), heart failure, kidney damage, and vision loss (due to thickened, damaged, or torn blood vessels in the eye). Description of the intervention Changing lifestyle \u2010 eating a healthy diet with less salt, exercising regularly, quitting smoking, limiting alcohol intake, and maintaining a healthy weight \u2010 can help to control high blood pressure. When these lifestyle changes are not enough, treatment with antihypertensive drugs is recommended. Practitioners use several classes of antihypertensive drugs such as diuretics, angiotensin\u2010converting enzyme (ACE) inhibitors, angiotensin\u2010receptor blockers (ARBs), beta blockers, and calcium channel blockers to lower blood pressure. They also use other medications to treat high blood pressure, including alpha blockers, alpha\u2010beta blockers, centrally acting drugs, vasodilators, and aldosterone antagonists. How the intervention might work Following are the mechanisms of action of the most commonly used antihypertensive drug classes. Thiazide and thiazide\u2010like diuretics lower blood pressure over the long term through a mechanism of action that is not fully understood ( Zhu 2005 ). After long\u2010term use, thiazides lower peripheral resistance. The mechanism of these effects is uncertain, as it may involve effects on 'whole body', renal autoregulation, or direct vasodilator actions ( Hughes 2004 ). Thiazides act on the kidney to inhibit reabsorption of sodium (Na + ) and chloride (Cl \u2010 ) ions from the distal convoluted tubules in the kidneys by blocking the thiazide\u2010sensitive Na + \u2010Cl \u2010 symporter ( Duarte 2010 ). Beta blockers are competitive antagonists that block the receptor sites for epinephrine (adrenaline) and norepinephrine on adrenergic beta receptors. Some block activation of all types of beta\u2010adrenergic receptors (\u03b2 1 , \u03b2 2 , and \u03b2 3 ), and others are selective for one of the three types of beta receptors ( Frishman 2005 ). ACE inhibitors block the conversion of angiotensin I (AI) to angiotensin II (AII) and thus decrease the actions of angiotensin II. The end result consists of lowered arteriolar resistance and increased venous capacity; decreased cardiac output, cardiac index, stroke work, and volume; lowered resistance in blood vessels of the kidneys; and increased excretion of sodium in the urine. Renin and AI are increased in concentration in the blood as a result of negative feedback on conversion of AI to AII. Levels of AII and aldosterone are decreased. Bradykinin is increased because ACE is responsible for inactivation of bradykinin. Angiotensin\u2010receptor blockers (ARBs) block the activation of angiotensin II AT 1 receptors. Blockage of AT 1 receptors directly causes vasodilation, reduces secretion of vasopressin, and reduces production and secretion of aldosterone. Calcium channel blockers block the calcium channel and inhibit calcium ion influx into vascular smooth muscle and myocardial cells. They reduce blood pressure through various mechanisms including vasodilation, reduction in the force of contraction of the heart, slowing of the heartbeat, and direct reduction of aldosterone production. Alpha 1 \u2010adrenergic receptor blockers inhibit the binding of norepinephrine (noradrenaline) to \u03b1 1 receptors on vascular smooth muscle cells. The primary effect of this inhibition is vasodilation, which decreases peripheral vascular resistance, leading to decreased blood pressure. Central sympatholytic drugs reduce blood pressure mainly by stimulating central \u03b1 2 \u2010adrenergic receptors in the brainstem centres, thereby reducing sympathetic nerve activity and neuronal release of norepinephrine to the heart and the peripheral circulation. Vasodilators act directly on the smooth muscle of arteries to relax their walls so blood can move more easily through them. Thiazide and thiazide\u2010like diuretics lower blood pressure over the long term through a mechanism of action that is not fully understood ( Zhu 2005 ). After long\u2010term use, thiazides lower peripheral resistance. The mechanism of these effects is uncertain, as it may involve effects on 'whole body', renal autoregulation, or direct vasodilator actions ( Hughes 2004 ). Thiazides act on the kidney to inhibit reabsorption of sodium (Na + ) and chloride (Cl \u2010 ) ions from the distal convoluted tubules in the kidneys by blocking the thiazide\u2010sensitive Na + \u2010Cl \u2010 symporter ( Duarte 2010 ). Beta blockers are competitive antagonists that block the receptor sites for epinephrine (adrenaline) and norepinephrine on adrenergic beta receptors. Some block activation of all types of beta\u2010adrenergic receptors (\u03b2 1 , \u03b2 2 , and \u03b2 3 ), and others are selective for one of the three types of beta receptors ( Frishman 2005 ). ACE inhibitors block the conversion of angiotensin I (AI) to angiotensin II (AII) and thus decrease the actions of angiotensin II. The end result consists of lowered arteriolar resistance and increased venous capacity; decreased cardiac output, cardiac index, stroke work, and volume; lowered resistance in blood vessels of the kidneys; and increased excretion of sodium in the urine. Renin and AI are increased in concentration in the blood as a result of negative feedback on conversion of AI to AII. Levels of AII and aldosterone are decreased. Bradykinin is increased because ACE is responsible for inactivation of bradykinin. Angiotensin\u2010receptor blockers (ARBs) block the activation of angiotensin II AT 1 receptors. Blockage of AT 1 receptors directly causes vasodilation, reduces secretion of vasopressin, and reduces production and secretion of aldosterone. Calcium channel blockers block the calcium channel and inhibit calcium ion influx into vascular smooth muscle and myocardial cells. They reduce blood pressure through various mechanisms including vasodilation, reduction in the force of contraction of the heart, slowing of the heartbeat, and direct reduction of aldosterone production. Alpha 1 \u2010adrenergic receptor blockers inhibit the binding of norepinephrine (noradrenaline) to \u03b1 1 receptors on vascular smooth muscle cells. The primary effect of this inhibition is vasodilation, which decreases peripheral vascular resistance, leading to decreased blood pressure. Central sympatholytic drugs reduce blood pressure mainly by stimulating central \u03b1 2 \u2010adrenergic receptors in the brainstem centres, thereby reducing sympathetic nerve activity and neuronal release of norepinephrine to the heart and the peripheral circulation. Vasodilators act directly on the smooth muscle of arteries to relax their walls so blood can move more easily through them. Why it is important to do this review Most of the early trials evaluating antihypertensive drug therapy were conducted in lower\u2010risk people younger than 60 years. The first definitive clinical trial evidence supporting blood pressure\u2010lowering treatment was produced in the mid\u20101980s. Before that time, policy makers and clinicians were reluctant to recommend treatment, particularly for the elderly; some regarded systolic hypertension as a natural feature of aging, and others feared excessive harm from blood pressure lowering in this age group. When all drug therapies are included in one review, the underlying assumption is that the benefits of lowering blood pressure are independent of the mechanism by which this is achieved. This assumption has not been proven, and it is likely that different drugs lowering blood pressure by different mechanisms will have effects that are independent of the blood pressure\u2010lowering effect. A drug that lowers blood pressure could have pharmacological and physiological actions independent of blood pressure lowering, and these other actions (both known and unknown) could enhance or negate effects on health outcomes associated with the decrease in blood pressure. This possibility is supported by an analysis suggesting that blood pressure lowering explains only about 50% of the treatment effect in antihypertensive trials ( Boissel 2005 ). It is important to know and compare the benefits and harms of antihypertensive drug therapy in different age groups of patients with hypertension \u2010 18 to 59 years old; and 60 years or older. Our aim is to document the best available evidence for adult patients 60 years or older. A meta\u2010analysis in patients 80 years or older from earlier trials by Gueyffier 1999 showed a trend towards increased mortality. Therefore we planned subgroup analyses of patients 60 to 79 years old, and 80 year or older. This is the second substantive update of this review. It was originally published as Mulrow 1998 , and the first update was published as Musini 2009 . A Cochrane Review titled \"Pharmacotherapy for hypertension in adults age 18 to 59 years old\" was published recently ( Musini 2017 ). A Cochrane Review titled \"First line drugs for hypertension\" has recently been updated ( Wright 2018 ).",
        "summary": "In people aged 60 years or older with moderate to severe systolic and/or diastolic hypertension (average 182/95 mmHg), antihypertensive agents reduce mortality and cardiovascular, cerebrovascular morbidity, assuming people can tolerate them. High\u2010certainty evidence shows that antihypertensive drug therapy reduced all\u2010cause mortality compared with placebo or no treatment/observation (63 vs 102 per 1000 people died at nearly four years of follow\u2010up; all results on average). Moderate\u2010certainty evidence shows lower rates of cardiovascular mortality and morbidity (100 vs 138 per 1000 people), cerebrovascular mortality and morbidity (37 vs 56 per 1000 people), and coronary heart disease mortality and morbidity (38 vs 49 per 1000 people) with antihypertensive drug therapy. These risk reductions held for people aged 60 to 79 years, but results for people age 80 years or older were less consistent; researchers observed the same age\u2010related pattern for people with isolated systolic hypertension. However, benefits must be balanced against an increase in patient withdrawal from studies because of medication intolerance (168 vs 7 per 1000 people)."
    },
    "CD008814": {
        "query": "How do different chemotherapy regimens compare in people with untreated Hodgkin lymphoma?",
        "document": "Background Description of the condition Hodgkin's lymphoma (HL) is a malignancy of the lymph nodes and lymphatic system with possible involvement of other organs ( Mauch 1999 ; De Vita 2000 ). The disease is rare, with an annual incidence of approximately 3 per 100,000 in most countries, although in certain low\u2010income countries the incidence in children is higher and Epstein Barr virus (EBV) association and mixed cellularity subtype are more frequent ( Mueller 1999 ). Most sufferers are young people, the incidence being greatest in the third decade of life ( Mueller 1999 ). The malignant cells stem from lymphocytes, but the causes of the malignancy are poorly understood ( De Vita 2000 ). Untreated, HL is fatal within a few years in most cases, but today the large majority of patients are cured. Description of the intervention Treatment strategies are determined by the disease stage and other prognostic factors. Early\u2010stage patients without adverse factors usually receive a combination of mild chemotherapy (two to three cycles) and limited radiotherapy ( GHSG HD10 ; EORTC H8\u2010U ) . Early\u2010stage patients with adverse prognostic factors are usually treated with moderate chemotherapy (four to six cycles) combined with radiotherapy ( von Treskow 2012 ; EORTC H8\u2010U ). Advanced\u2010stage patients receive intensive chemotherapy, typically six to eight cycles, with or without additional radiation ( Engert 2012 ; MF\u2010GITIL\u2010IIL ; Canellos 2009 ). The optimal treatment strategy is still controversial. Relevant criteria include (a) efficacy in controlling HL; (b) options and prognosis for second\u2010line therapy for those for whom first\u2010line treatment fails; and (c) acute toxicity, late effects and quality of life. As cure rates of HL patients have dramatically improved over recent decades, so that today the great majority of even advanced\u2010stage patients reach a lasting complete remission, treatment toxicity and subsequent quality of life have increased in importance. Strategies to reduce late toxicity and improve long\u2010term quality of life include avoidance of irradiation, reduction of irradiation fields or dose, reduction of number of chemotherapy cycles and avoidance of certain drugs such as alkylating agents or bleomycin. Recently, positron emission tomography (PET) has been employed during treatment to more reliably assess tumour status and thus identify patients requiring less or further treatment, thus reducing the treatment burden for a subgroup of patients. The carcinogenic effects of ionising radiation were demonstrated in the 1930s, and since then its potential for causing almost any kind of cancer has been demonstrated ( Boice 1988 ). Risks appear to be higher for young people. At low doses the risk increases linearly with dose. At therapeutic doses a further increase in risk with dose was seen in certain sites but not in others. In contrast, the carcinogenicity of chemotherapy was only discovered in the 1960s with the development of effective combination regimens. Further, radiologic imaging (especially computed tomography and PET) for staging and restaging purposes may also contribute slightly to the risk of secondary malignant neoplasm ( Beyan 2007 ). Due to the high rate of cure of HL patients and their predominantly young age at diagnosis, they have ample 'opportunity' to develop treatment\u2010related secondary malignancies. How the intervention might work Secondary malignant neoplasms are perhaps the most serious late effect of treatment ( Henry\u2010Amar 1996 ). Secondary malignant neoplasms can be divided into three classes. These are acute myeloid leukaemia and myelodysplastic syndrome (AML/MDS), non\u2010Hodgkin lymphomas (NHL) and solid tumours. Secondary AML/MDS occur typically three to eight years after chemotherapy treatment, reaching a cumulative risk of one to three per cent in most studies. Secondary NHL occurs at a constant rate of about 0.2% per year independent of treatment type. Secondary solid tumours usually occur later, typically five to 20 years after treatment, with no evidence of a decline in incidence even after 20 years. Cumulative incidences of up to 34% have been estimated, representing a relative risk of up to five compared with the general population. Solid tumours appear to occur after both radiotherapy and chemotherapy. The impact of secondary malignant neoplasms on the long\u2010term survival of HL patients is considerable as long\u2010term survival rates especially for patients who develop therapy\u2010related leukaemia are poor. ( Henry\u2010Amar 1992 ). Why it is important to do this review The effect of treatment modality on secondary malignant neoplasm rates has been investigated in several analyses of large data sets, including many analyses that were pooled over several patient cohorts. Case\u2010control studies have also been performed, particularly for investigations of specific types of secondary malignant neoplasm (SMN). Characteristics and key results of all identified studies which analysed at least 50 secondary malignant neoplasms or at least 20 secondary malignant neoplasms of a particular type (that is, AML, NHL or a certain solid tumour site) are summarised in Table 1 (all SMN), Table 2 (secondary solid tumours and NHL) and Table 3 (AML/MDS). Other authors, such as Aleman 2003 , investigated long\u2010term cause\u2010specific mortality after HL, including the relationship between secondary malignant neoplasms and treatment modality. The relationship between treatment and secondary malignant neoplasm risk has been reviewed by Henry\u2010Amar 1993 , Tucker 1993 and Ng 2004 . Whilst it is widely accepted that AML is largely chemotherapy\u2010induced and NHL is largely independent of treatment modality, it is unclear which, if any, treatment modality can help to avoid solid tumours. The conclusions of the various investigators who compared solid tumour risks after various treatment modalities are far from unanimous. The situation is complicated by the large number of anatomic sites at which a solid tumour can occur as well as by the much higher risk of solid tumour in the general population compared with the very low risk of AML/MDS and NHL. Further, this risk varies widely according to age, sex and other personal and environmental factors. The quality and quantity of relevant data on solid tumour incidence is limited because of their very late occurrence. Publication Characteristics Number of incident cases Treatment groups Analysis methods Conclusions (all types) Conclusions solid tumours Conclusions (AML) Conclusions (NHL) Bhatia 1996 15 centres (USA, Manchester, Amsterdam); 1955\u20101986; MFU = 11.4 yrs.; N = 1 380 (children < 16 yrs.) 88 SMN (+9 excluded non\u2010melanoma skin cancers); 24 AML (+2 other leukaemias), 47 ST, 9 NHL RT, CT, RT+CT (total treatment) Cox regression separately for ST, AML, NHL All ST: no differences; breast cancer only: RT dose (RR 5.9 for dose > 20 Gy) No differences reported Higher risk with more alkylating agents Biti 1994 1 centre (Florence); 1960\u20101988; N = 1 121 73 SMN (+5 excluded basocellular skin cancers); 60 ST, 11 AML (MDS excluded), 2 NHL (A) RT, CT, RT+CT, CRT; total treatment. (B) RT, CT, CRT (primary treatment only), censored at relapse Cox regression Higher risk after primary CT compared with IF/M alone; higher risk with CT+(S)TNI compared with IF/M alone Same trend as for all SM Higher risk with primary CT (\u00b1RT); higher risk with more cycles of CT Boivin 1995 Embedded case\u2010control study; 14 Canadian and US centres; 1940\u20101987; MFU = 7 yrs.; N = 10 472 (9 280 followed for at least one year) 560 SMN; 403 ST, 122 AML, 35 NHL RT, CT as time\u2010dependent variables, primary and salvage RT, CT Cox regression with splenectomy, RT, CT as time\u2010dependent covariates Significantly more with CT than without CT (ST and NHL analysed together) Significantly more with CT than without CT (more with MOPP than with ABVD) Dietrich 1994 1 centre (France); 1960\u20101983; N = 892 (continuously disease\u2010free HD only) N = 56 (first FU \u2010year excluded); 37 ST (excluding bcc), 11 ANLL/MDS, 8 NHL RT versus CRT; Mantle\u2010RT versus EF\u2010RT; SM before progression/relapse only Cox regression; All RR compared with IF (= MF/inverted Y\u2010RT) Significant excess only with MOPP+EF (RR 10.86, P < 0.001) and MOPP+IF (RR 4.99, P = 0.015). Same tendency as for SM, but significant only for MOPP+EF Increased risk only for MOPP+EF (RR 16.55, P = .004) No difference in treatment Dores 2002 16 US and European cancer registries; 1935\u20101995; N = 32 591 2 153 SMN; 1 726 ST, 169 ANLL, 162 NHL RT versus CT versus CRT (primary treatment) No direct comparison between treatment groups: all results as RR according to primary treatment compared with normal population. Significantly higher RR with CRT (95% CI 2.6\u20103.6) compared with either RT alone (2.1\u20102.4) or CT alone (1.5\u20101.9). Digestic tract and female breast: Significantly higher risks with RT than without RT. Foss\u2010Abrahamsen 1993 1 centre (Oslo); 1968\u20101985; MFU = 8 yrs.; N = 1 152 68 SMN (+6 excluded non\u2010melanoma skin cancers); 9 AML, 8 NHL, 51 ST RT, CT, CT+RT (total treatment) Cox regression Greater risk of SM for pts. who received both CT and RT Koshy 2012 SEER registry database; 1988.2006; N = 12 247 ca. 650 SMN (5.3%) RT versus no RT (primary treatment) Kaplan\u2010Meier; no explicit comparison no increase due to RT Mauch 1996 1 centre (JCRT Boston, USA); 1969\u20101988; N = 794 72 SMN; 53 ST, 8 AML, 10 NHL RT(no relapse), RT\u2010relapse\u2010CT, CRT; total treatment RRs compared with normal population, no direct treatment comparisons RT alone RR 4.1, RT+CT RR 9.75, P < 0.05 Same effect as with all SMN Same effect as with all SMN Ng 2002 4 centres (all affil. to Harvard); 1969\u20101997; MFU = 12 yrs.; N = 1319 (mainly early stages); (996 pts. with fu > 10 years were included in analysis of treatment effect) 181 SMN (N = 162 for pts. with fu > 10 yrs.); 131 ST, 23 AML, 24 NHL RT, CRT (total treatment); also separate analyses of non\u2010relapsed cases and relapsed cases RRs calculated relative to normal population (age/sex\u2010specific); CI from Poisson distribution RR higher with CRT than RT alone (6.1 versus 4.0, P = 0.015); (non\u2010relapsed cases only: 5.9 versus 3.7, P = 0.016). Analysed by radiation field size, this effect was only significant for TNI (\u00b1CT). RR higher with CT+TNI than for CT+Mantle/EF Rodriguez 1993 1 centre (M.D. Anderson, Houston, USA); 1966\u20101987; N = 1 013 66 SMN (first FU\u2010year excluded); 38 ST, 14 AML/MDS, 14 NHL IF versus EF (+MOPP); CT versus CRT; RT versus CRT. Total therapy Cox regression RT versus CRT: no difference (P = 0.37). CT versus CRT: less SM with CRT (P = 0.001). But less courses of CT with CRT than with CT only! Scholz 2011 Multi\u2010centre (mainly Germany); 1978\u20101998); N = 5 357 67 AML, 97 NHL Primary: RT, conventional CT for intermediate stage, conventional CT for advanced stage, escalated BEACOPP Parametric model; separate effects of primary and salvage treatment Higher risk with escalated BEACOPP than conventional CT No differences Swerdlow 1992 > 60 BNLI centres, UK; 1970\u20101987; N = 2 846 113 SMN; 80 ST, 16 AML, 17 NHL Alkyl. CT, Alkyl. CT +RT, IF\u2010RT (+/\u2010 nonalk. CT), EF\u2010RT (+/\u2010 nonalk. CT) (total treatment) Poisson regression No difference overall (nor for lung ca. alone) More with CT or CRT (similar) than with RT No differences Swerdlow 2000 BNLI, Royal Marsden, St. Bartholomews; 1963\u20101993; N = 5 519 322 SMN; 228 ST, 44 AML, 50 NHL CT, RT, CRT (total treatment) Poisson regression. RR compared with normal population, no direct treatment comparisons Higher RR for CRT (SIR 3.9, 95% CI 3.2 \u2010 4.6) than for CT (SIR 2.6, 95% CI 2.1 \u2010 3.2) or RT (SIR 2.3, 95% CI 1.9 \u2010 2.8). Higher risk for CRT (SIR 38.1, 95% CI 24.6\u201055.9) or CT (SIR 31.6, 95% CI 19.7\u201047.6) than for RT (SIR 1.2, 95% CI 0.1\u20105.2) No significant differences Swerdlow 2011 UK, 1963\u20102001 459 SMN; 302 ST, 75 AML, 82 NHL CT, CRT Higher risk for CRT than for CT alone Tucker 1988 Stanford UMC; 1968 \u2010 ?(year needed); N = 1 507 83 SMN (first FU\u2010year excluded); 46 ST, 28 AML, 9 NHL RT, RT+adj. CT, RT+salvage CT, RT+intravenous\u2010gold, CT (total treatment) Kaplan\u2010Meier, Gehan test No differences (except: more with radiotherapy + intravenous\u2010gold) Higher risk with CT than RT No differences van Leeuwen 1994a 2 centres (the Netherlands); 1966\u20101986; MFU = 9 yrs.; N = 1 939 146 SMN; 93 ST, 31 AML, 23 NHL CT, RT, CRT (total treatment) (A) Person\u2010years analysis. (B) Cox regression B: for lung cancer only: trend to more for RT (P = 0.08) or CRT (P = 0.07) than for CT. Otherwise no differences A: AML not increased for RT; large increase for CT (CT similar to CRT). B: AML more for CT (P = 0.009) or CRT (P = 0.04) than for RT B: trend to more for CRT than for either CT or RT (P = 0.06) AML = acute myeloid leukaemia; ANLL = acute nonlymphocytic leukemia; CT = chemotherapy; CRT = chemotherapy plus radiotherapy combined; NHL = non\u2010Hodgkin lymphoma; FU = follow\u2010up; HD = Hodgkins disease; MFU = median follow\u2010up Publication Characteristics Number of solid tumours / NHL Treatment groups Analysis methods Conclusions (solid tumours) Conclusions (NHL) Behringer 2004 Multi\u2010centre (mainly Germany); 1983\u201098; N = 5 367 127 CT, RT, CT+EF, CT+IF/local RR compared with general population. No direct treatment comparisons. Birdwell 1997 Stanford UMC (USA); 1961\u20101994; MFU = 10.9 yrs.; N = 2 441 25 gastrointestinal cancers RT, CRT (total treatment) RR compared with general population. No direct treatment comparisons. Risk of gastrointestinal cancer not significantly greater with CRT (RR 3.9, 95% CI 2.2 to 5.6) than with RT (RR 2.0, CI 1.0 to 3.4) De Bruin 2009 5 centres (the Netherlands); 1965\u20101995; N = 1 122 120 breast cancers RT field and CT regimen in women under 41 years with supradiaphragmatic irradiation (N = 782) Cox regression Significantly greater risk of breast cancer with mantle RT than mediastinal RT Enrici 1998 Rome, Italy; 1972\u20101996; MFU = 84 months; N = 391 20 NHL (A) RT, CT, CRT (initial treatment) censored at relapse. (B) RT, CT, CRT (total treatment) Kaplan\u2010Meier and Cox regression No difference between treatment modalities Foss\u2010Abrahamsen 2002 1 centre (Oslo); 1968\u20101985; MFU = 14 yrs.; N = 1 024 26 lung, 23 breast, 31 NHL RT, CT, CRT (total treatment) RR compared with general population. No direct treatment comparison Tendency to greater lung and breast cancer risk with RT or CRT versus CT No difference between treatment modalities Hancock 1993 Stanford UMC (USA); 1961\u20101990; MFU = 10 yrs.; N = 885 25 breast cancers RT, CRT (total treatment) RR compared with general population. No direct treatment comparisons RT versus CRT: Tendency of more breast cancers with CRT, but not significant. RT: RR 3.5 (95% CI 1.9\u20105.8), CRT: RR 5.7 (95% CI 3.1\u20109.5) Hodgson 2007 13 cancer registries; 1970\u20102001, 5\u2010year survivors; N = 18 862 1 490 ST RT, CT, CRT (primary treatment, RT supra\u2010 or infradiaphragmatic according to SMN site) RR by Poisson regression significantly greater risk of breast cancer and other supradiaphragmatic cancer with RT or CRT versus CT Kaldor 1992 Case\u2010control study; 12 cancer registries (Europe, Canada), 6 large hospitals (Europe); from 1960 onwards; N = 25 665 98 lung cancers RT, CT, CRT Standard case\u2010control study methods. RR compared with RT Higher risk with CT, risk increase with number of CT cycles and RT dose to the lung. Meattini 2010 One centre (Florence, Italy); 1060\u20102003; N = 1 538 39 breast cancers RT, CT, CRT (primary treatment); RT field; CT regimen Cox regression No significant differences (breast) Swerdlow 2001 Nested case\u2010control study; multi\u2010centre (Britain); 1963\u20101995; N = 5 519 88 lung cancers RT, CT, CRT (total treatment) Conditional logistic regression No significant differences in lung cancer risk between RT, CT, CRT. (exception: adenocarcinomas \u2010 greater risk with CT than without.) Risk greater with MOPP than without MOPP Swerdlow 2012 UK, 1956 \u2010 2003 373 breast cancers RT, CRT Breast cancer standardised incidence ratio (SIR) is highest among patients receiving RT at a young age Travis 2002 Embedded case\u2010control study; 7 cancer registries; 1965\u20101994; N = 19 046 222 lung cancers RT, alkylating CT, RT with alk. CT, RT + salvage alk. CT, neither (total treatment) Conditional logistic regression Lung cancer risk increases with RT dose to the lung and with use of alkylating agents Travis 2003 Embedded case\u2010control study; 6 cancer registries; 1965\u20101994; N = 3 817 women 105 breast cancers RT, alkylating CT, RT with alk. CT, RT + salvage alk. CT, neither (total treatment) Conditional logistic regression Breast cancer risk increases with RT dose to breast and decreases with use of alkylating CT and with radiation of ovaries van Leeuwen 1995 Embedded case\u2010control study; 2 centres (the Netherlands); 1966\u20101986; N = 1 939 30 lung cancers RT, CT, CRT. RT dose to lung (total treatment) Conditional logistic regression Risk of lung cancer tended to increase with increasing RT dose (P = 0.01); RR(> 9 Gy versus 0) = 9.6. No significant differences between RT, CT, CRT van Leeuwen 2003 Embedded case\u2010control study; 4 centres (the Netherlands); 1965\u201088; N = 2 637 48 breast cancers RT, CRT. RT dose to breast, ovary. CT cycles, dose of alkylating agents Conditional logistic regression Breast cancer risk increases with RT dose and decreases with modality CRT; no CT dose effect CT = chemotherapy; CRT = chemotherapy plus radiotherapy combined; NHL = non\u2010Hodgkin lymphoma; FU = follow\u2010up; HD = Hodgkins disease; MFU = median follow\u2010up Publication Characteristics Number of AML/MDS Treatment groups Analysis methods Conclusions (AML/MDS) Brusamolino 1998 2 centres (Italy); 1975\u20101992; MFU = 10 yrs.; N = 1 659 36 AML/MDS RT, CT, CT+RT. Total treatment A.Log\u2010rank tests (univariate) to compare treatment groups B.Embedded case\u2010control study with conditional logistic regression analysis. A. Higher risk after CT than RT (P = 0.04); higher risk with CT than with CRT (P = 0.05); higher risk with MOPP+RT than with MOPP/ABVD or with ABVD+RT (P = 0.002); higher risk with EF + MOPP than with IF+MOPP (P = 0.01) B. higher risk after CT than RT (OR 4.1; P = 0.05); higher risk after CRT than RT (OR 6.4; P = 0.02); higher risk after MOPP+RT than ABVD+RT (OR 5.9; P = 0.001) or MOPP/ABVD Eichenauer 2014 GHSG HD7\u2010HD15, PROFE, BEACOPP\u201014 (1993\u20102009); MFU: 72 months, N = 11 952 106 AML/MDS RT, CT, CRT Significantly higher risk after 4 or more cycles of escalated BEACOPP Josting 2003 Multi\u2010centre (GHSG (Germany) HD1\u2010HD9); 1981\u20101998; MFU = 55 months; N = 5 411 46 AML/MDS CT, RT, CRT, HDCT with SCT. Primary treatment, not censored at relapse Kaplan\u2010Meier. No direct treatment comparison No significant differences between treatment protocols Kaldor 1990 Case\u2010control study; 12 cancer registries (Europe, Canada), 6 large hospitals (Europe); 1960\u2010?(year needed); N = 29 552 149 AML/MDS (at least one year after HD diagnosis) RT, CT, CRT. Total treatment Standard case\u2010control study methods. RR compared with RT Higher risk with CT than with RT (RR 9.0; CI 4.1\u201020); higher risk with CRT than with RT (RR 7.7; CI 3.9\u201015). No difference in CT versus CRT; but there was a dose\u2010related increase in the risk in pts. who received RT alone Koontz 2013 Stanford (1974\u20102003); N = 754 24 AML/MDS RT, CT, CRT Increased risk with higher doses of alkylating agents Pedersen\u2010Bjergaard 1987 1 centre (Copenhagen); 1970\u20101981; N = 391 20 ANLL/preleukaemia Low, intermediate, or high dose of alkylating agents. Total treatment Cox regression Risk increases with increasing (total) log dose of alkylating agents (P = 0.0024, regr. coefft. = 0.69) van Leeuwen 1994b Embedded case\u2010control study; 2 centres (Netherlands); 1966\u20101986; N = 1 939 44 Leukemias (incl. 32 ANLL, 12 MDS) RT, CT, RT+CT. Total treatment Conditional logistic regression More risk with CT than with RT alone; <= 6 cycles: P = 0.08, RR = 8.5; > 6 cycles: P < 0.001, RR = 44 AML = acute myeloid leukaemia; ANLL = acute nonlymphocytic leukemia; CT = chemotherapy; CRT = chemotherapy plus radiotherapy combined; NHL = non\u2010Hodgkin lymphoma; FU = follow\u2010up; HD = Hodgkins disease; MFU = median follow\u2010up The above\u2010mentioned studies make non\u2010randomised comparisons of secondary malignant neoplasm rates between treatments since even if randomised trial data were included, the data from several trials and from non\u2010randomised cases were pooled. Therefore, the benefits of randomisation do not necessarily apply to the treatment comparisons made in these studies. The patients receiving different treatment modalities may be non\u2010comparable with respect to several known or unknown factors, which may be related to secondary malignant neoplasm risk. In short, these comparisons may be 'confounded'. One literature\u2010based and two individual\u2010patient\u2010data meta\u2010analyses comparing the effectiveness of different treatment modalities in HL have already been published by others ( Shore 1990 ; Loeffler 1998 ; Specht 1998 ). The results demonstrate that the use of combined modality therapy improves disease\u2010free survival, compared with chemotherapy alone (advanced stages) or radiotherapy alone (early stages), but the 10\u2010year overall survival (OS) rates were not significantly improved. Additional radiation was in fact associated with a slightly lower OS compared with the use of further chemotherapy in advanced disease. Similarly, more extensive radiotherapy improved disease\u2010free survival but not OS when compared with limited irradiation. Loeffler 1998 analysed leukaemia\u2010related deaths but the survival estimates were limited to 10 years after HL diagnosis, too early for the effect of solid tumours to be felt. Specht 1998 analysed secondary malignant neoplasm\u2010related deaths for patients without recurrence of HL. The only meta\u2010analysis of the first occurrence of secondary malignant neoplasms after HL was performed by our group from 2000 to 2004 ( Franklin 2005 ; Franklin 2006a ) as a Cochrane systematic review using individual patient data (IPD) from randomised trials comparing chemotherapy alone, radiotherapy alone and combined chemo\u2010radiotherapy, and comparing extended\u2010field with involved\u2010field radiotherapy. This review largely confirmed the results of previous meta\u2010analyses concerning overall and progression\u2010free survival (PFS). Concerning secondary malignant neoplasms, the use of a combined modality, particularly in early\u2010stage disease, was shown to reduce secondary malignant neoplasm risk compared with radiotherapy alone. This is possibly due to the greatly improved HL tumour control and consequent reduced need for intensive salvage treatment. In contrast, eliminating additional radiotherapy, particularly in advanced\u2010stage patients, slightly reduced the risk of secondary malignant neoplasms. No significant reduction in secondary malignant neoplasm risk from the use of involved field (instead of extended field) could be demonstrated, although the risk of secondary breast cancer was significantly reduced. The reliability and current relevance of the results were limited due to the considerable number of trials for which no data were obtained, the inclusion of older trials with outdated treatments and the uncertain quality of secondary malignant neoplasm data. In order to clarify the relationship between treatment modality and secondary malignant neoplasm risk, long\u2010term follow\u2010up data from large numbers of patients are required since only a small percentage will incur a secondary malignant neoplasm within a given time interval. Further, specific sites of solid tumours may have to be analysed separately, resulting in even smaller incidences. Due to the many confounding factors (age, sex, smoking habits, etc.) conclusions should be based on randomised comparisons between treatment modalities, in contrast to pooled data analyses. All these factors argue for a systematic overview of the risk of secondary malignant neoplasms. The present review aims to tailor our previous review to questions of current relevance, modern treatments and high\u2010quality trials. A review of this type compares treatment 'policies', that is the choice of first\u2010line treatment modality, rather than the influence of radiation and drugs on a biologic level. Not only these two influences, but also accompanying diagnostic procedures, supporting medication, second\u2010line treatment if necessary and treatment for other late effects, may contribute to the overall secondary malignant neoplasm risk associated with a treatment policy. Conclusions concerning the biologic influence of radiation and drugs per se must remain tentative. Due to the influence of personal factors on secondary malignant neoplasm risk, the many types of secondary malignant neoplasms and the time\u2010to\u2010event nature of the data, a meta\u2010analysis based on IPD makes the best possible use of the information available. We collected data on all three classes of secondary malignant neoplasms as well as on efficacy (overall and PFS). We compared the effect of treatment modality on the risk of secondary malignant neoplasms as a whole as well as AML, NHL and solid tumours, and overall and disease\u2010free survival. The comparison of effectiveness both updates and extends the previous IPD meta\u2010analyses mentioned above. Furthermore, information on effectiveness is needed to put the results concerning secondary malignant neoplasm risk into context, since all outcomes must be considered together in choosing the optimal treatment modality.",
        "summary": "Dose\u2010intensified chemotherapy may improve survival outcomes for people with advanced Hodgkin lymphoma compared with standard dose ABVD\u2010like chemotherapy but may increase secondary cancer rates. However, reviewers did not assess quality of life or chemotherapy\u2010related adverse effects; therefore, the impact of different regimens on these outcomes remains unclear. In people with early\u2010stage (favorable or unfavorable risk groups) Hodgkin lymphoma, evidence of moderate to high quality from randomized controlled trials (RCTs) shows that the number of cycles of chemotherapy received (two to four cycles vs four to six cycles) had no clear impact on overall survival, disease progression\u2010free survival, or development of secondary malignant cancers over a median follow\u2010up of approximately eight years. Subgroup analyses by age (\u2264 or > 50 years) and gender also showed no clear differences between the two regimens. In people with advanced\u2010stage Hodgkin lymphoma, RCTs provided moderate\u2010quality evidence showing that dose\u2010intensified chemotherapy yielded better disease progression\u2010free survival (PFS) rates versus ABVD\u2010like chemotherapy (typically consisting of doxorubicin, bleomycin, vinblastine, and dacarbazine drugs) across a median follow\u2010up of approximately seven years. The eight\u2010year PFS rate was 75% with dose\u2010intensified chemotherapy versus 69% with ABVD\u2010like chemotherapy, indicating that in absolute numbers, an extra 50 people per 1000 would survive without disease progression if they were given dose\u2010intensified chemotherapy. Overall survival (OS) tended to be better for people receiving dose\u2010intensified chemotherapy, although this difference was not statistically significant; the eight\u2010year OS rate was 85% with dose\u2010intensified chemotherapy versus 82% with ABVD\u2010like chemotherapy, indicating that in absolute numbers, an extra 20 per 1000 people would survive with dose\u2010intensified chemotherapy (moderate\u2010quality evidence). In the subgroup of trials that compared escalated BEACOPP with ABVD\u2010like chemotherapy, OS and PFS were more favorable with BEACOPP; PFS appeared less favorable with Stanford chemotherapy than with ABVD\u2010like chemotherapy. Low\u2010quality evidence from RCTs suggests no clear differences in rates of secondary malignant neoplasms between dose\u2010intensified chemotherapy and ABVD\u2010like chemotherapy. However, when assessed separately, acute myeloid leukemia/myelodysplastic syndrome occurred more often after dose\u2010intensified chemotherapy than ABVD\u2010like chemotherapy. Subgroup analyses by age (\u2264 or > 50 years) and gender show that the largest subgroups of people aged \u2264 50 years and women showed an increased rate of secondary cancers with intensified chemotherapy; the response of men was less clear, and the subgroup of people aged > 50 years was too small to permit conclusions. Review authors did not assess quality of life or adverse effects for any comparison."
    },
    "CD010406": {
        "query": "For adults with influenza admitted to hospital, what are the effects of corticosteroids given to relieve symptoms associated with severe influenza?",
        "document": "Background Description of the condition Influenza is a significant cause of morbidity and mortality worldwide and has a high financial burden. Seasonal influenza occurs annually during the winter months in temperate zones of both the Northern and Southern hemispheres and year round in the tropics ( Caini 2016 ). Global estimates of seasonal influenza from the World Health Organization (WHO) report 1000 million cases, including three to five million cases of severe illness annually ( WHO 2018 ). Between 291,000 and 645,800 respiratory deaths associated with influenza are estimated to occur globally each influenza season; 58% of these are in individuals aged 65 years and above ( Iuliano 2018 ). The reported per capita total cost of a case of influenza illness in national studies ranges from USD 27 to USD 52 in European countries and USD 45 to USD 63 in the United States ( Peasah 2013 ). Estimates of the influenza\u2010related hospitalisation rate in the USA range from 63 to 107 per 100,000 individuals annually at a cost of USD 11,096 to USD 83,216 per admission; amongst adults, hospitalisation rates are highest in individuals aged 65 years age and above (309/100,000) ( Peasah 2013 ; Zhou 2012 ). The population\u2010based incidence estimate for influenza\u2010associated critical illness in the USA is 12 per 100,000 person\u2010years; this represents 1.3% of all critical illness hospitalisations, or 3.4% of critical illness hospitalisations during the influenza season ( Ortiz 2014 ). Estimates from the UK indicate an influenza\u2010attributable annual general practitioner consultation rate of 2156 per 100,000 population and a corresponding annual hospitalisation rate of 34 per 100,000 population ( Cromer 2014 ). Pandemic influenza occurs unpredictably and infrequently due to reassortment of the influenza virus or adaptive mutation of a virus that has crossed the species barrier ( Taubenberger 2008 ). Although the case fatality ratio associated with the recent influenza A (H1N1) pandemic in 2009 and 2010 was lower in comparison to previous pandemics (0.03% versus 2.5% in 1918 and 1919) ( Donaldson 2010 ), a modelling study of global mortality due to the recent pandemic estimated 201,200 respiratory deaths and 83,300 cardiovascular deaths, with 80% of the deaths in individuals younger than 65 years ( Dawood 2012 ). This shift in mortality towards younger age groups is estimated to have led to between 334,000 and 1,973,000 'years of life lost' in the USA alone ( Viboud 2010 ). Worldwide clinical data from the influenza A (H1N1) pandemic in 2009 revealed that more than one\u2010fifth of hospitalised individuals experienced severe disease requiring admission to an intensive care unit (ICU) ( Jain 2009 ; Muthuri 2013 ; Richard 2012 ). The onset of critical illness following hospital admission occurred rapidly (median one day) and was commonly due to acute respiratory distress syndrome with refractory hypoxaemia, septic shock, and/or multisystem organ failure, often requiring prolonged ventilation ( Jain 2009 ; Kumar 2009 ). Critical care delivery systems were overwhelmed, especially in low\u2010 and middle\u2010income countries, affecting entire hospital services downstream ( Ortiz 2013 ). The mortality associated with critical care admission due to severe influenza was high (14% to 22%) ( Jain 2009 ; Richard 2012 ). Current antiviral treatment options for influenza are limited to the neuraminidase inhibitors (NI) and adamantanes, although widespread adamantane use has been hampered by the global emergence of drug resistance ( Deyde 2007 ). A Cochrane Review of randomised placebo\u2010controlled trials (RCTs) reported a reduced time to first alleviation of symptoms by 0.6 to 0.7 days in NI\u2010treated adults, but no differences were seen between the two groups with regard to hospitalisation rates or occurrence of influenza\u2010related adverse events ( Jefferson 2014 ). In contrast, an individual patient level meta\u2010analysis of over 29,000 patients with 2009 influenza A H1N1 virus (H1N1pdm09) infection from 78 observational studies across the world found that NI treatment at any time, in comparison to no treatment, was associated with a 19% reduction in mortality risk; early treatment (within two days of symptom onset) was associated with a 52% reduction in mortality risk in comparison to late treatment ( Muthuri 2013 ). Description of the intervention Endogenous corticosteroids are produced principally in the adrenal glands from cholesterol and are regulated by the hypothalamic\u2010pituitary\u2010adrenal (HPA) axis ( Molenaar 2012 ); they possess several anti\u2010inflammatory, immunomodulatory, and vascular properties including inhibition of pro\u2010inflammatory cytokines, reduction of leucocyte trafficking, stimulation of apoptosis of T\u2010lymphocytes, maintaining endothelial integrity and vascular permeability and regulation of vascular tone by inhibition of vasodilators (nitrous oxide) and increasing sensitivity to vasopressors ( Cain 2017 ; Coutinho 2011 ; Kaufmann 2008 ). These properties form the rationale for testing corticosteroids in sepsis and related conditions. A systematic review of RCTs investigating sepsis and septic shock reported that in critically ill individuals with sepsis, corticosteroid use probably results in increased 7\u2010day shock reversal and small reductions in ICU and hospital length of stay, and may achieve a small reduction or no reduction in short\u2010term (28\u2010 to 31\u2010day) mortality and possibly a small reduction in long\u2010term mortality ( Rochwerg 2018 ). For the treatment of bacterial meningitis, corticosteroids appear to reduce hearing loss and neurological complications ( Brouwer 2015 ), while in tuberculous meningitis, an improvement in survival was reported ( Prasad 2016 ). With regard to respiratory infections, a recent Cochrane Review of systemic corticosteroid use in community\u2010acquired pneumonia found a reduction in mortality in adults with severe pneumonia, but not in those with non\u2010severe pneumonia ( Stern 2017 ). Time to clinical cure and length of ICU and hospital stay were also decreased in those treated with corticosteroids, as well as a reduction in the number of people developing respiratory failure or shock and complications of pneumonia. There is limited evidence that systemic corticosteroids as adjunctive therapy to antibiotics in people with acute sinusitis may offer modest benefits for short\u2010term symptom relief ( Venekamp 2014 ). A review found that in children with croup, corticosteroid treatment was associated with improved symptoms at two hours and decreased readmission rates and length of stay ( Gates 2018 ). No benefits were seen in hospital admission rates or length of stay in hospital following systemic or inhaled corticosteroid use in infants and young children with acute viral bronchiolitis ( Fernandes 2013 ). The role of corticosteroids for the treatment of influenza is highly controversial. While some case series have reported improved outcomes with corticosteroid treatment of severe influenza ( Quispe\u2010Laime 2010 ), other cohort studies have suggested the opposite ( Diaz 2012 ; Liem 2009 ). Despite the ongoing controversy, 9% of hospitalised individuals and up to 69% of critically ill individuals during the 2009 influenza A (H1N1) pandemic were prescribed corticosteroid therapy ( Brun\u2010Buisson 2011 ; Diaz 2012 ; Kumar 2009 ; Muthuri 2013 ). The WHO consultation on human influenza A (H5N1) infection reported that 47% to 70% of patients received corticosteroids during the 2004 to 2005 outbreak in Southeast Asia ( WHO 2005 ). How the intervention might work Viral replication and production of cytokines through activation of the host innate immune system are central to the pathogenesis of influenza infection ( de Jong 2006 ). Elevated or excessive production of cytokines (hypercytokinaemia) correlates with symptoms and fever in acute influenza ( Lee 2011 ; McClain 2016 ). Comparisons between patients with mild and severe pandemic influenza have revealed significantly higher levels of cytokines (especially interleukin\u20106) in the plasma of patients with severe disease ( Yu 2011b ), and similar findings have been replicated in studies of severe seasonal influenza ( Heltzer 2009 ). A combination of excessive pro\u2010inflammatory cytokine induced inhibition of the HPA axis, substrate (cholesterol) deficiency, structural damage to the adrenal gland due to infarction of haemorrhage and peripheral corticosteroid resistance could lead to absolute or relative corticosteroid insufficiency during critical illness ( Annane 2017 ; Marik 2009 ). The overall incidence of adrenal insufficiency in people with critical illness is estimated to be around 20%, and up to 60% in those with sepsis and septic shock ( Marik 2009 ). Administration of corticosteroids during critical illness, including severe influenza, may attenuate this state of adrenal insufficiency and help maintain homeostasis, and control dysregulation of the immune system. Why it is important to do this review Treatment options for influenza are limited. Corticosteroids may offer an additional therapeutic option; although they are frequently prescribed for severely ill individuals with influenza, there is controversy regarding their benefits and harms. A systematic review of the current evidence would a) highlight the quality of the available evidence and b) valuably inform current clinical practice and future research needs.",
        "summary": "Based on retrospective cohort studies, there is insufficient evidence to assess the effects of corticosteroids given to relieve symptoms associated with severe influenza to adults admitted to hospital with influenza. There is some suggestion that the use of corticosteroids in adults with severe influenza may in fact increase mortality, the need for mechanical ventilation, and hospital\u2010acquired infection. However, evidence was obtained from non\u2010randomized studies of very low certainty that were subject to substantial heterogeneity."
    },
    "CD010181": {
        "query": "Is there an association between platinum\u2010based chemotherapy and hearing loss in children with cancer?",
        "document": "Background Platinum\u2010based therapy, including cisplatin, carboplatin, oxaliplatin or a combination, is used to treat a variety of paediatric malignancies. One of the most important adverse effects is the occurrence of hearing loss (ototoxicity). It usually manifests as bilateral, symmetrical, sensorineural hearing loss first affecting the higher frequencies (6000 Hz or greater) ( McHaney 1983 ) and it is often accompanied by tinnitus ( Reddel 1982 ). The hearing loss not only develops during platinum\u2010based therapy but also years after completion of the therapy ( Bertolini 2004 ; Knight 2005 ). This might be explained by the prolonged retention of platinum in the body; up to 20 years after treatment circulating platinum is still detectable in the plasma ( Gietema 2000 ). Platinum\u2010induced hearing loss seems to be irreversible and worsening of hearing loss occurs during follow\u2010up ( Bertolini 2004 ; McHaney 1983 ). There is a wide variation in the reported frequency of platinum\u2010induced hearing loss; frequencies as high as 88% have been described ( McHaney 1983 ). Several risk factors have been mentioned in the literature, such as the type of platinum analogue used. Cisplatin seems to cause substantially more hearing loss than carboplatin and the highest incidence of hearing loss has been found in people who received both cisplatin and carboplatin ( Bertolini 2004 ; Dean 2008 ); the ototoxicity of oxaliplatin as compared to the other platinum analogues is not as well established but oxaliplatin seems to be the least ototoxic ( Eloxatin SPC ). Furthermore, the incidence of platinum\u2010induced hearing loss seems to be dose\u2010dependent, increasing with higher cumulative doses ( Bertolini 2004 ; Li 2004 ; McHaney 1983 ; Schell 1989 ), and with higher individual doses ( Li 2004 ; Reddel 1982 ). Different dosing formulas, like dose per body surface area or per kilogram bodyweight, can influence the platinum doses actually received, especially in infants ( Leahey 2012 ; Qaddoumi 2012 ). In addition, bolus injections seem to be more ototoxic than longer infusion durations ( Reddel 1982 ), although this was not confirmed in a Cochrane systematic review ( Van As 2014a ). Cranial radiotherapy ( Schell 1989 ), younger age ( Li 2004 ; Qaddoumi 2012 ; Schell 1989 ), genetic variants ( Grewal 2010 ; Ross 2009 ) and other host\u2010specific factors ( Veal 2001 ), impaired renal function at the time of platinum treatment ( Skinner 2004 ) and other ototoxic drugs, such as aminoglycosides ( Cancer in Children 2005 ; Skinner 2004 ), and furosemide ( Gallagher 1979 ), have been reported as additional risk factors. Although platinum\u2010induced hearing loss is not life\u2010threatening, loss of hearing, especially during the first three years of life and even when only borderline to mild, can have important implications. It can negatively impact speech and language development, which may lead to difficulties with school performance and psychosocial functioning ( Dean 2008 ; Gregg 2004 ; Skinner 2004 ). This is even more true for children who experience dual sensory loss, like people with retinoblastoma or optic pathway glioma. One systematic review and its update have shown that at the moment there is no evidence that underscores the use of medical interventions, such as amifostine, to prevent the occurrence of platinum\u2010induced ototoxicity ( Van As 2012a ; Van As 2014b ). More insight into the prevalence of and risk factors for platinum\u2010induced hearing loss is essential in order to develop less ototoxic treatment protocols for the future treatment of children with cancer and to develop adequate follow\u2010up protocols for childhood cancer survivors treated with platinum\u2010based therapy. This is, to our knowledge, the first systematic review on this important topic.",
        "summary": "In children with cancer (most commonly retinoblastoma or neuroblastoma) undergoing platinum\u2010based chemotherapy, prevalence of hearing loss ranged from 2% to 90% across studies. Among studies, seven different assessment tools were used to measure this outcome, resulting in 10 different definitions. Other factors that may have contributed to this wide range in prevalence include different doses of platinum\u2010based chemotherapy; variation in concomitant therapy such as aminoglycoside antibiotics and radiotherapy fields involving the internal ear; changes in chemotherapy regimens across time (studies covered the period between 1962 and 2012); and the range of cancers included in the studies. No study reported the prevalence of tinnitus. Given these factors and the methodological limitations of the included studies, the impact of platinum\u2010based chemotherapy on hearing among children with cancer remains uncertain."
    },
    "CD012730": {
        "query": "How do antibiotics compare with placebo for induction and maintenance of remission in adolescents and adults with Crohn's disease?",
        "document": "Background Description of the condition Crohn's disease (CD) is an inflammatory disorder of the gastrointestinal tract that most commonly affects the ileum and the colon. Characteristic histologic features of the disease include transmural inflammation and mucosal ulceration. The exact etiology of CD is unclear, however both genetic and environmental factors are important contributors ( Elson 2005 ; Scribano 2013 ). In this regard, the human microbiome is considered to be a key environmental risk factor. In animal models, interactions between the mucosal immune system and commensal bacteria contribute to the observed pathological changes seen in CD ( Elson 1995 ; Elson 2005 ; Rath 1999 ). Subsequent human studies have demonstrated that patients with CD have higher concentrations of intestinal and colonic bacteria ( Scribano 2013 ), and higher populations of specific bacteria ( Gevers 2014 ), compared to healthy controls. Patients with CD may also have impaired barrier function that facilitates translocation of microbes into the mucosa ( Marks 2006 ). Pathogenic bacterial strains, including Escherichia coli ( Mylonaki 2005 ), have been isolated in the mucosal and mesenteric lymph nodes of these patients ( Ambrose 1984 ). Furthermore, there is a change in the microbial composition with fewer species overall and a relative overrepresentation of Enterobacteriaceae , Proteobacteria , Actinobacteria ( Sartor 2008 ), and Bacteroides ( Barnich 2007 ). These observations support the notion that the pathological response in CD is driven by an abnormal response to the host microbiome and that manipulation of the flora though antibiotic treatment might be a potential therapy ( Sartor 2008 ). Description of the intervention Given the proposed link between increased intestinal bacterial concentrations and chronic inflammation, antibiotics have been considered for the treatment of CD ( Swidsinski 2002 ). Studies have suggested Escherichia coli as specific bacterial targets, among others ( Mylonaki 2005 ; Sartor 2008 ). How the intervention might work Several antibiotics have been evaluated for the treatment of CD. Reduction of the bacterial load in the intestinal mucosa might reduce the pathological immune response in the intestinal mucosa ( Scribano 2013 ; Swidsinski 2002 ). Furthermore, antibiotics also act to limit bacterial translocation and reduce the concentration of adherent bacteria to the lumen and mucosa ( Scribano 2013 ). In patients who have high levels of Escherichia in their microbiome, treatment with mesalamine showed a decrease in intestinal inflammation. This further suggests the crucial role the gut microbiome may have in IBD pathophysiology and the potential use for antimicrobial agents ( Kostic 2014 ). Cumulatively, these data have raised the possibility that alteration of the mucosal flora may have a therapeutic role in CD by inhibiting the stimulus for pathogenic immune responses ( Ott 2004 ; Swidsinski 2002 ). Why it is important to do this review Given the extensive animal and human data that support the role of bacteria in the pathogenesis of CD, it is reasonable to postulate that antibiotic therapy might be effective for either induction or maintenance of remission in CD. However, several potential problems exist with this approach. First, use of broad\u2010spectrum antibiotics is a very blunt strategy that may aggravate the aforementioned dysbiosis. Second, the resident flora are determined by both genetic and dietary factors that may be difficult or impossible to modify on a chronic basis. Therefore, treatment, if effective, might have to be continued indefinitely. Finally broad\u2010spectrum antibiotic therapy is associated with important adverse effects, notably an increased risk of Clostridium difficile infection. For these reasons evidence from high quality randomized controlled trials (RCTs) is necessary before antibiotics are accepted as effective and safe for the treatment of CD. No current recommendations exist regarding the antibiotic of choice, dose, or duration for treatment of CD. The most recent guidelines published by the World Gastroenterology Organisation support the use of antibiotics in perianal disease, fistulizing disease, and bacterial overgrowth secondary to stricturing disease, despite limited supporting evidence ( Bernstein 2016 ). There is evidence regarding antibiotic use in post\u2010operative CD management ( Bernstein 2016 ).",
        "summary": "Antibiotics seem to confer no major benefit over placebo for adults with Crohn's disease, and any effects of antibiotics on the number of people entering clinical remission are modest. There seems to be no impact on maintenance of remission nor on serious adverse events; however, clinically important effects cannot be ruled out entirely because of uncertainty of the evidence. High\u2010certainty evidence shows that although more people entered clinical remission with antibiotics compared with no antibiotics, the difference may not be clinically meaningful (on average, 500 vs 578 per 1000 people failed to enter clinical remission). Reviewers found no evidence of a difference between groups in failure to maintain clinical remission at one year after the start of treatment (on average, 491 vs 564 per 1000 people), incidence of serious adverse events between six weeks and one year (on average, 16 vs 9 per 1000) (both low\u2010certainty evidence), or overall adverse events between six weeks and one year (on average, 385 vs 441 per 1000; high\u2010certainty evidence). Reviewers did not report health\u2010related quality of life."
    },
    "CD008093-1": {
        "query": "How does amiodarone compare with other antiarrhythmic drugs for the secondary prevention of sudden cardiac death in at\u2010risk adults?",
        "document": "Background Description of the condition From a clinical point of view, any unexpected death can be considered a 'sudden death', brought on by conditions as diverse as arrhythmias, aortic dissection, subarachnoid haemorrhage, acute myocardial infarction or massive pulmonary embolism. Traumatic death is usually excluded from this category. As there may be prognostic and therapeutic differences (i.e. subarachnoid haemorrhage or pulmonary embolism), researchers and clinicians recognise a distinct category known as 'sudden cardiac death' (SCD). A widely accepted definition is \"natural death due to cardiac causes, heralded by abrupt loss of consciousness within an hour of the onset of acute symptoms; pre\u2010existing heart disease may have been known to be present, but the time and mode of death are unexpected\" ( Myerburg 2004 ). SCD is one of the leading causes of cardiac death. Incidence increases with age and is three to four times more frequent in men than women at all ages ( Merghani 2013 ; MMWR 2002 ). Accurately estimating its real incidence is difficult, but according to data obtained from death certificates, SCD may cause 63.4% of total cardiac mortality in the United States ( MMWR 2002 ). This data probably overestimates SCD prevalence, as it is based only on clinical presentation ( MMWR 2002 ; Zheng 2001 ). Incidence rates, varying from 0.36 to 1.28/100,000 participants per year, have been reported by some emergency services, but these tend to underestimate the real incidence, as it only refers to participants who survive to the hospital ( Sara 2014 ). Incidence increases from 1/100,000 for those aged < 35 years to 100/100,000 in individuals aged \u2265 35 years old ( John 2012 ). A prospective observational study reported that 7% to 18% of overall mortality in the general population (of all ages) in the USA was due to SCD ( Stecker 2014 ). Globally, estimated incidence of SCD would then be approximately 4 to 5 million cases every year. However, this number may be inaccurate, as, on the one hand, SCD incidence rates in low\u2010 and middle\u2010income countries may not be equivalent to those in high\u2010income countries ( Vedanthan 2012 ), and on the other hand, incidence has declined over the past two decades, from 4.7 per 1000 person\u2010years in 1990\u20132000 to 2.1 per 1000 person\u2010years in 2001\u20132010 ( Niemeijer 2015 ). The main pre\u2010existing heart disease leading to SCD in high\u2010income countries is coronary heart disease (CHD); there is a general acceptance that SCD accounts for around 50% of all CHD\u2010related death and that the proportion of all SCDs resulting from CHD is around 80% ( Myerburg 2012 ). Other types of cardiopathy (e.g. hypertrophic cardiomyopathy, non\u2010ischaemic cardiomyopathy, arrhythmogenic right ventricular dysplasia) can also lead to SCD, while there is no structural abnormality in only 5% of cases ( Consensus 1997 ). The SCD event is most commonly caused by the sudden onset of monomorphic ventricular tachycardia (VT) that degenerates into ventricular fibrillation (VF), and less frequently by the abrupt onset of polymorphic VT/VF, bradyarrhythmias or heart blocks ( Priori 2001 ; Zipes 2006 ). However, the proportion of participants with pulseless electrical activity or asystole has increased over the past two decades ( Teodorescu 2010 ). More recently, research has identified diabetes mellitus as an independent risk factor for SCD ( Jouven 2005 ). The downward trend in SCD incidence might be due to better diagnosis and treatment of heart disease and most importantly primary prevention of SCD and cardiovascular disease in general through improved management of behaviours and other risk factors ( Niemeijer 2015 ). In this context, clinicians use electrophysiological (EP) testing with intracardiac recording and electrical stimulation at baseline, followed by administration of antiarrhythmic drugs for arrhythmia assessment and risk stratification. EP testing has been used to document the inducibility of VT, evaluate drug effects, assess the risks of recurrent VT or SCD, and assess the indications for implantable cardiac defibrillator (ICD) therapy. For example, in participants with CHD, asymptomatic non\u2010sustained VT and a left ventricular ejection fraction (LVEF) less than 40%, the inducibility of sustained VT ranges from 20% to 40% and confers worse prognosis, with an increased risk of SCD or death from other causes ( Buxton 2000 ). However, in participants with CHD and a lower LVEF (less than 30%), non\u2010inducibility doesn't necessarily portend a good prognosis ( Buxton 2002 ), and persistent inducibility while receiving antiarrhythmic drugs confers an even worse prognosis ( Wilber 1990 ). Description of the intervention Amiodarone, one of the main class III antiarrhythmics, is a benzofuran derivative approved by the US Food and Drug Administration (FDA) for the treatment of patients with life\u2010threatening ventricular tachyarrhythmias when other drugs are ineffective or not tolerated ( FDA 2013 ). Researchers have proposed the drug as an alternative to ICD, categorising it as having a 2a level of evidence (weight of evidence is in favour of usefulness/efficacy) for prophylaxis of SCD in participants with CHD and left ventricular (LV) dysfunction ( Zipes 2006 ). The onset of action after intravenous administration is generally within one to two hours, but after oral administration, the onset of action may require anywhere from two to three days and often from one to three weeks. On occasion, it may take even longer, to the point that achieving a steady state without a loading dose takes about 265 days ( Braunwald 2001 ). However, research has also associated the use of amiodarone with toxicity involving the lungs, thyroid gland, liver, eyes, skin and nerves ( Connolly 1997 ). Pulmonary toxicity is the drug's most serious potential adverse effect, and some series have described its frequency as high as 17%, although the incidence when compared with placebo is less than 1% ( Pollak 1999 ). Thyroid toxicity is the most common complication requiring intervention, occurring in up to 10% of participants receiving long\u2010term amiodarone therapy. Minor adverse effects are nausea, anorexia, photosensitivity, and a blue discolouration of the skin ( Siddoway 2003 ). The frequency of most adverse effects is related to total amiodarone exposure ( Siddoway 2003 ), but amiodarone is slowly, variably and incompletely absorbed, which makes adverse events unpredictable. Extensive hepatic metabolism occurs with desethylamiodarone as a major metabolite, both extensively accumulating in the liver, lung, fat, 'blue' skin, and other tissues. How the intervention might work Generally speaking, class III antiarrhythmic drugs act by prolonging the action potential\u00b4s duration of the myocardial cell by lengthening the repolarisation phase and thus the effective refractory period. This prolongation is believed to facilitate termination and prevention of both ventricular re\u2010entry arrhythmias by producing block within re\u2010entry circuits, and thus providing both an elevation of the ventricular fibrillation threshold and a reduction of the ventricular defibrillation threshold ( Brendorp 2002 ). As a class III antiarrhythmic drug, amiodarone prolongs the QT interval, slows the heart rate and atrioventricular nodal conduction (via calcium channel and beta\u2010receptor blockade), prolongs refractoriness (via potassium and sodium channel blockade), and slows intracardiac conduction (via sodium channel blockade) ( Siddoway 2003 ). By blocking the potassium repolarisation currents, it can inhibit or terminate ventricular arrhythmias by increasing the wavelength for reentry ( Zipes 2006 ). Why it is important to do this review According to current evidence, ICD therapy, when compared with antiarrhythmic drugs, reduces mortality in high risk participants with reduced LVEF in both CHD and non\u2010ischaemic cardiopathy, for both primary and secondary prevention ( AVID 1997 ; Connolly 2000 ; Desai 2004 ; Kuck 2000 ). While ICD therapy may improve survival in selected patient populations, it may diminish patients' quality of life ( Gehi 2006 ). In a study comparing ICD versus no ICD in participants who underwent coronary artery bypass graft surgery, the use of ICD was associated with lower levels of psychological well\u2010being and reduced physical and emotional role functioning ( Namerow 1999 ). On the other hand, a recent analysis by Marks et al. from the Sudden Cardiac Death \u2010 Heart Failure Trial (SCD\u2010HeFT) showed that subjective measures of physical function did not differ significantly between the ICD and placebo groups at any time point, but there was a short\u2010term increase in psychological well\u2010being among participants with ICD therapy throughout the first year after implantation, a benefit that did not persist at 30 months ( Bardy 2005\u2013SCD\u2010HeFT ). The occurrence of ICD shocks reduced the quality of life, but only if quality of life was measured within one to two months after the shock ( Bardy 2005\u2013SCD\u2010HeFT ). However, the elevated up\u2010front costs of ICD therapy (between EUR 11,000 and 19,000) impede its ready availability in the health systems of low\u2010 and middle\u2010income countries, even though costs tend to diminish along patients' longevity ( Biffi 2011 ). Regarding amiodarone, evidence from trials has been inconsistent, with some studies showing a moderate effect and others no effect at all ( Bardy 2005\u2013SCD\u2010HeFT ; Heidenreich 2002 ; Strickberger 2003 ). Indirect evidence of effect comes from a recent systematic review concluding that ICD discharges were reduced in participants with ICD plus amiodarone compared to participants with ICD alone. Assuming ICD discharges follow ventricular arrhythmias, one could infer that amiodarone reduces the number of arrhythmic episodes ( Ferreira 2007 ), provided the ICDs were programmed with similar arrhythmia detection times, as different detection times could mean different thresholds for considering any disturbance to be an arrhythmic episode ( Scott 2014 ). On the other hand, data collected since the 1980s has convincingly proven that beta\u2010blocking treatment is associated with an improved clinical outcome in several patient groups. The efficacy of this treatment in people with post\u2010myocardial infarction (MI) relates to a drug\u2010associated reduction in all\u2010cause mortality and is not necessarily related to the time after the acute event, when therapy starts ( Yusuf 1985 ). People with a history of congestive heart failure (CHF) or depressed left ventricular function tend to experience the greatest benefits in mortality reduction. Data suggests that in participants post\u2010MI, potasium channel blockers such as dofetilide or d\u2010sotalol have neutral or even harmful effects regarding all\u2010cause mortality ( K\u00f8ber 2000 ; Torp\u2010Pedersen 1999 ; Waldo 1996 ), and, for example, both of these drugs result in a higher rate of Torsade de Pointes than amiodarone ( Brendorp 2002 ). However, calcium\u2010channel blockers such as verapamil have shown favourable effects, if only in people without heart failure ( DAVIT II 1990 ). It is also important to note that chronic treatment with antiarrhythmic drugs is associated with severe adverse effects, including the potential induction of life\u2010threatening arrhythmias (e.g. increased mortality is associated with the long\u2010term use of quinidine; Coplen 1990 ). A previous systematic review of randomised controlled trials, which evaluated amiodarone versus other antiarrhythmics or placebo for the prevention of SCD in participants with or without ICD, concluded that it significantly reduced SCD and cardiac mortality, but not all\u2010cause mortality. However, the authors did not carry out a separate analysis for participants with or without ICD, and the review only evaluated amiodarone for primary prevention ( Piccini 2009 ). If amiodarone proves to be beneficial in SCD prevention, it would constitute a valid alternative in situations where economic constraints limit the widespread use of ICD.",
        "summary": "It is difficult to draw firm conclusions as to the comparative efficacy of amiodarone and other antiarrhythmics for secondary prevention of sudden cardiac death as only low\u2010quality evidence was found. When amiodarone was compared with an alternative antiarrhythmic drug (sotalol, celivarone, metoprolol or drug not specified) for the secondary prevention of sudden cardiac death, no differences were detected between groups in sudden cardiac death (very low\u2010quality evidence), overall cardiac death (2 RCTs, 273 participants), or all\u2010cause mortality (low\u2010quality evidence). When a small subgroup of 377 people with implantable cardiac defibrillators (ICDs) were analyzed separately, more people experienced a sudden cardiac death with amiodarone when compared with celivarone (75 versus 3 deaths per 1000 people), but due to the small numbers of participants, the uncertainty surrounding this estimate was extremely large. In terms of adverse events, hyperthyroidism was more commonly associated with amiodarone, but not hypothyroidism, pulmonary adverse events or withdrawals due to adverse events. Event rates were extremely low for adverse events; therefore the analyses were likely to have been too small to reliably detect differences between groups."
    },
    "CD010671": {
        "query": "How does yoga compare with exercise for adults with chronic non\u2010specific low back pain?",
        "document": "Background Description of the condition Low back pain, defined as pain or discomfort in the area between the lower rib and the gluteal folds, is a common, potentially disabling condition ( Koes 2006 ). One systematic review on the prevalence of low back pain reported adjusted summary estimates for point prevalence of 11.9% (standard deviation (SD) 2.0%) and one\u2010month prevalence of 23.2% (SD 2.9%) ( Hoy 2012 ). The US National Health Interview Survey (NHIS) estimated the three\u2010month prevalence of low back pain as 26.4% ( Deyo 2006 ). The unadjusted summary estimate from the systematic review for one\u2010year prevalence was 38.0% (SD 19.4%) ( Hoy 2012 ). Estimates of lifetime prevalence vary greatly in different regions of the world, and mean estimates range from 38.9% ( Hoy 2012 ) to 85% ( Balague 2012 ). Low back pain is associated with loss of work productivity, poor quality of life, and high medical expenses, and is a substantial economic burden on society ( Deyo 2006 ; Dagenais 2008 ). Back pain is sometimes associated with a likely aetiology (e.g. radiculopathy or spinal stenosis), but most low back pain cases are of unknown origin and are classified as non\u2010specific ( van Tulder 1997 ). Low back pain may also be classified according to the duration of pain as acute (less than four weeks), subacute (between four weeks and three months), or chronic (three months or greater) ( van Tulder 2006 ; Chou 2007 ). Most episodes of low\u2010back pain are mild ( Cassidy 2005 ), and activity limitations are rare ( Lawrence 2008 ). Symptoms often improve during the first six weeks ( Buchbinder 2012 ; Costa 2012 ). A small subset of chronic and severe cases is responsible for much of the disability and related medical costs due to low back pain ( Luo 2004 ). Low back pain is an intermittent and recurring condition. Among people with a resolved episode of low back pain, it is estimated that between 24% and 74% will have a recurrent episode within one year ( Pengel 2003 ; Stanton 2008 ). The usual treatment for low back pain is self\u2010care and non\u2010prescription medication such as paracetamol (acetaminophen) or non\u2010steroidal anti\u2010inflammatory drugs. For chronic low back pain, National Institute for Health and Care Excellence (NICE) guidelines, which are in the process of being updated as of September 2016, recommend exercise and some manual therapies in addition to non\u2010prescription medication ( NICE 2009 ). There are also guidelines suggesting that there is good evidence of moderate benefit for interdisciplinary rehabilitation ( Chou 2009 ). The current evidence does not provide guidance on selecting one treatment approach over another or when specific treatments are warranted, and the best treatment approaches remain unclear ( Haldeman 2008 ), while many treatments are costly and of unclear effectiveness ( Deyo 2009 ). Description of the intervention Yoga is a mind\u2010body practice originating from ancient India which has also become popular in the West over the last century ( Saper 2004 ). There are many branches and styles of yoga practice, with varying philosophies and practices, but all may be characterized by the integration of physical poses (asanas) and controlled breathing (pranayama), and frequently also the incorporation of meditation (dhyana) ( Hewitt 2001 ; Hayes 2010 ). According to the 2007 NHIS, the use of yoga in the US increased between 2002 and 2007, and in 2007 over 13 million adults had used yoga during the previous year ( Barnes 2008 ; Birdee 2008 ). According to the 2012 NHIS, the use of yoga in the US increased further in subsequent years and in 2012 over 21 million adults had used yoga during the previous year ( Cramer 2016a ). Therapeutic yoga is the use of yoga to help people with health problems manage their condition and reduce their symptoms ( International Association of Yoga Therapists 2016 ). Yoga has been suggested as being useful in managing pain and associated disability across a range of conditions, including back pain ( McCall 2007 ; Bussing 2012 ). In the 2002 NHIS Alternative Medicine Supplement survey over 10 million US adults described using yoga for health reasons; 10.5% of yoga users said that their use was for musculoskeletal conditions and 76% of these users reported that the yoga was helpful ( Birdee 2008 ). In the 2012 NHIS, 19.7% of yoga users said their use was specifically for back pain ( Cramer 2016a ). How the intervention might work Several potential benefits have been proposed in relation to the practice of yoga in persistent pain conditions, including changes in physiological, behavioural and psychological factors ( Wren 2011 ). Potential mechanisms for these changes include improved flexibility and muscular strength derived from practicing the physical poses of yoga, increased mental and physical relaxation derived from practicing controlled breathing or meditation exercises, and improved body awareness gained through both the physical and mental aspects of yoga ( Sorosky 2008 ; Daubenmier 2012 ). Why it is important to do this review Yoga is one of several complementary therapies often used to treat low back pain, and in surveys people frequently report that it is helpful ( Wolsko 2003 ; Birdee 2008 ). Several randomized controlled trials (RCTs) have tested the effectiveness of yoga in relieving the symptoms of low back pain. Since yoga is a commonly used therapy for a highly prevalent, recurrent, and bothersome health problem for which there are no clearly satisfactory treatments, and large RCTs are available, it is important to evaluate critically the current evidence for yoga as a treatment for low back pain.",
        "summary": "For adults with chronic non\u2010specific low back pain, reviewers found little to no difference between yoga and exercise in back\u2010specific function, clinical improvement at six weeks or six months, or adverse events. Pain might slightly decrease with yoga compared with exercise, and quality of life might improve at four weeks to seven months. However, most evidence is of very low certainty, implying little confidence in these results."
    },
    "CD010631": {
        "query": "How does chlorpromazine compare with atypical antipsychotic drugs at improving outcomes in people with schizophrenia?",
        "document": "Background Description of the condition Schizophrenia is a severe form of mental health disorder. It has a high lifetime prevalence rate, affecting (4 per 1000 people ( Saha 2005 ), but low incident rate because of the chronic nature of the illness. The median incident rate of schizophrenia is 15.2 per 100,000 people ( McGrath 2008 ). The International Classification of Diseases (ICD) classifies the illness into categories F20 to F29 as \u2018schizophrenia, schizotypal and delusional disorder\u2019 ( ICD\u201010 1992 ), particularly \u2018schizophrenia\u2019 in F20. The ICD\u201010 states that \"schizophrenic disorders are characterized in general by fundamental and characteristic distortions of thinking and perception, and affects that are inappropriate or blunted\". The Diagnostic and Statistical Manual of the American Psychiatric Association has also used the term \u2018schizophrenia\u2019 ( DSM\u2010IV\u2010TR 2000 ). The prognosis of schizophrenia is quite variable, and in the past psychiatrists were not very optimistic about its treatment ( Kraeplin 1919 ). However, recent studies show that the outcome of schizophrenia treatment is better than previously thought. The use of phenothiazines may have contributed to this as well as other factors, such as improving community services ( Bland 1978 ). Description of the intervention Psychiatrists have prescribed typical antipsychotic drugs since the 1950s, when the first antipsychotic medication, chlorpromazine, was synthesized. Chlorpromazine was first used as an antihistaminic agent to treat allergies. Later, surgeons used it as a pre\u2010surgical medication to sedate people before surgical procedures ( Laborit 1951 ). In 1952, Paul Charpentier, from Laboratories Rh\u00f4ne\u2010Poulenc in France, and Delay and Deniker's team described the antipsychotic properties of chlorpromazine ( Delay 1952 ). Chlorpromazine is considered a pivotal discovery in the field of psychosis treatment, with other antipsychotics often measured in 'chlorpromazine equivalents' ( Turner 2007 ; Yorston 2000 ). There are now many antipsychotic drugs available. They are broadly divided into two groups: \u2018typical antipsychotic drugs\u2019 and \u2018atypical antipsychotic drugs\u2019. Typical antipsychotic drugs are also known as \u2018first generation\u2019, \u2018conventional\u2019 or \u2018classical\u2019 antipsychotic drugs, e.g. chlorpromazine and haloperidol. Atypical antipsychotic drugs are also known as \u2018second generation\u2019 or \u2018newer antipsychotic drugs\u2019, e.g. clozapine, risperidone, quetiapine and olanzapine. Typical antipsychotic drugs have a good reputation regarding their efficacy in treating the 'positive' symptoms of schizophrenia (e.g. delusions and hallucinations) ( Mathews 2007 ). They are also well known for their adverse effects, such as movement disorders (extrapyramidal symptoms (EPS) or extrapyramidal side effects (EPSE)), sedation, metabolic syndromes and sometimes potentially fatal conditions, such as neuroleptic malignant syndrome ( Arana 2000 ). The second generation antipsychotic drugs arrived on the market, claiming notable differences. They had a reputed low side\u2010effect profile and, according to pharmaceutical companies, higher efficacy ( Janssen 1988 ). However, research funded independently of pharmaceutical companies has suggested that there may be little difference between the older and newer drugs ( Adams 2014 ). This has subsequently fuelled debate as to whether new atypical antipsychotics are more effective than older established first\u2010generation antipsychotics, and whether questioning the efficacy of the two classifications of drugs creates an improper generalisation of antipsychotics that do not form a homogenous class ( Leucht 2009 ). Against this backdrop, chlorpromazine remains a benchmark drug in the treatment of schizophrenia. Although imperfect, it is relatively inexpensive and remains one of the most common drugs used for treating people with schizophrenia worldwide ( Adams 2005 ). How the intervention might work Chlorpromazine is an aliphatic phenothiazine, which is one of the widely\u2010used typical antipsychotic drugs. Chlorpromazine is reliable for its efficacy and one of the most tested first generation antipsychotic drugs. It has been used as a \u2018gold standard\u2019 to compare the efficacy of older and newer antipsychotic drugs. It blocks alpha 1, 5HT2A, D2 and D1 receptors in the brain, and thus it works as an antipsychotic. It also has effect on muscarinic, serotonin and H1 receptors. By blocking D2 receptor it can also cause extrapyramidal side effects. Other adverse effects include dry mouth, blurred vision, restlessness, sedation, neuroleptic malignant syndrome ( DSM\u2010IV 1994 ) etc. On the other hand, atypical antipsychotic drugs by definition may cause decreased or no extrapyramidal side effects ( Kinon 1996 ). Different atypical antipsychotic drugs act in different ways; for example, clozapine blocks D2 and 5HT2 receptors ( Meltzer 1989 ). Both clozapine and quetiapine blocks more 5HT2 receptors than D2 receptors. olanzapine blocks 5HT2A, 5HT6, D1, D2, D3 and muscarinic receptors ( Zhang 1999 ). Why it is important to do this review This is one of a family of related Cochrane reviews on this important compound ( Table 1 ). Comparison Reference Chlorpromazine versus placebo Adams 2014 Chlorpromazine versus haloperidol Leucht 2008 Chlorpromazine doses Liu 2009 Chlorpromazine cessation Almerie 2007 Chlorpromazine for acute aggression Ahmed 2010 Chlorpromazine is one of two oral antipsychotic drugs on the World Health Organization's Essential Drug list ( WHO 2011 ). It is globally accessible and has been known for its effectiveness in schizophrenia treatment since the 1950s ( Adams 2014 ), and it is also the most commonly used and inexpensive treatment for schizophrenia ( Odejide 1982 ). Expensive new generation drugs are heavily marketed worldwide as a better treatment for schizophrenia. However, this may not be the case and may be an unnecessary drain on very limited resources ( Adams 2006 ). Also, comparisons with new generation drugs, which are coming off\u2010patent and are therefore more accessible, are important to assist informed and independent choice of treatment for people with schizophrenia.",
        "summary": "Chlorpromazine seems to be as effective as atypical antipsychotic drugs at improving outcomes in people with schizophrenia, although might be associated with more adverse effects. However, these results should be interpreted with caution, as they are mostly based on very low\u2010quality evidence. Also, most trials were undertaken in China and therefore, it is unclear if results could be applicable to other populations."
    },
    "CD001302": {
        "query": "How do volume expanders compare with placebo/no treatment for preventing ovarian hyperstimulation syndrome?",
        "document": "Background Description of the condition Ovarian hyperstimulation syndrome (OHSS) is an iatrogenic, serious and potentially fatal complication of ovarian stimulation which affects 1% to 14% of all in vitro fertilisation (IVF) or intracytoplasmic sperm injection (ICSI) cycles ( Garcia\u2010Velasco 2003 ). OHSS may be associated with massive ovarian enlargement, extracellular exudate accumulation combined with profound intravascular volume depletion, ascites, hydrothorax, haemoconcentration, liver dysfunction and renal failure ( Aboulghar 2003 ; Vloeberghs 2009 ). It can lead to cancellation of an IVF cycle and prolonged bed rest or hospitalisation, which may have significant emotional, social, and economic impacts ( Delvigne 2002 ; Engmann 2008 ). OHSS can be classified into an early form that is related to the ovarian response and exogenous human chorionic gonadotrophin (hCG) administration, and is detected three to nine days after hCG administration. A late form of OHSS, diagnosed 10 to 17 days later, is due to endogenous hCG ( Mathur 2000 ) and is categorised as mild, moderate, severe or life\u2010threatening. The aetiology of OHSS is not completely clear at this moment; however the syndrome is strongly associated with serum hCG and certain vasoactive substances ( Enskog 2001 ; Rizk 1997 ). Description of the intervention Many strategies have been tried to prevent OHSS and cycle cancellation such as coasting (withholding gonadotrophins before the ovulation trigger is given) ( D'Angelo 2011 ), gonadotropin\u2010releasing hormone (GnRH) agonist as an oocyte trigger in GnRH antagonist cycles ( Kol 2008 ; Youssef 2014 ), natural cycle IVF ( Edwards 2007 ), cabergoline ( Tang 2012 ), embryo freezing ( D'Angelo 2007 ), and in vitro oocyte maturation ( Loutradis 2006 ). Unfortunately, none of the strategies currently employed completely prevent OHSS after hCG administration ( Egbase 2000 ). Recently the role of vascular endothelial growth factor (VEGF), as mediator of hCG\u2010dependent ovarian angiogenesis, has emerged ( Cerrilo 2009 ). VEGF is expressed in human ovaries ( Yan 1993 ) and levels significantly increase after hCG administration leading to increased vascular permeability ( Foong 2006 ). It has been proposed that the administration of intravenous fluids such as human albumin, hydroxyethyl starch (HES), dextran or polygeline might result in a restoration of intravascular volume and inactivation of the vasoactive intermediates responsible for the pathogenesis of OHSS ( Asch 1993 ; Chen 1997 ; Isik 1997 ; Kissler 2001 ; Shalev 1995 ). How the intervention might work Albumin has both osmotic and transport functions. It contributes about 75% of the plasma oncotic pressure and administration of 50 g human albumin solution will draw more than 800 mL of extracellular fluid into the circulation within 15 minutes ( McClelland 1990 ). It has been suggested that the binding and transport properties of human albumin play a major role in the prevention of severe OHSS, as albumin may result in binding and inactivation of the vasoactive intermediates responsible for the pathogenesis of OHSS. The osmotic function is responsible for maintaining the intra\u2010vascular volume in the event of capillary leakage, thus preventing the sequelae of hypovolaemia, ascites and haemoconcentration ( Shalev 1995 ). Hydroxyethyl starch (HES) is a plasma expander that has gained recent attention as an alternative to albumin in reducing the incidence of severe OHSS. Because HES is a non\u2010biological substance, its use avoids any potential concern about viral transmission that may be present with albumin ( Abramov 2001 ; Chen 2003a ). Dextran is a complex, branched glucan composed of chains of varying lengths. It is used as an antithrombotic, to reduce blood viscosity, and as a volume expander in anaemia ( Endo 2004 ). Polygeline is a type of intravenous colloid with 3.5% urea\u2010linked gelatin used to treat OHSS ( Gamzu 2002 ). It is used in the prevention or treatment of shock associated with reduction in effective circulating blood volume due to haemorrhage, loss of plasma or loss of water and electrolytes from persistent vomiting and diarrhoea. Mannitol is a naturally occurring sugar alcohol used for its osmotic diuretic purposes; it is used as plasma expander for the protection of renal failure and in cases of intracerebral oedema ( Shawkat 2012 ). Why it is important to do this review OHSS is one of the most common adverse effects of assisted reproductive technology\u2010controlled ovarian hyperstimulation (ART\u2010COH) cycles. OHSS can result in hospital admission and in some cases in critical illness. Therefore the aim of this review is to evaluate the evidence from randomised controlled trials (RCTs) to determine whether volume expanders can reduce the incidence of moderate and severe OHSS in high\u2010risk women undergoing IVF/ICSI treatment This review provides a new evidence base for physicians and stakeholders considering the use of plasma expanders in women at high risk of developing OHSS who are undergoing IVF/ICSI treatment. This is an update of a Cochrane review first published in 1999, and previously updated in 2002 and 2011. This is the first update that aims to report 'moderate or severe OHSS' as a primary outcome, as opposed to the original primary outcome of 'severe OHSS'.",
        "summary": "Volume expanders seem to prevent ovarian hyperstimulation syndrome (OHSS), but most evidence is of low to very low certainty, precluding firm conclusions. Of note, albumin also appears to reduce pregnancy rates. In infertile women undergoing in vitro fertilization (IVF) or intracytoplasmic sperm injection (ICSI) treatment cycles and at high risk of OHSS because of high estradiol levels, very low\u2010 to low\u2010certainty evidence suggests that volume expanders (albumin, hydroxyethyl starch [HES], or mannitol) may help to prevent OHSS (on average, 46 to 288 vs 67 to 517 per 1000 women experienced this syndrome). Moderate\u2010certainty evidence shows decreased pregnancy rates with albumin compared with placebo (on average, 458 vs 541 per 1000 women) but not with HES or mannitol. Reviewers found no RCTs reporting live birth rates or adverse effects."
    },
    "CD010605": {
        "query": "How do probiotics and immunonutrition affect outcomes in people with acute pancreatitis?",
        "document": "Background Description of the condition Acute pancreatitis (AP) is a potentially life\u2010threatening inflammatory disorder of the pancreatic gland, with an incidence in most Western and Asian countries ranging between 10 and 30 per 100,000 inhabitants, and accounting for more than 270,000 hospital admissions in the United States annually ( Goldacre 2004 ; Imamura 2004 ; Lindkvist 2004 ; NIS 2012 ). An indicative increase in the incidence of AP has been reported and has been attributed to the use of more accurate diagnostic tests (i.e. computed tomography (CT) and endoscopic ultrasound) and to an increase in the incidence of gallstones and obesity ( Frey 2006 ; Yadav 2006 ). In about 80% to 85% of cases, AP presents as a mild and self\u2010limiting disease, requiring only conservative treatment; the remaining 15% to 20% of cases represent severe forms of the disease characterised by the development of local and systemic complications ( Sakorafas 2010 ; Tonsi 2009 ). Local complications consist of possible tissue destruction or necrosis; formation of a pseudocyst \u2010 an abnormal collection of fluid or necrotic material for which walls are formed by the pancreas and other surrounding organs; and formation of an enclosed collection of liquefied, dead and infected tissue, called abscess. Systemic complications are caused by a systemic inflammatory response possibly leading to organ failure (most commonly, kidney failure, respiratory failure and shock). The most common causes of AP are gallstone disease and excessive alcohol consumption, which account for more than two\u2010thirds of cases ( Munsell 2010 ). Less common causes include metabolic disorders such as hypertriglyceridaemia (abnormal elevation of serum triglycerides \u2010 normal constituents of oil and fat) and hypercalcaemia (abnormal elevation of serum calcium), autoimmune pancreatitis, various bacterial or viral infections (i.e. mumps, Coxsackievirus, Mycoplasma pneumoniae ), parasitic infestations of the biliary tract (e.g. Ascaris lumbricoides ), abdominal abnormalities, trauma and drugs (e.g. steroids, sulphonamides, furosemide, thiazides). Although the disease mechanisms of AP are still controversial, it is believed that a causative factor leads to uncontrolled activation of enzymes (chemical compounds that promote chemical reactions) within the pancreatic tissue and to self\u2010digestion of the gland, causing release of molecules that mediate the inflammatory response, tissue damage and possible necrosis. These local changes can trigger an intense inflammatory cascade leading to the development of systemic inflammatory response syndrome (SIRS) \u2010 a generalised inflammatory response affecting different organs and whole organ systems, which can consequently cause organ failure and death ( Frossard 2008 ; Kilciler 2008 ). The described events represent the first phase of the clinical course of severe acute pancreatitis (SAP), which can be followed in up to 40% of cases by a second phase marked by infection of the dead (necrotic) pancreatic tissue ( Haney 2007 ). Infected pancreatic necrosis usually develops after the first week of disease and is associated with a significant increase in the prevalence of organ failure, with death occurring in about 30% of cases ( B\u00fcchler 2000 ; Uhl 2002 ). According to clinical guidelines ( Banks 2006 ; Forsmark 2007 ; UK Working Party on Acute Pancreatitis 2005 ), the diagnosis of AP is established by the presence of two of the following three features: a compatible clinical presentation, including abdominal pain, nausea and vomiting; a three\u2010fold or greater elevation in serum amylase or lipase concentrations (digestive enzymes essential in the breakdown of starch and fat, which are released to a greater extent from the inflamed pancreas into the blood); or evidence of AP on CT. No specific treatment is available for AP. Most patients respond well to conservative management, including fluid volume resuscitation, pain control, oxygen administration, use of anti\u2010vomiting drugs and introduction to and administration of regulated food intake. Severe cases require admission to an intensive care unit and continuous monitoring of vital signs. Severe acute pancreatitis precipitates metabolic distress, leading to increased total energy expenditure and enhanced protein consumption. Therefore, nutritional support is an essential part of disease treatment ( Gianotti 2009 ; Meier 2006 ); several studies have suggested certain advantages of enteral nutrition (EN) versus total parenteral nutrition (TPN) ( Al\u2010Omran 2010 ; Yi 2012 ). Enteral nutrition comprises nutritional preparations in liquid form, which are absorbed by the intestines. It usually involves the administration of nutrients directly into the stomach or small intestine in patients who have difficulty swallowing via specific tubes that can be placed throughout the oral or nasal cavity or can be surgically implanted through the abdominal wall directly into the specified gastrointestinal organ. Enteral nutrition can also be given orally, most often as a supplement to a specific diet in malnourished patients. Total parenteral nutrition is the intravenous administration of nutrients that a patient requires via a catheter inserted into a major central or smaller peripheral vein. Use of antibiotics to prevent infection of necrotic tissue is highly debated. A recent Cochrane systematic review showed no beneficial effects of antibiotic prophylaxis, except for imipenem, an antibiotic from the carbapenem group that has a broad antibacterial activity spectrum; its use has led to a significant decrease in the incidence of pancreatic infection ( Villatoro 2010 ). Endoscopic procedures that facilitate visualisation of the common bile duct should be considered in the early stages of severe gallstone pancreatitis with co\u2010existing bile duct obstruction, infection of the biliary tract (cholangitis) or sepsis (bacterial infection of the blood) ( Frossard 2008 ; Tse 2012 ). The most common procedure of this type is endoscopic retrograde cholangiopancreatography (ERCP), in which the biliary tract is visualised under X\u2010ray imaging when a contrast agent is injected from the initial part of the small bowel into the common bile duct. In these cases, cutting the sphincter of Oddi, a muscle that lies at the junction of the intestine with both the bile and the pancreatic ducts, could facilitate removal of bile duct stones or treatment of other causes of bile obstruction. Surgical removal of necrotic tissue, as well as fluid collections, pseudocysts or abscess drainage, is indicated only when infected tissue is present. Sterile necrosis should be treated conservatively ( Isaji 2006 ; Werner 2005 ). Description of the intervention For decades, one of the main principles applied in the treatment of patients with AP has been 'nil\u2010by\u2010mouth' (no oral intake), with or without TPN, to achieve suppression of pancreatic enzyme secretion and bowel rest. However, experimental and clinical studies have demonstrated that this approach can lead to increased risk of infectious complications due to bacterial overgrowth and translocation in the gut, resulting in higher morbidity (disease state rate) and mortality (death rate) among patients with severe forms of the disease. Furthermore, SAP is marked by an increase in the amount of energy required to perform vital functions at complete rest, also called basal metabolism, with a potentially negative effect on nutritional status and disease progression ( Meier 2006 ). Therefore, adequate nutritional support is essential, preferably provided by the enteral route. Administration should start as soon as possible, especially with pre\u2010existing malnutrition, usually within 48 hours of admission ( McClave 2009 ). Nutritional support is preferably administered via a tube inserted through the nasal cavity and the upper gastrointestinal tract (oesophagus and stomach) into the middle part of the small intestine, called the jejunum. This nasojejunal tube should be placed distal to the duodenojejunal junction (the point at which the initial part of the small intestine \u2010 the duodenum \u2010 ends and the jejunum begins) blindly, endoscopically or through radiological procedures. It has been discussed that tube positioning offers several advantages: It avoids the problem of decreased or absent movement of the stomach wall (gastroparesis) and possible duodenal obstruction due to inflammation or pseudocyst formation; it also provides increased energy delivery to the small bowel and ensures better pancreatic rest than tubes placed closer to the stomach ( Thomson 2008 ). However, studies show no significantly different effects between nasojejunal and nasogastric routes of administration, whereby nutrients are delivered into the stomach ( Eatock 2005 ; Kumar 2006 ). A wide range of EN formulations are available for clinical use and for different indications. They can be divided into three groups: polymeric, oligomeric and specialised formulations. Polymeric formulations contain intact proteins, and carbohydrates are represented in the form of maltodextrins, or water\u2010soluble molecules containing three or more glucose molecules, and oligosaccharides, which are molecules that consist of two to six simple basic sugar molecules known as monosaccharides. Finally, lipids in polymeric formulations are present in the form of long\u2010chain fatty acids. Oligomeric, also known as elemental or semi\u2010elemental, formulations comprise maltodextrins and monosaccharides, medium\u2010chain fatty acids and free fatty acids; protein components consist of smaller molecules, such as free amino acids, dipeptides and tripeptides (two or three interconnected amino acids). Oligomeric formulations are preferred to polymeric formulations for the treatment of patients with AP because they are usually associated with better tolerance and absorption in the gut and improved achievement of pancreatic rest ( Makola 2006 ; Tiengou 2006 ). However, they are several times more expensive than polymeric formulations. Specialised formulations represent a larger group of specifically designed formulas enriched with different supplements. These include immuno\u2010enhanced formulations, which are enhanced by substances potentially able to modify the immune response. They most often contain specific amino acids such as glutamine and arginine, omega\u20103 fatty acids and nucleotides (chemical compounds composed of a base, a sugar molecule and a phosphate group, which are the main structural elements of nucleic acids such as deoxyribonucleic acid (DNA)). Other specialised formulations include fibre\u2010enhanced formulations that can have prebiotic activity, meaning that they can stimulate the growth of normal enteral micro\u2010organisms. Some formulations are supplemented with probiotics (substances containing live bacteria or yeasts that add to the normal gastrointestinal flora) and may contain probiotics and prebiotic fibres, which usually are called symbiotics; disease\u2010specific formulations are available ( Petrov 2009 ). The cost of these specialised preparations is even higher, but evidence of their efficiency is not reliable. In addition, formulations enriched with certain strains of probiotics have been associated with increased mortality ( Besselink 2008 ; Gianotti 2006 ). How the intervention might work Intestinal barrier dysfunction has a pivotal role in the course of AP. It is known that micro\u2010organisms responsible for pancreatic infection and septic complications are generally common enteric bacteria normally present in the gut ( Beger 1986 ; MacFie 1999 ). Disruption and overgrowth of these bacterial populations that form the normal intestinal flora in a metabolically deprived and immobile bowel could lead to bacterial and endotoxin translocation, meaning that bacteria and their toxic products could move through the intestinal membrane to emerge in the lymphatic or internal organ circulation. This mechanism is further supported by increased permeability of the intestinal membrane and local ischaemia (insufficient blood supply) of the gut due to dynamic changes in blood flow regulation in AP. The intense inflammatory state and the above mentioned processes cause impairment of the patient's immunological system ( Xu 2006 ). Direct delivery of nutrients to the gut and stimulation of metabolic activity help maintaining the structural and functional integrity of the intestinal mucosa, thereby possibly reducing septic complications and morbidity ( Buchman 1995 ). Data suggest that EN reduces the acute phase response by preserving protein metabolism of internal organs and down\u2010regulating the cytokine response (proteins acting as mediators between cells, as in the generation of an immune response) ( Windsor 1998 ). The use of immuno\u2010enhanced formulas is supposed to intensify this effect. Glutamine released from muscle tissue acts as a gene promotor for cellular protection and immune responsiveness by activating the peroxisome proliferator\u2010activated receptor gamma, an intracellular receptor that regulates glucose metabolism and fatty acid storage. In addition, glutamine is a potent antioxidant through its metabolite glutathione, which is a tripeptide important for the protection of various cellular structures and the detoxification of harmful compounds. Furthermore, glutamine stimulates production of arginine \u2010 another supplement that has demonstrated potential effects by influencing the production of nitric oxide (a naturally occurring gas in the body that stimulates blood vessel dilation and improves blood flow). Nucleotides act as prebiotics \u2010 substances that stimulate the growth of beneficial enteric bacteria. Fish oils containing omega\u20103 fatty acids have a suppressive effect on endothelial cells and pro\u2010inflammatory mediators. Their effects are believed to result from inhibition of nuclear factor kappa B (a protein that controls gene expression), displacement of arachidonic acid from cellular membranes and stimulation of leukotriene B4 and prostaglandin E2 production ( Santora 2010 ). Arachidonic acid is an essential fatty acid that is the precursor to leukotrienes and prostaglandins, which are classes of molecules produced by cells to mediate allergic and inflammatory reactions. Why it is important to do this review Acute pancreatitis represents a global burden of morbidity and mortality with an increasing incidence. As the result of differences among the studies conducted to date and a variety of accessible preparations for enteral feeding, a systematic review of specific formulations is needed to try to determine the most efficient and cost\u2010effective use of enteral nutrition in these patients.",
        "summary": "The randomized controlled trial evidence for the use of probiotics or immunonutrition in adults with acute pancreatitis who were receiving enteral nutrition was of very low\u2010 to low\u2010quality, precluding firm conclusions as to their benefits or harms from being drawn. Very low quality evidence showed no apparent difference between groups in all\u2010cause mortality, systemic inflammatory response syndrome (SIRS) organ failure or adverse events, when probiotics (bacterial strains varied) were compared with placebo/no probiotics in adults with acute pancreatitis. When assumptions were made as to the outcomes of the people who withdrew from the trial (therefore missing data), these assumptions affected the results of, and potential conclusions drawn from, the analyses. Consequently, the reliability of the main pooled results based on the people still available for analysis at the time of follow\u2010up must be questioned, and firm conclusions regarding the impact of enteral nutrition on these outcomes cannot be drawn. There was no apparent difference between groups in the duration of hospital stay or complications; quality of life, urinary tract infection and septicemia were not assessed in the trials. Low\u2010quality evidence showed that the use of immunonutrition (mostly polymeric enteral nutrition supplemented with a variety of macro\u2010 and/or micronutrients) reduced all\u2010cause mortality (77 versus 159 died per 1000 people) in adults with acute pancreatitis, when compared with unenriched enteral nutrition (or enriched only with fibers), placebo or no intervention. Very low\u2010 to low\u2010quality evidence showed no apparent difference between groups in the number of people who developed SIRS, organ failure, duration of hospital stay, complications, or adverse effects. There were only three participants in the trials with missing data (two immunotherapy, one control); varying the assumptions for the outcomes of these participants did not alter the results."
    },
    "CD010412": {
        "query": "How does substitution of doctors for nurses in primary care impact accessibility, quality, and continuity of care?",
        "document": "Background Description of the topic Most countries are facing a chronic shortage and maldistribution of health workers ( Campbell 2013 ). It is acknowledged that human\u2010resource shortages in public healthcare systems play an important role in unsatisfactory health outcomes such as higher maternal mortality rates ( Campbell 2013 ). The problem of human\u2010resource shortages is particularly challenging in low\u2010 and middle\u2010income countries (LMICs) in sub\u2010Saharan Africa, and in parts of Asia and the Americas. At the same time, the demand for health care is rising. There is a need to strengthen health systems and equip them with effective and efficient health service delivery strategies, as well as increase the coverage and reach of the effective services that are already in place ( WHO 2008 ). Governments worldwide are using several approaches to address this problem. One key approach is the moving of tasks from more specialised or highly\u2010trained to less specialised or less highly\u2010trained health workers, for instance by transferring certain tasks from doctors to nurses or midwives; sometimes referred to as 'task\u2010shifting' or 'optimising' ( WHO 2004 ). By reorganising the health workforce in this way, policymakers hope to make more efficient use of the human resources already available ( WHO 2012 ). One particular type of task\u2010shifting is the substitution of doctors by nurses. Doctor\u2010nurse substitution may help to address doctor shortages and reduce doctor workload. Substitution is not a new strategy. For example, high\u2010income countries (HIC) such as Australia, the UK and the USA have extended nurses' tasks to include the prescription of routine medications ( Cutliffe 2002 ; Hobson 2010 ; Stenner 2010 ). Also, a number of LMICs such as Ethiopia, Haiti, Malawi, Mozambique, Namibia, Rwanda, Uganda and Zambia are currently implementing this strategy to address the chronic shortage of health workers, particularly in the context of generalised HIV epidemics ( Assan 2008 ; Freund 2015 ; Koenig 2004 ; Morris 2009 ). One overview of systematic reviews considered the evidence for policy options for human resources, such as substitution or shifting tasks between different types of health workers, and assessed the effectiveness of these strategies in LMICs ( Chopra 2008 ). Results showed that evidence from LMICs is sparse, and the studies are less rigorous than those from high\u2010income settings. The authors concluded that more reviews on the effects of policy options to improve human resources in such countries are needed. Different arguments can be put forward to explain why doctor\u2010nurse substitution strategies are employed (e.g. Contandriopoulos 2015 ; Freund 2015 ; Kooienga 2015 ; Martinez\u2010Gonzalez 2014a ; Newhouse 2011 ). Substitution may reduce the cost of providing health care (as nurses are usually paid less than doctors), and hence may be more affordable for the health systems and users of care. Substitution may reduce the cost of providing health care (as nurses are usually paid less than doctors), and hence may be more affordable for the health systems and users of care. This is the main reason that policymakers may consider substituting doctors with nurses. Evidence on this is not clear\u2010cut ( Dierick\u2010van Daele 2009 ; Hollinghurst 2006 ; Liu 2012 ). The Cochrane Library includes a review exploring the effectiveness of the substitution of general practitioners (family doctors) by nurses in primary care ( Laurant 2018 ). This review suggested that nurse\u2010led care may make little or no difference to the cost of care compared to doctor\u2010led primary care ( Laurant 2018 ). In another systematic review of substitution (task\u2010shifting) strategies for HIV care in Africa, the authors concluded that the delegation of tasks to nurses offered cost\u2010effective care to more patients than a doctor\u2010centred model ( Callaghan 2010 ). Substitution may improve access to primary care services as nurses may be available in settings where access to doctors is limited. Substitution may improve access to primary care services as nurses may be available in settings where access to doctors is limited. Substitution of doctors with nurses is one strategy for improving access. Nurses tend to provide more health advice (although an overall effect size could not be calculated), and are likely to achieve slightly higher levels of patient satisfaction compared to primary care doctors ( Laurant 2018 ). Other reviews have also shown that nurses in advanced roles represent a substantial source of human capital for increasing access to (primary) care ( Martinez\u2010Gonzalez 2014a ; Martinez\u2010Gonzalez 2014b ). Substitution may enhance the quality of services provided in primary care. For example, patient education may be better when delivered by nurses. Substitution may enhance the quality of services provided in primary care. For example, patient education may be better when delivered by nurses. Trained nurses can provide equal or potentially probably even better quality of care than primary care doctors and achieve equal or better health outcomes for patients ( Laurant 2018 ; moderate\u2010certainty evidence (GRADE)). Substitution may result in better retention of the nursing workforce by providing new clinical career pathways for experienced and higher educated nurses, further addressing nursing workforce shortages. Substitution may result in better retention of the nursing workforce by providing new clinical career pathways for experienced and higher educated nurses, further addressing nursing workforce shortages. Deploying nurses as professional substitutes for doctors may improve retention among the nursing workforce ( Kroezen 2015 ). However, the potential relationships between the implementation of substitution strategies and health system objectives are not straightforward and might vary based on the setting and the organisation of care. The complexity of doctor\u2010nurse substitution and its interactions with the contextual factors in each setting has meant that it is difficult to explain why and how the intervention works, or does not work, in different settings. Substitution might also address equity concerns (for instance, by improving access to those most in need and most likely to benefit from care) without incurring additional costs. Furthermore, the long\u2010term cost\u2010effectiveness of a service might differ from short\u2010term outcomes, which are easier to assess. Rashid 2010 conducted a systematic review exploring the benefits and limitations of the expansion of clinical tasks among nurses working in general practice in the UK. The focus of the review was to establish whether the findings of a previous Cochrane Review ( Laurant 2005 ) were still relevant in the light of the more recent expansion of nurses' clinical tasks in the UK general practice setting. In this review, they integrated qualitative evidence from the UK with evidence on the effectiveness of doctor\u2010nurse substitution in primary care. The authors clustered the findings of this review under three themes: the impact on patients, on nurse competence, and on UK National Health Service policy. According to the findings, patients generally thought that all general practice nurses would be able to deal with simple conditions, but preferred to consult with a general practitioner if they thought it necessary. Indeed, there were concerns about nurses' knowledge base, particularly in diagnostics and therapeutics, and their levels of training and competence in tasks formerly undertaken by general practitioners. The review concluded that studies in this key area of healthcare policy were limited. As most of this limited evidence was from the UK, it was unclear to what extent these findings would apply to other settings. Description of the intervention In doctor\u2010nurse substitution strategies, nurses take on roles that were previously performed by doctors. The nature of the contribution that nurses substituting for doctors provide in clinical practice is complex and depends on several factors, including the setting, the tasks assigned to nurses, and the extent to which these tasks are accepted. Tasks can be supplementary to those performed by doctors or can be a substitution for doctors' tasks. This QES focused on tasks in which nurses substituted for doctors, meaning that they provided the same services as doctors ( Laurant 2018 ). Why is it important to do this synthesis? The last decade have seen strong development in systematic review methodology for synthesising qualitative studies, including within Cochrane ( Noyes 2009 ). The Cochrane Qualitative and Implementation Methods Group has identified around 500 such reviews; although very few of these are of direct relevance to policymakers making health workforce decisions in LMICs. It has been argued that in all countries, including resource\u2010poor countries, evidence\u2010informed decision\u2010making is essential ( Chinnock 2005 ; Garner 1998 ; Oxman 2010 ). Policymakers need different types of evidence when choosing appropriate strategies. This includes reliable evidence about local context; but also global research evidence about the effectiveness of different strategies, and about potential factors influencing their implementation and success. A QES can help in identifying factors influencing the success of substitution interventions, including the attitudes and experience of the health workers themselves; as well as those of other stakeholders ( Harden 2004 ; Thomas 2008 ). The previous review on this issue conducted by Rashid was limited to UK studies only and covered a specific period of time (2004 to 2009) ( Rashid 2010 ). While the Cochrane intervention review on doctor\u2010nurse substitution concluded that the effectiveness of doctor\u2010nurse substitution strategies was promising (with certainty of the evidence (GRADE) moderate for mortality, patient health status, satisfaction and resource utilisation; and low for quality of life), the results of the included trials were heterogeneous ( Laurant 2018 ). This finding is not unexpected given the complexity and variability of these types of interventions. In addition, the level of organisation and support associated with these trial interventions may have been higher than in real\u2010life settings. If these types of interventions are to be successfully implemented, we need a clearer understanding of the factors that influence their implementation, success and sustainability. Such factors may include the values and preferences of stakeholders and the feasibility and applicability of the intervention for particular settings and healthcare systems.",
        "summary": "The effects of substituting doctors for nurses in primary care on accessibility, quality, and continuity of care are perceived as beneficial by most recipients of health care and by doctors and nurses. Confidence in the evidence was rated as moderate to high. Recipients of health care report that substitution of doctors for nurses in primary care increases accessibility and provides advantages such as shorter time to obtain appointments, shorter waiting times, and lower travel costs. Recipients in most studies were satisfied with nurses' social skills and felt that benefits included nurses listening more carefully, paying more attention, using face\u2010to\u2010face interaction, taking a holistic approach to care, and speaking to them on their own level. In contrast, some recipients felt it was easier to communicate with doctors because they had known them for longer, or because they believed nurses were too overworked to help recipients improve their knowledge and skills. Some recipients reported concerns over the continuity of care provided by nurses and felt insecure about losing contact with their doctors because they believed that doctors\u2019 continued involvement in their care is important. From the perspective of allied healthcare professionals, doctors and nurses saw substitution and collaborative practices as a way of providing quick access to some services; improving quality of care by taking advantage of the social skills of nurses; improving safety by relying on nurses' ability to put together several parts of a complex patient picture through their clinical competence, leadership, and collaborative practice; and increasing time available to focus on each patient's situation. Some doctors felt that substitution improved the continuity of care and believed that recipients would prefer to see the same nurse rather than different doctors."
    },
    "CD007874": {
        "query": "What are the effects of lidocaine for reducing propofol\u2010induced pain on induction of anesthesia?",
        "document": "Background Propofol is used intravenously for the induction and maintenance of anaesthesia. The main disadvantage of propofol is pain on injection, however it is a popular induction agent for ambulatory surgery as it provides smoother induction and faster recovery than other agents such as thiopental, which is considered to be the standard induction agent ( Kevin 2003 ; McCluskey 2003 ). Description of the condition Propofol is a phenol compound that causes skin and mucous membrane irritation, leading to pain at the injection site in 80% to 90% of people ( Kwak 2007a ). Although the underlying mechanisms are still not fully understood, pain immediately after injection of propofol may be caused either by direct stimulation of nociceptors and free nerve endings in the venous wall or indirectly by the release of mediators, such as bradykinin and prostaglandin E 2, which stimulate afferent nerve endings, leading to a delayed onset of pain ( Brooker 1985 ; Eriksson 1997 ; Wong 2001 ). Description of the intervention Many different physicopharmacological interventions have been proposed to reduce the incidence and severity of this adverse effect of propofol ( Appendix 2 ; Jalota 2011 ; Picard 2000 ). Lidocaine in various dosages and concentrations, or combined with other interventions for reducing pain on propofol injection, has been extensively evaluated and seems to be the most promising drug for this condition. To reduce pain on propofol injection, lidocaine is administered either by mixing with the propofol and injecting both together, or by injecting separately, as an intravenous pretreatment prior to the propofol injection. Concerning the physicochemical reaction of propofol\u2010lidocaine mixtures, there have not been any reports of adverse reactions in vivo. However in vitro there was a report of coalescence of oil droplets (diameters \u2265 5 micron) 30 minutes after the addition of 40 mg lidocaine to propofol. This reaction is time\u2010 and dose\u2010dependent and may cause pulmonary embolism, depending on the dose of lidocaine. In addition, propofol concentrations in the mixture with 40 mg of lidocaine decreased linearly and significantly from 4 hours to 24 hours after preparation, while those combined with 20 mg or less of lidocaine were unchanged ( Masaki 2003 ). How the intervention might work Lidocaine appears to provide good results in preventing pain on injection by propofol. The mechanisms of action are still unclear. A preceding injection of lidocaine caused a reduction of pain, probably due to the direct effect of local anaesthetics on vascular smooth muscle ( Nicol 1991 ). In addition, the discomfort of pain on injection could be reduced by mixing a small amount of lidocaine to the propofol. This might be because lidocaine hydrochloride is a weak free base cation solution, which would lower the pH after mixing it with propofol ( Brooker 1985 ; Eriksson 1997 ). However, the right dose and concentration are required to demonstrate good efficacy ( Adachi 2002 ). Why it is important to do this review Although people suffer from pain only temporarily after injection of propofol, the level of pain may be severe ( Michael 1996 ). This results in them having a terrible anaesthesia experience. Therefore, many anaesthesiologists are concerned about this issue and a great amount of research has been conducted in order to prevent the pain from propofol injection. Among all the drugs and interventions that can decrease pain on propofol injection, lidocaine is a very common drug, being both effective and easily available worldwide. The cost of lidocaine is also relatively low. Therefore, lidocaine is of particular interest to us. Even though Picard 2000 produced a quantitative systematic review on this subject, and Jalota 2011 produced a systematic review, , there have been a significant number of trials using lidocaine to prevent pain from propofol injection since these reviews were published. Also, no systematic review focusing on lidocaine for preventing high\u2010intensity pain has been published. Therefore, the aim of this systematic review was to evaluate, with the best available evidence, the efficacy of lidocaine treatment for preventing high\u2010intensity pain following propofol injection.",
        "summary": "Lidocaine is an effective treatment for reducing pain induced by propofol injection during anesthesia induction in adults. High\u2010quality evidence showed that lidocaine (\u2264 20 mg to > 20 mg, or \u2264 0.2 mg/kg to > 0.2 mg/kg; either as an admixture or as pretreatment), in comparison with placebo, significantly reduced the proportion of adults with high\u2010intensity pain (on average 77 to 141 versus 305 to 464 per 1000 people), or any pain (on average 209 to 287 versus 500 to 702 per 1000 people) associated with propofol administration, with no apparent increase in adverse effects (low\u2010quality evidence). However, the analysis examining adverse effects was based on information from relatively few participants, and the results should be interpreted with caution."
    },
    "CD006027": {
        "query": "How do nonsteroidal anti\u2010inflammatory drugs (NSAIDS) compare with other analgesics and each other in people with acute renal colic?",
        "document": "Background Description of the condition Renal or ureteric colic is a symptom complex that is characteristic for the presence of obstructing urinary tract calculi. Urolithiasis is a relatively common disease and its incidence and prevalence is increasing worldwide due to lifestyle and dietary factors. The prevalence of urolithiasis is estimated at between 10% and 15% in the United States ( Pearle 2012 ). Caucasian males are more likely to develop urinary calculi ( Menon 2002 ).The symptoms include flank or abdominal pain radiating to the groin or genitalia. The central factors in the pathogenesis of renal colic are obstruction of the urinary flow and increased pressure proximal to the point of blockage. The increasing pressure stimulates the synthesis and release of prostaglandins. Prostaglandins promote vasodilation and increased urine output leading to higher pressure inside the collecting system. Renal colic pain is typically intense. Nausea and vomiting are common. Although most calculi pass spontaneously and do not need surgical intervention, during this period patients may suffer from severe pain. Therefore, satisfactory analgesia is of paramount importance in their management. Description of the intervention A wide range of drugs (opioids and non\u2010opioids) are used to treat pain and discomfort in patients with acute renal colic. The non\u2010opioid drugs include but not limited to: NSAIDs (nonsteroidal anti\u2010 inflammatory drugs), antispasmodics, acetaminophen, calcium channel blockers and desmopressin. NSAIDs are commonly used as standard analgesics and opioids are used as rescue medications for acute renal colic. These two groups of medications have been compared in a previous review ( Holdgate 2005a ). In this present study we compared the analgesic effects of non\u2010opioids for acute renal colic. NSAIDs mainly work by inhibiting the cyclooxygenase enzyme which induces a subsequent inhibition in prostaglandin synthesis ( Vane 1971 ). Antispasmodic medications are sometimes used alone or in combination with other analgesics for treatment of acute renal colic and work by inducing smooth muscle relaxation in urinary tract. Acetaminophen which is a non\u2010salicylate with weak anti\u2010inflammatory potency is thought to work by inhibition of a third isoform of cyclooxygenase (COX\u20103) ( Chandrasekharan 2002 ). How the intervention might work During the initial phase of obstruction glomerular vasodilation leads to increase urine output and further increase in intra\u2010ureteral pressure. This in turn results in prostaglandin synthesis in the ureteral wall, contraction of smooth muscle and further pain. Thus, pain control may be aimed at inhibiting prostaglandin synthesis (prostaglandin inhibitors or non\u2010steroidal anti\u2010inflammatory drugs (NSAIDs)), reducing spastic ureteral contraction (antispasmodics) or diminishing the pain by intervening at the level of central nervous system (opioids) ( Gulmi 2002 ). Why it is important to do this review A plethora of NSAIDs has been used for renal colic, belonging to different classes. In a systematic review by Holdgate 2005a , NSAIDs and opioids were both effective in the management of renal colic but there was higher risk of nausea and vomiting with opioids. There is no systematic review of the efficacy and side effects of these different agents or classes. In addition NSAIDs have not been compared to other non\u2010opioid medications in terms of their efficacy and side effect profiles.",
        "summary": "NSAIDs appear to provide better pain relief in people with acute renal colic when compared with antispasmodics; however, adverse events were poorly reported, and the analyses included dipyrone/metamizole, which was banned by the US Food and Drug Agency due to safety concerns in 1977 although it remains available in other countries. When an NSAID (baralgan, diclofenac, ketorolac, dipyrone/metamizole) was compared with an antispasmodic (hyoscine, drotaverine, ondansetron, papaverine, cizolirtine) in people with acute renal colic, no difference was detected in terms of pain scores 60 minutes after administration. More people experienced a 50% reduction in pain within 60 minutes with an NSAID compared with hyoscine (on average 666 versus 273 per 1000 people), but not when compared with drotaverine or cizolirtine; the trials evaluating these two drugs were very small and unlikely to be adequately powered. There was no apparent difference between NSAIDs and antispasmodics in the need for rescue medication after 30 minutes if adequate pain relief was not achieved. When the trial comparing cizolirtine with metamizole was removed from the analysis, fewer people needed rescue medication with NSAIDs (on average 114 versus 333 per 1000 people). Cizolirtine citrate is a new analgesic that is neither an NSAID nor antispasmodic, and is therefore clinically different from the other drugs in the comparison. The removal of this trial from other analyses did not alter those results. Dipyrone/metamizole (contained in baralgan) was banned by the US Food and Drug Agency 1977. When an NSAID (piroxicam, avafortan, baralgan, dipyrone, or diclofenac) was combined with an antispasmodic (phloroglucinol, hyoscine, pitofenone, papaverine or unspecified antispasmodic) and compared with an NSAID alone the mean pain score was slightly lower (by 2 points on a 100\u2010point scale) with the combination. No differences were detected in the number of people experiencing a 50% reduction in pain within 60 minutes, needing rescue medication after 30 minutes if adequate pain relief was not achieved, or recurrence of pain. When different NSAIDs were compared, there were no consistent patterns across pain outcomes. The trials evaluating NSAIDS, alone or in combination with another non\u2010opioid, compared with a non\u2010opioid analgesia were too small to provide clinically meaningful results. The reporting of adverse effects was variable across comparisons and trials. No trial reported serious adverse effects such as gastro\u2010intestinal bleeding or kidney impairment. Overall, gastrointestinal adverse effects seemed to be more commonly associated with NSAIDS than non\u2010NSAIDs, and when comparing different NSAIDs, gastrointestinal adverse effects seemed to be a common occurrence."
    },
    "CD011136": {
        "query": "How does videolaryngoscopy compare with direct laryngoscopy for adults requiring tracheal intubation?",
        "document": "Background Description of the condition Securing the patient\u2019s airway is a critical step in providing general anaesthesia. Recent data from the Fourth National Audit Project of the Royal College of Anaesthetists and Difficult Airway Society (NAP4) in the UK suggest that tracheal intubation is used for airway management in 38.4% of general anaesthetics, estimated at 1.1 million procedures per year ( Woodall 2011 ). A cuffed tracheal tube, which is considered the most reliable device for securing the airway, is inserted through the mouth and larynx and into the trachea to enable oxygenation and ventilation, and to prevent aspiration, during general anaesthesia. A clear view may be achieved by flexing the lower cervical spine and extending the upper cervical spine (a 'sniffing the morning air' position), enabling the intubator to create 'line of sight' to the larynx to pass the tracheal tube. Retractor type laryngoscopes, typically a detachable metal blade with handle (e.g. the Macintosh curved blade), are used to retract the tongue and soft tissue in the floor of the mouth during this procedure, which is termed 'direct laryngoscopy'. However, although these laryngoscopes may be adequate for moving soft tissue, the intubator still requires line of sight to the larynx, provided by correct head and neck positioning of the patient. Failed or difficult intubation is associated with complications, such as increased risk of hypertension, desaturation, unexpected admission to the intensive care unit (ICU) and death ( Caplan 1990 ; King 1990 ; Rose 1994 ). Such difficulties during intubation are estimated to occur in 1% to 6% of cases, whereas failed intubation occurs in only 0.1% to 0.3% ( Crosby 1998 ; Shiga 2005 ). Airway management difficulties are increased when patients are obese ( Juvin 2003 ; Lundstrom 2009 ). In the UK, NAP4 showed that obese patients accounted for 42% of patients who experienced a major airway complication during anaesthesia ( Cook 2011 ). Functional residual capacity (FRC), which is the volume of air left in the lungs at the end of normal expiration, is reduced in obese patients; this, along with other factors, reduces respiratory reserve and makes these patients vulnerable to hypoxia if an airway is lost, making airway management more time critical and increasing the risk of postoperative chest infection and other complications ( Adams 2000 ; Malhotra 2008 ; Marley 2005 ). In addition to obesity, intubation may prove difficult for other reasons, for example, restrictions in neck flexion, a narrow jaw opening, an enlarged tongue, poor tissue mobility and cervical instability. Predictive tests, for example, the Mallampati or Wilson index test ( Mallampati 1985 ; Wilson 1988 ), are used before anaesthesia is given. The Mallampati score, which is based on the view of the soft palate when the patient opens his mouth, is the most widely used predictor of difficult intubation, but this and other prediction tests have been shown to have low positive predictive value for difficult intubation ( Shiga 2005 ). Patients who are admitted to the ICU and to the emergency department may differ from elective patients scheduled for general anaesthesia. Many patients are admitted to the ICU or the emergency department because they have vulnerable airways, which may be due to major trauma requiring cervical spine protection, airway swelling, direct airway trauma or lung injury, major head and neck surgery or infection. Critical care teams may need to provide airway management in the emergency department at very short notice without the presence of an anaesthetist ( Cook 2011 ). Alternative devices, such as a videolaryngoscope (VLS), rely on fibreoptic or digital technology to transmit an image from the tip of the laryngoscope to an eyepiece or monitor, where it is viewed by the intubator. These devices may be flexible or rigid in design for the purpose of assisting in difficult intubations and reducing difficulty, failure, trauma and other complications. For this review, we are interested in the rigid videolaryngoscope, which uses a blade to retract the soft tissues and transmits a video image to a screen attached to the end of the handle or to a monitor. This design enables a lighted view of the larynx without direct 'line of sight' and therefore can assist when difficulty is encountered (or predicted) with direct laryngoscopy. The Cormack and Lehane classification system describes the intubator\u2019s view of the larynx during laryngoscopy ( Cormack 1984 ), with a score or 4 indicating a poor view and a score of 1 indicating a good view. Studies suggest that the use of videolaryngoscopes improves these visualization scores (e.g. a Storz V\u2010Mac videolaryngoscope compared with a Macintosh laryngoscope in Kaplan 2006 ). Videolarngoscopes may therefore provide the possibility of more successful intubation for patients in whom direct laryngoscopy may be difficult. They also may be used after unsuccessful attempts to intubate with direct laryngoscopy. Why it is important to do this review Use of a videolaryngoscope may aid visualization, but evidence is required to establish whether this equates with increased success of intubation with reduced complications. Recent non\u2010Cochrane reviews of VLS models have concentrated on their impact on process measures, such as the view of the larynx, first\u2010time and overall intubation success rates and intubation time, and have concluded that there is limited evidence to support their use in tracheal intubation in unselected populations and in those with a known or anticipated difficult direct laryngoscopy ( Griesdale 2012b ; Healy 2012 ; Niforopoulou 2010 ). A systematic review and meta\u2010analysis of 17 studies of the GlideScope reported advantages for non\u2010expert intubators ( Griesdale 2012b ). No reviews have considered the use of VLS specifically in obese patients. The prevalence of obesity is increasing in both developed and developing countries (current figures: http://www.oecd.org/ ), as is the number of obese patients requiring anaesthesia. It is important to establish whether videolaryngoscopy is a more effective technique for this patient group, as well as for other selected and unselected groups. We wish to update the non\u2010Cochrane reviews above by focusing only on evidence derived from randomized controlled trials (RCTs) and by considering, when possible, patient relevant outcomes such as complications. We aimed to consider studies in both unselected and selected populations, and to include studies of obese participants. This review will continue the work of the current review authors in published reviews such as \"Supraglottic airway devices versus tracheal intubation for airway management during general anaesthesia in obese patients\" ( Nicholson 2013a ) and \"Tracheal intubation with a flexible intubation scope for obese patients requiring general anaesthesia\" ( Nicholson 2013b ). This review does not focus on videolaryngoscopy in children, as this topic is the focus of another Cochrane review ( Abdelgadir 2014 ).",
        "summary": "Use of videolaryngoscopy seems to improve the success of tracheal intubation compared with use of a Macintosh blade in circumstances in which intubation is difficult. Moderate\u2010quality evidence shows that use of videolaryngoscopy resulted in fewer cases of failed intubation among people requiring tracheal intubation when compared with use of a Macintosh blade (on average, 35 vs 94 per 1000 people). Subgroup analyses conducted by type of scope (GlideScope, Pentax AWS, McGrath, C\u2010MAC), airway difficulty (predicted not difficult, predicted difficult, simulated difficult), and experience of personnel (experienced with both devices, experienced with videolaryngoscopy) suggest that this benefit may be most pronounced with C\u2010MAC scopes and difficult intubations. The latter statement is supported by the finding that more people had a lower intubation difficulty score with videolaryngoscopy than with a Macintosh blade (523 vs 133, with the best score of zero per 1000 people). Trials reported no differences between videolaryngoscopy and a Macintosh blade in success at the first attempt (moderate\u2010quality evidence) or in number of intubation attempts (28 randomized controlled trials [RCTs], 6692 participants). Results show no difference overall between groups in visualization of the larynx, but when numbers of people with specific visualization scores were analyzed separately, more people had better scores (1 or 2) and fewer people had worse scores (3 or 4) with videolaryngoscopy."
    },
    "CD011990": {
        "query": "Can keratinocyte growth factor (KGF) help to prevent oral mucositis in people receiving cancer treatment?",
        "document": "Background Description of the condition Treating cancer with chemotherapy, radiotherapy of the head and neck, or targeted therapy can cause toxic oral side effects ( Al\u2010Dasooqi 2013 ; Scully 2006 ; Sonis 2004 ). Perhaps the most widely researched of these side effects is oral mucositis ( Al\u2010Dasooqi 2013 ), which affects at least 75% of high risk patients (those receiving head and neck radiotherapy or high\u2010dose chemotherapy) ( Scully 2006 ). Oral mucositis may be under\u2010reported in lower risk groups for various reasons: their tendency to be outpatients with less observation; less reporting of moderate mucositis; or patients and clinicians wishing to avoid any disruption to optimal cancer treatment ( Scully 2006 ). Simply put, oral mucositis affects the oral mucosa (the mucous membrane of moist tissue lining the oral cavity) and can lead to the development of lesions (ulcers). However, the process that leads to oral mucositis is complex and multifactorial, with Sonis' five\u2010phase model being a widely accepted description of the sequence of events underlying the condition ( Sonis 2004 ; Sonis 2009 ). Initiation: DNA damage caused by chemotherapy or radiotherapy results in the loss of ability to proliferate in the basal cells of the epithelium (the external layers of cells lining the oral mucosa). This produces reactive oxygen species (ROS). Primary damage response: radiotherapy, chemotherapy, ROS, and DNA strand breaks all contribute to the activation of transcription factors such as nuclear factor kappa beta (NF\u2010K\u03b2), and sphingomyelinases. All this leads to the upregulation of pro\u2010inflammatory cytokines (e.g. tumour necrosis factor alpha \u2010 TNF\u2010\u03b1), nitric oxide, ceramide, and matrix metalloproteinases, resulting in the thinning of the epithelium through tissue injury and cell death, culminating with the destruction of the oral mucosa. Signal amplification: some of the molecules in the previous phase can lead to the exacerbation and prolonging of tissue injury through positive or negative feedback (e.g. TNF\u2010\u03b1 can positively feedback on NF\u2010K\u03b2 thus inducing more pro\u2010inflammatory cytokine production). Ulceration: bacteria colonise ulcers and their cell wall products infiltrate the submucosa (the connective tissues beneath the oral mucosa), activating tissue macrophages (white blood cells that respond to infection or damaged/dead cells), which results in further production of pro\u2010inflammatory cytokines, inflammation, and pain. Healing: signalling from the extracellular matrix of the submucosa results in epithelial proliferation and differentiation, and thus a thickening of the epithelium. The local oral flora are reinstated. Initiation: DNA damage caused by chemotherapy or radiotherapy results in the loss of ability to proliferate in the basal cells of the epithelium (the external layers of cells lining the oral mucosa). This produces reactive oxygen species (ROS). Primary damage response: radiotherapy, chemotherapy, ROS, and DNA strand breaks all contribute to the activation of transcription factors such as nuclear factor kappa beta (NF\u2010K\u03b2), and sphingomyelinases. All this leads to the upregulation of pro\u2010inflammatory cytokines (e.g. tumour necrosis factor alpha \u2010 TNF\u2010\u03b1), nitric oxide, ceramide, and matrix metalloproteinases, resulting in the thinning of the epithelium through tissue injury and cell death, culminating with the destruction of the oral mucosa. Signal amplification: some of the molecules in the previous phase can lead to the exacerbation and prolonging of tissue injury through positive or negative feedback (e.g. TNF\u2010\u03b1 can positively feedback on NF\u2010K\u03b2 thus inducing more pro\u2010inflammatory cytokine production). Ulceration: bacteria colonise ulcers and their cell wall products infiltrate the submucosa (the connective tissues beneath the oral mucosa), activating tissue macrophages (white blood cells that respond to infection or damaged/dead cells), which results in further production of pro\u2010inflammatory cytokines, inflammation, and pain. Healing: signalling from the extracellular matrix of the submucosa results in epithelial proliferation and differentiation, and thus a thickening of the epithelium. The local oral flora are reinstated. However, there remains a lack of clarity around mechanisms and risk factors for oral mucositis, particularly areas such as genetic predisposition and microbial effects. Understanding of the pathobiology leading to mucosal toxicity as a result of targeted therapies (e.g. mammalian target of rapamycin (mTOR) inhibitor\u2010associated stomatitis \u2010 mIAS) is also currently limited, but it is thought to differ from chemotherapy\u2010 and radiotherapy\u2010induced mucositis, and the clinical presentation of the ulcers is more similar to aphthous stomatitis ( Al\u2010Dasooqi 2013 ; Boers\u2010Doets 2013 ; Peterson 2015 ). Oral mucositis is an acute condition and, when caused by chemotherapy, ulceration normally occurs one week after treatment and resolves within three weeks of treatment ( Sonis 2009 ). Radiotherapy\u2010induced oral mucositis takes longer both to develop and to heal, with ulceration normally occurring around two weeks into a seven\u2010week treatment cycle, and resolving three to four weeks after treatment has ended ( Sonis 2009 ). Ulceration is the most significant phase as it leads to pain of varying severity, and difficulties with eating, swallowing, and talking ( Scully 2006 ). This in turn leads to the consumption of pain relief medication, nutritional support (i.e. nasogastric or intravenous feeding), treatment of the oral mucositis, specialist oral hygiene care, increased medical appointments and use of staff and resources, and, in some instances, hospitalisation ( Jensen 2014 ; Miller 2001 ; Trotti 2003 ). Thus the negative impact on the quality of life of cancer patients, when they are already suffering, is severe ( Elting 2008 ; Epstein 1999 ). Further problems can occur in immunosuppressed patients if whole bacteria on the ulcer surface cross into the underlying submucosa, potentially leading to bacteraemia and sepsis, which require antibiotics and hospitalisation, and can cause death ( Jensen 2014 ; Peterson 2015 ; Scully 2006 ). Therefore, oral mucositis can be a dose\u2010limiting condition, disrupting a patient's optimal cancer treatment plan ( Jensen 2014 ; Peterson 2015 ; Sonis 2004 ). The additional costs associated with oral mucositis are significant, with one study reporting a median incremental cost of USD 18,515 per patient ( Nonzee 2008 ). These costs have been reported to be as much as USD 42,749 more per patient when ulcerative oral mucositis is present ( Sonis 2001 ). Description of the intervention As described above, oral mucositis occurs partly as result of the loss of regenerative ability of the oral epithelial cells. Growth factors and anti\u2010inflammatory cytokines are used to counteract the biological processes leading to this loss of proliferative ability. Growth factors and anti\u2010inflammatory cytokines include ( Raber\u2010Durlacher 2013 ): keratinocyte growth factor; colony\u2010stimulating factors; epidermal growth factor; transforming growth factor\u2010beta; whey\u2010derived growth factor; interleukin\u201011; ATL\u2010104; trefoil factor. keratinocyte growth factor; colony\u2010stimulating factors; epidermal growth factor; transforming growth factor\u2010beta; whey\u2010derived growth factor; interleukin\u201011; ATL\u2010104; trefoil factor. How the intervention might work The growth factors described here are proteins that bind to receptors of target cells and either increase the proliferation of the epithelial cells that form the mucous membrane lining of the oral cavity, or promote the recovery of the white blood cells that contribute to the maintenance of oral health following conventional or high dose chemotherapy (with or without radiotherapy) ( Raber\u2010Durlacher 2013 ). Anti\u2010inflammatory cytokines are also proteins or glycoproteins that bind to receptors of target cells, and are thought to alter the complex balance of pro\u2010 and anti\u2010inflammatory cytokines involved in the pathogenesis of oral mucositis ( Raber\u2010Durlacher 2013 ). Currently, evidence\u2010based guidelines recommend growth factors for the prevention of oral mucositis in patients with haematological cancers undergoing high\u2010dose chemotherapy and total body irradiation prior to haematopoietic stem cell transplantation ( Lalla 2008 ). It has been postulated that tumour cells may also have receptors accommodating cytokines and growth factors, thus encouraging the proliferation of cancer cells in solid tumours ( Lalla 2008 ; von B\u00fcltzingsl\u00f6wen 2006 ). A 2010 systematic review suggested that the risk of acute myeloid leukaemia (AML) or myelodysplastic syndrome (MDS) is increased in people with various cancers receiving chemotherapy with granulocyte colony\u2010stimulating factor (G\u2010CSF) when compared to those receiving chemotherapy without G\u2010CSF ( Lyman 2010 ). The authors concluded that it was not clear whether the increased AML/MDS risk was due to G\u2010CSF or due to the increased chemotherapy dose\u2010intensity in those patients. However, the review also reported a reduction in overall mortality for those receiving G\u2010CSF. Why it is important to do this review This Cochrane Review is part of a series that will replace the previously published Cochrane Review covering all interventions for the prevention of oral mucositis in patients with cancer receiving treatment ( Worthington 2011 ). The Mucositis Study Group (MSG) of the Multinational Association of Supportive Care in Cancer/International Society of Oral Oncology (MASCC/ISOO) is a group that was set up in 1998 for the purpose of producing international evidence\u2010based clinical practice guidelines for managing mucositis (both oral and gastrointestinal), which they first published in 2004, with the latest update published in 2014 ( Lalla 2014 ). In order to facilitate the future updating of Cochrane Reviews on this topic, and also to make them more usable to clinicians, guideline developers, and consumers, we have decided to divide the original Cochrane Review into the same intervention categories as those used by MASCC/ISOO, which are as follows: basic oral care/good clinical practice; growth factors and cytokines; anti\u2010inflammatory agents; antimicrobials, mucosal coating agents, anaesthetics, and analgesics; laser and other light therapy; cryotherapy; natural and miscellaneous agents; amifostine. basic oral care/good clinical practice; growth factors and cytokines; anti\u2010inflammatory agents; antimicrobials, mucosal coating agents, anaesthetics, and analgesics; laser and other light therapy; cryotherapy; natural and miscellaneous agents; amifostine. We believe that following the MASCC/ISOO structure will better enable the Cochrane Reviews to feed into such guidelines. We can also be more thorough and rigorous in our assessment and summarising of the evidence in each of the categories, which was not feasible in a single Cochrane Review approaching 150 included studies. It is also important to do this review as it is consistently shown to be the most used review produced by Cochrane Oral Health (in terms of full\u2010text downloads). It was also ranked by an expert panel of oral medicine specialists as being the most important topic in the field of oral medicine in an international prioritisation exercise carried out by Cochrane Oral Health in 2014 ( Worthington 2015 ).",
        "summary": "People with head and neck cancer due to receive radiotherapy and chemotherapy and those with mixed cancers (sarcoma, metastatic colon or rectal adenocarcinoma) due to receive chemotherapy may benefit from keratinocyte growth factor (KGF) for prevention of oral mucositis. Effects of use of KGF for people receiving bone marrow/stem cell transplantation after conditioning for hematological cancers are less clear. Use of KGF results in lower rates of moderate and severe mucositis in people with head and neck cancer due to receive radiotherapy and chemotherapy (on average, 848 vs 932 per 1000 people had moderate and severe oral mucositis [moderate\u2010quality evidence]; 553 vs 700 per 1000 people had severe mucositis [high\u2010quality evidence]). Rates are also lower in people with mixed cancers (sarcoma, metastatic colon or rectal adenocarcinoma) due to receive chemotherapy (on average, 353 vs 631 per 1000 people had moderate and severe oral mucositis [moderate\u2010quality evidence]; 46 vs 154 per 1000 people had severe mucositis [low\u2010quality evidence]). Results for people receiving bone marrow/stem cell transplantation after conditioning for hematological cancers are less clear: Low\u2010quality evidence suggests that fewer people developed moderate and severe mucositis (on average, 755 vs 848 per 1000 people) but that there was no clear difference between KGF and placebo in rates of severe mucositis. RCTs including up to 476 participants have reported no apparent differences between groups in interruptions to cancer treatment nor in oral pain, although some RCTs included very small sample sizes. Adverse events were poorly reported, but events commonly associated with KGF included skin rash, erythema or exanthema, febrile neutropenia, gastrointestinal disorders, flushing, dysgeusia, and nausea and vomiting."
    },
    "CD003139": {
        "query": "In women with early poor prognosis breast cancer, how does high\u2010dose chemotherapy plus autologous bone marrow or stem cell transplantation compare with conventional chemotherapy?",
        "document": "Background Description of the condition Breast cancer is the most common cancer occurring in women and is the primary cause of cancer deaths among women worldwide ( Ferlay 2015 ; WHO 2000 ). Its incidence is increasing in most countries ( Bray 2004 ). The lifetime risk of a woman developing breast cancer is about one in eight in the United States and one in nine in England and Wales ( ACS 2002 ; DOH 2002 ). In women with early breast cancer, all detectable cancer is restricted to the breast or local lymph nodes ( Clarke 2008 ). Women who have multiple positive lymph nodes when they are first diagnosed are at high risk of recurrent disease. Without adjuvant chemotherapy, the median recurrence rate at five years is over 60% for women with four to nine positive nodes and over 70% for women with more than 10 positive nodes ( Nemoto 1980 ). Established chemotherapy regimens have a high failure rate, improving the chance of 10\u2010year survival by about 11% for women under 50 and about 3% for older women ( Clarke 2008 ). Description of the intervention Researchers from the 1970s onwards described a dose\u2010response relationship in the action of chemotherapy drugs against cancer ( Frei 1980 ); thus it was observed that in the treatment of breast cancer the percentage of women responding to therapy is positively associated with the dose intensity of the drugs received, dose intensity being a function of both the dose and the timing of the chemotherapy regimen ( Hryniuk 1986 ). The technique of autologous bone marrow or peripheral blood stem cell transplant (autograft) was considered an exciting development because it addressed the problem of bone\u2010marrow toxicity which had previously limited the dose of chemotherapy drugs that could be safely given. The use of autograft with chemotherapy permitted the administration of doses many times higher than could otherwise be used. Results of animal studies were encouraging, as were non\u2010randomised patient trials which commenced during the 1980s and which achieved prolonged survival times using high\u2010dose chemotherapy and autograft for women with advanced breast cancer ( Antman 1992 ; Peters 1988 ; Williams 1992 ). Why it is important to do this review Evidence from non\u2010randomised studies has been criticised for participant selection bias and other design weaknesses ( Eddy 1992 ). The first randomised trials began in 1991. Several randomised controlled trials have now been carried out among women with early poor prognosis breast cancer, which is defined as breast cancer that has spread to multiple local lymph nodes without any evidence of distant metastases (spread). The aim of this review was to consider these studies with respect to the relative effectiveness and safety of the treatments they compare.",
        "summary": "In women with early poor prognosis breast cancer, high\u2010dose chemotherapy increases treatment\u2010related mortality and is associated with little or no benefit in survival. There is high\u2010quality evidence that high\u2010dose chemotherapy plus autologous bone marrow or stem cell transplantation does not increase the likelihood of overall survival at any stage of follow\u2010up, compared with standard chemotherapy. High\u2010dose chemotherapy appears to improve the likelihood of event\u2010free survival at three years, but this effect is no longer apparent at longer duration of follow\u2010up. Treatment\u2010related deaths are more frequent with the high\u2010dose regimen and non\u2010fatal morbidity is also more common and more severe in women treated with high\u2010dose chemotherapy, compared with standard chemotherapy. On average, women in the high\u2010dose group report worse quality\u2010of\u2010life scores immediately after treatment, although differences tend to disappear over time."
    },
    "CD010651": {
        "query": "How does surgical management compare with thoracostomy drainage in people with pleural empyema?",
        "document": "Background Description of the condition Empyema refers to pus in the pleural space. The pathogenesis of empyema is not well understood, but it is hypothesised to be caused by transmigration of bacteria into the pleural space, most commonly from adjacent lung infection or pneumonia. Other potential causes include penetrating chest wall injuries and thoracic procedures ( Ferguson 1996 ). Approximately 20% to 57% of people with pneumonia develop a parapneumonic effusion, of whom some can progress to pleural empyema ( Sahn 2007 ). Progression of the condition begins with the exudative stage characterised by a shift of pulmonary interstitial fluid into the pleural space from an increased capillary permeability. The second is the fibropurulent stage, where the pleural space becomes infected and loculation (development of separate cavities) occurs. Lastly, organisation of the chronic inflammation results in proliferation of fibroblasts and a thickening of the pleural space, known as a pleural peel, which can be seen on imaging ( Light 2006 ). Clinically, the three stages can be referred to as uncomplicated parapneumonic effusion (UPPE), complicated parapneumonic effusion (CPPE), and pleural empyema, respectively. Uncomplicated parapneumonic effusion usually resolves with antibiotic therapy alone, but CPPE and empyema require additional interventions. The laboratory diagnosis of CPPE can be made with any one of the following pleural fluid features: positive bacterial studies, glucose level below 60 mg/dL, pH below 7.20, or lactate dehydrogenase (LDH) more than three times the upper normal limit for serum ( Light 2006 ). Medical imaging examination findings of pleural loculations and septations on ultrasound, or pleural thickening and enhancement on computed tomography (CT), can also suggest the diagnosis; however, the definitive diagnosis is a positive culture ( Sahn 2007 ). This review examined the available evidence for the various treatments of CPPE and empyema. We compared outcomes from surgical and non\u2010surgical methods of treatment with the aim of determining what constitutes the optimal strategy for management. Description of the intervention Surgical interventions include video\u2010assisted thoracoscopic surgery (VATS) or open thoracotomy ( Appendix 1 ). Non\u2010surgical management includes thoracentesis and insertion of a chest tube (thoracostomy), with or without the use of intrapleural fibrinolytics ( Appendix 1 ). Descriptions of interventions follow. Video\u2010assisted thoracoscopic surgery (VATS) ( Appendix 1 ) enables visualisation of the pleural cavity for drainage of pus and disruption of septations. A temporary chest tube is left in place for postoperative drainage of any re\u2010accumulated effusions ( Light 2006 ). Open thoracotomy involves surgical exploration of the pleural space and drainage of the empyema ( Light 2006 ). Complications of both VATS and thoracotomy include postoperative pneumothorax, intercostal neuralgia, and associated anaesthetic risks ( Yim 1996 ). Postoperative complications and longer recovery periods are associated with open thoracotomy ( Jaff\u00e9 2003 ), hence VATS is the more commonly performed surgical procedure for empyema. Nonetheless, thoracotomy may be the initial choice in select cases, or alternatively, VATS may be converted to open thoracotomy if necessary ( Lardinois 2005 ). Non\u2010surgical management of empyema includes thoracentesis and chest tube insertion (thoracostomy). Thoracentesis involves aspirating the pleural fluid through a percutaneously inserted catheter, which can be performed under ultrasound or CT guidance. Potential complications from this procedure include haemothorax, pneumothorax, catheter malposition, and bronchopleural fistula ( Jones 2003 ). Chest tube insertion involves dissection of a small area of the chest wall muscle and the placement of a chest tube ( Oddel 1994 ). Duration of treatment is usually no longer than 7 to 10 days or when drainage is minimal, as guided by evidence from clinical tests or radiographic images, or both, of empyema resolution ( Oddel 1994 ). In patients not responding to treatment or who require a prolonged period of chest tube placement, surgical intervention may be considered. Complications associated with tube thoracostomy include chest tube malposition, tissue trauma, and re\u2010expansion pulmonary oedema ( Miller 1987 ). Intrapleural fibrinolysis initially used a combination of streptokinase and streptodornase and is an adjunct to chest tube drainage to facilitate fibrinolysis of loculations ( Tillett 1951 ). Side effects due to impurities led to a decline in its use, but the availability of a more purified form and successful trial of urokinase have led to a re\u2010appraisal of this modality ( Aye 1991 ; Temes 1996 ). More recently, a 2008 Cochrane review concluded that intrapleural fibrinolytic therapy conferred significant benefit in reducing the requirement for surgical interventions ( Cameron 2008 ); however, when subgroup analysis was performed on high\u2010quality trials, the benefit was not significant. How the intervention might work A cornerstone of the management of patients with severe infection is source control. This refers to interventions which aim to control the foci of infections ( Marshall 2009 ). There is a long history of the recognition that draining empyema is beneficial, dating back to the time of Hippocrates ( Cameron 2008 ). However, during the progression of empyema there is a tendency for the formation of loculations (separate cavities) and adhesions which may limit the effectiveness of drainage ( Light 1985 ). It is hypothesised that surgical intervention or the addition of intrapleural fibrinolytics to thoracostomy drainage may reduce the impact of loculations and adhesions on the drainage of empyema. Why it is important to do this review The British Thoracic Society pleural disease guidelines recommend that patients found to have a significant pleural collection associated with pneumonia should have a diagnostic pleural fluid aspirate performed ( Davies 2010 ). Patients found to have CPPE or empyema would then have chest tube drainage along with appropriate antibiotic therapy. Patients who subsequently have persistent sepsis and pleural collection despite chest tube drainage and antibiotics should be referred for surgical management. However, some institutions proceed directly to surgical management if the initial pleural fluid aspirate is thick pus or there are extensive loculations present on imaging. There is currently no clear consensus as to which patients benefit from primary surgical intervention versus non\u2010surgical management. An earlier Cochrane review concluded that there were insufficient large trials to suggest a preference for any particular intervention ( Coote 2005 ). This review aimed to reconcile the issue by comparing the results of surgical and non\u2010surgical therapies for the treatment of CPPE and empyema, thereby providing clinicians with best evidence for management.",
        "summary": "Surgical management appears to be associated with better outcomes than thoracostomy in people with pleural empyema. Moderate\u2010quality evidence showed that in children hospitalized with empyema, open thoracotomy drainage reduced hospital stay (on average, by six days) compared with closed\u2010tube thoracostomy, with fewer procedural complications at up to three months after discharge (on average, 130 vs 600 per 1000 children). These results were derived from a single, underpowered, randomized controlled trial (RCT) with only 30 participants that reported no mortality in either group. For children, adolescents, and adults hospitalized with pleural empyema, moderate\u2010quality evidence shows that video\u2010assisted thoracoscopic surgery (VATS) decreases length of hospital stay (on average, by three days), and procedural complications at up to six months after discharge (on average, 11 vs 23 per 1000 people) compared with thoracostomy. RCTs included more people in this comparison (around 250 participants), but event rates were low and analyses may still be underpowered. Of note, subgroup analysis showed no difference in length of hospital stay among children when a small trial in 20 adults was removed from the analysis, highlighting the need for caution when interpreting results from small analyses."
    },
    "CD010791": {
        "query": "What are the effects of medication reconciliation for improving transitions of care?",
        "document": "Background Errors in the prescribing and administration of medication are frequent, costly and harmful ( Bates 2007 ). More than 40% of medication errors result from inadequate medication reconciliation at care transitions ( Hughes 2008 ). Transitional care provides for the continuity of care as patients move between different stages and settings of care ( Coleman 2004 ). The prevalence of medication discrepancies arising at care transitions have been reported in many different settings (hospital, community and long\u2010term care facilities) and stages of care (admission, transfer and discharge); in particular, transitioning between an inpatient and outpatient setting is associated with an increase in medication errors relative to other stages of care ( Boockvar 2006 ; Coleman 2004 ; Moore 2003 ; Tam 2005 ). Prevalence of adverse events post hospitalisation as high as 19% have been reported with the majority of these related to adverse drug events (ADEs), which may be the result of medication error ( Forster 2003 ). \"Medication reconciliation is a conscientious, patient centred, inter\u2010professional process that supports optimal medicines management\" ( Greenwald 2010 ). The process aims to create the most accurate list of medications at all transition points, with the goal of providing the correct medications to the patient ( Karapinar 2011 ). Different patient groups and locations have been studied. A variety of intervention types have been investigated, including information technology ( Kramer 2007 ; Schnipper 2009 ), pharmacist\u2010led ( Gillespie 2009 ), and more complex multifaceted interventions ( Koehler 2009 ). The benefits of medication reconciliation interventions are often assessed by comparing medication regimens across transitions and reporting discrepancy reduction as the primary outcome. A previous systematic review reported that although unintended medication discrepancies were common, clinically significant discrepancies may affect only a few patients ( Kwan 2013 ). Challenges arise in identifying those discrepancies that are considered clinically significant and which may give rise to patient harm. Therefore, despite reconciliation being recognised as a key aspect of patient safety, there remains a lack of consensus and evidence as to the most effective methods of implementing reconciliation and calls have been made to strengthen the evidence base prior to widespread adoption ( Greenwald 2010 ). Description of the condition Transitional care describes the care provided to patients to ensure the co\u2010ordination and continuity of healthcare as they transfer between different settings or different stages of care (or both) within the same settings ( Coleman 2003a ). Improved continuity of prescribed medication via medication reconciliation for patients at care transitions is recommended by national standard setting bodies and internationally led initiatives (e.g. World Health Organization's (WHO) High 5s project ( IHI 2011 ; NICE 2007 ; WHO 2006 ). However, the effectiveness, and most effective method of conducting reconciliation, remains unclear. Description of the intervention Medication reconciliation consists of the following three steps ( IHI 2011 ). Verification: a current medication list is developed using one or more sources of information (e.g. general practitioner medical records, patient's own supply, pharmacy records). Clarification: medication and dosages are checked for appropriateness. Here appropriateness means ensuring that there are no unintentional changes, rather than a medication review leading to optimal medication appropriateness). Reconciliation: newly prescribed medications are compared to old and any changes made are documented. Verification: a current medication list is developed using one or more sources of information (e.g. general practitioner medical records, patient's own supply, pharmacy records). Clarification: medication and dosages are checked for appropriateness. Here appropriateness means ensuring that there are no unintentional changes, rather than a medication review leading to optimal medication appropriateness). Reconciliation: newly prescribed medications are compared to old and any changes made are documented. How the intervention might work Failure to reconcile medications can result in medication error and subsequent ADEs ( IHI 2011 ). Interventions to improve medication reconciliation may work by improving the communication between all those involved in the medication\u2010use process (dispensing, administration, monitoring across settings and stages of care), including the patient. Additionally, these interventions may well help in reducing transcribing errors, improved monitoring of prescriptions, information technology systems and reorganisation of care delivery. Why it is important to do this review Medication reconciliation is incorporated into the National Patient Safety Goals of the Joint Commission under the umbrella of improving the safety of using medications ( The Joint Commission 2013 ). The National Institute for Health and Care Excellence (NICE) in collaboration with the National Patient Safety Agency in the UK encouraged the standardisation of reconciliation processes within healthcare organisations ( NICE 2007 ). The Canadian Patient Safety Institute and the Institute for Safe Medication Practices (Canada) have advocated for medication reconciliation and the WHO launched the High 5s project, focusing on care transitions, as well as the 3rd Global Patient Safety Challenge: Medication without Harm in 2017 ( Donaldson 2017 ). The findings of this proposed review are relevant at both a national and international level. Regulatory bodies, healthcare institutions, patient safety advocates, healthcare practitioners and the wider public would be receptive audiences for the findings from a systematic review of the most effective method of medicines reconciliation.",
        "summary": "For people transitioning between settings (home, secondary care hospitals, outpatient or primary care clinics, and/or long\u2010term care), medication reconciliation (creating the most accurate list possible of medications a patient is taking and comparing against the physician's admission, transfer, and/or discharge orders, with the goal of providing correct medications to the patient at all transition points) may reduce the number of people who have a medication discrepancy (299 vs 563 per 1000 people; approximately one less medication discrepancy per person; all results on average), but the evidence is of very low certainty. Evidence shows that medication reconciliation may have little or no impact on mortality (low\u2010certainty evidence) or on the number of unplanned rehospitalizations (moderate\u2010certainty evidence), as event rates are low in both groups. Low\u2010certainty evidence suggests little to no difference in the incidence of adverse drug events, but the result for change in preventable adverse drug events was imprecise, making the effect very uncertain. It is worth noting that most trials were conducted in people who were being admitted to secondary care hospitals."
    },
    "CD011661": {
        "query": "Is there randomized controlled trial evidence to support the use of etrolizumab for induction of remission in people with ulcerative colitis?",
        "document": "Background Description of the condition Ulcerative colitis (UC) is a chronic inflammatory disease of unknown etiology characterized by bloody diarrhoea, abdominal pain, and fecal urgency. Worldwide incidence rates of UC range between 1.25 to 20.3 new cases per 100,000 persons per year, with approximately 10 to 12 new cases per 100,000 persons per year in North America and Europe ( Danese 2011 ; Fedorak 2010 ; Molodecky 2012 ). In the United States it is estimated that the direct and indirect costs associated with the disease range between 8.1 billion USD and 14.9 billion USD per annum ( Cohen 2010 ). Evidence suggests that UC is caused by an inappropriate immune response that is triggered by a combination of environmental, genetic and immunological factors ( Bouma 2003 ). UC is treated with broad\u2010spectrum anti\u2010inflammatory drugs including corticosteroids, 5\u2010aminosalicylic acid (5\u2010ASA) products, immunosuppressive therapies (e.g. azathioprine, 6\u2010mercaptopurine and methotrexate) and biologics such as tumor necrosis factor alpha (TNF\u2010\u03b1) antagonists (e.g. infliximab (Remicade\u00ae), adalimumab (Humira\u00ae), certolizumab pegol (Cimzia\u00ae) and golimumab (Simponi\u00ae)) and alpha4beta7 (\u03b14\u03b27) inhibitors (e.g. vedolizumab) ( Bickston 2014 ; Feagan 2012a ; Feagan 2012b ; Ford 2011a ; Ford 2011b ; Kornbluth 2010 ; Lawson 2006 ; Timmer 2012 ). These medications are effective to varying degrees, however patients often do not respond, become corticosteroid dependent, fail therapy or experience significant drug\u2010related adverse events ( Faubion 2001 ; Gisbert 2015 ). Aminosalicylates are effective for mild to moderate disease ( Feagan 2012a ; Feagan 2012b ; Ford 2011a ), while corticosteroids are often required for those who fail to respond to 5\u2010ASAs ( Ford 2011b ; Turner 2007 ). Corticosteroids are highly effective for induction of remission, but are not useful for maintenance of remission and carry significant adverse effects, including osteoporosis, glucose intolerance, and increased risk of infection ( Bjarnason 1997 ; Dignass 2010 ; Lichtenstein 2006 ). Immunosuppressives, including 6\u2010mercaptopurine and azathioprine, play a limited role in maintenance of remission in UC ( Podolsky 2002 ; Timmer 2012 ). Furthermore, these drugs may increase the risk of lymphoma and non\u2010melanoma skin cancer in people with inflammatory bowel disease (IBD) ( Ariyaratnam 2014 ; Smith 2010 ). TNF\u2010\u03b1 antagonists are useful for both induction and maintenance of remission in UC ( Reinisch 2011 ; Sandborn 2012 ; Sandborn 2014 ), however their use has been associated with a number of serious adverse events involving both hypersensitivity and opportunistic infection ( Ford 2013 ). Patients who fail therapy, develop toxic megacolon or have severe attacks of ulcerative colitis require colectomy, which frequently results in post\u2010operative complications including infection, pouchitis, fistula formation, and bowel obstruction ( Loftus 2008 ). New pharmaceutical agents, particularly those specific to the intestinal tract, may be more effective than conventional therapies and reduce the need for surgery. Description of the intervention Anti\u2010adhesion molecules such as natalizumab and vedolizumab represent a novel biologic option for the treatment of UC. Natalizumab was shown to be effective in patients with Crohn's disease, however its use has been associated with immunosuppression of the central nervous system and the development of progressive multifocal leukoencephalopathy (PML) ( Van Assche 2005 ). Conversely, vedolizumab has proven to be effective for induction and maintenance of remission in UC and well\u2010tolerated by patients ( Bickston 2014 ; Feagan 2013 ). While natalizumab regulates leukocyte trafficking by blocking both the alpha4beta1 (\u03b14\u03b21) and alpha4beta7 (\u03b14\u03b27) integrins, vedolizumab selectively targets the latter, thereby exclusively inhibiting T\u2010cell homing to the gut. Etrolizumab is a novel anti\u2010integrin that selectively targets the \u03b27 subunits of the \u03b14\u03b27 and \u03b1E\u03b27 integrins that regulate trafficking and retention of T\u2010cell subset lymphocytes in the intestinal mucosa. How the intervention might work Etrolizumab binds with high affinity to \u03b14\u03b27 ( Holzmann 1989 ; Hu 1992 ), and \u03b1E\u03b27 ( Cepek 1993 ). By this mechanism, it blocks the homing and retention of leukocyte subpopulations in the intestinal mucosa, which occur via binding with the cell adhesion molecules (MAdCAM\u20101) and E\u2010cadherin, respectively. Since these cell adhesion molecules are found mostly in the intestinal mucosa, etrolizumab is felt to be gut specific. In a mouse model, etrolizumab selectively blocks lymphocyte homing to the gastrointestinal tract, with no apparent effect on lymphocyte trafficking to the central nervous system or non\u2010mucosal tissues ( Stefanich 2011 ). Why it is important to do this review There is a need for targeted UC treatment options capable of achieving and sustaining remission, decreasing steroid dependence and maintaining immunocompetence while simultaneously avoiding the risk of serious adverse events. The aim of this systematic review is to summarize the currently available evidence regarding the efficacy and safety of etrolizumab for induction of clinical remission and response in patients with moderate to severe UC.",
        "summary": "Etrolizumab is a monoclonal antibody against subunits of integrin. It may help to induce remission in people with ulcerative colitis who are refractory to established therapy, but data on safety are sparse. Moderate\u2010quality evidence from one randomized controlled trial (RCT) with 119 participants showed that etrolizumab therapy led to clinical remission after 10 weeks more often than placebo (in 140 per 1000 people compared with none per 1000 people). The same RCT showed similar clinical response and endoscopic remission in both groups. Moderate\u2010quality evidence from one RCT showed that fewer people treated with etrolizumab than with placebo experienced adverse effects, but no evidence of a difference regarding serious adverse effects was found. None of the RCTs assessed quality of life."
    },
    "CD003264-0": {
        "query": "How do different doses of recombinant human growth hormone compare in children with chronic kidney disease?",
        "document": "Background Chronic kidney disease (CKD) is an uncommon but important condition. As defined by the need for dialysis or kidney transplant (stage 5 CKD; K/DOQI 2002 ), 32/1,000,000 children under the age of 15 have CKD ( Disney 1999 ). Many more children have less advanced CKD (stage 3 CKD with glomerular filtration rate (GFR) 30 to 59 mL/min/1.73 m\u00b2 and stage 4 CKD with GFR 15 to 29 mL/min/1.73 m\u00b2). The frequency of this problem is unclear. Growth retardation, one of the complications of CKD, is of concern to families in over 90% of children with stage 5 CKD ( Reynolds 1995 ). Approximately 60% of boys and 41% of girls who started renal replacement therapy before the age of 15 years attain a final adult height more than two standard deviations (12 cm) below the mean for healthy adults ( Rizzoni 1985 ). Impairment of growth can begin when the GFR falls to 50% of normal, and becomes an increasing problem once the GFR falls below 25% ( T\u00f6nshoff 1995 ). Over the past 20 years, recombinant human growth hormone (rhGH) treatment has been used to help short children with CKD attain an adult height more in keeping with their age group ( Berard 2008 ; Fine 1995a ; Haffner 2000 ). However, there have been concerns that rhGH may have an adverse effect on the preservation of native kidney function, predispose to acute rejection in kidney transplant recipients, and cause benign intracranial hypertension (BIH) and slipped capital femoral epiphysis ( Clayton 2000 ). Although several studies of rhGH treatment in children with CKD have been undertaken, uncertainty exists on the magnitude of benefits and side effects of the treatment ( Fine 1995a ). A systematic review of rhGH treatment was undertaken to evaluate growth outcomes to establish the effect of treatment over time. We sought to establish if the growth outcomes remained linear over time or if there was a waning effect of the treatment. Secondly, we examined the effect of varying doses of the treatment. Thirdly, we attempted to explore the effect of the following factors on treatment: age, sex, pubertal status and the stage of CKD (pre\u2010dialysis, on dialysis, post\u2010transplant). Finally, the study evaluated potential side effects of rhGH treatment.",
        "summary": "When 14 IU/m\u00b2/week recombinant human growth hormone (rhGH) was compared with 28 IU/m\u00b2/week in children with chronic kidney disease, evidence suggests that children had a better height velocity (on average by 1.2 cm/year; moderate\u2010quality evidence) and height velocity standard deviation score (SDS, on average by 1.48; low\u2010quality evidence) after 1 year with 28 IU/m\u00b2/week. Moderate\u2010quality evidence showed no apparent difference between groups in terms of height SDS or kidney function after 1 year of treatment. Adverse effects were poorly reported across the trials and trials could not be combined in a meta\u2010analysis. All of the analyses were very small (80 to 208 children) and therefore too underpowered to reliably detect differences between groups. One very small trial (16 children) compared a high dose (56 IU/m\u00b2/week) with 28 IU/m\u00b2/week. The trial was far too small to provide clinically useful results, and the high dose is not used in clinical practice."
    },
    "CD007026": {
        "query": "What are the effects of cerebrolysin in people with acute ischemic stroke?",
        "document": "Background Effective, simple, and reliable treatment methods are urgently needed to reduce stroke mortality and disability. Many clinical trials and Cochrane Reviews have addressed the question of benefits and risks of potential pharmacological treatment options for acute ischaemic stroke. However, strategies with proven therapeutic effects and an acceptable benefit\u2010to\u2010risk ratio are still lacking. Potential strategies can be grouped according to the existing evidence of their benefits and harms determining their role in clinical practice. Evidence of benefit Aspirin at a dose of 160 mg to 300 mg daily (orally or per rectum), started within 48 hours of onset of presumed ischaemic stroke appears to be the only effective treatment for early secondary prevention, reducing the risk of early recurrent ischaemic stroke without a major risk of early haemorrhagic complications, and improving long\u2010term outcomes ( Sandercock 2014 ). Despite the positive overall conclusions of a Cochrane Review ( Wardlaw 2014 ) and individual patient data meta\u2010analysis ( Emberson 2014 ) of thrombolysis in acute ischaemic stroke, there is still some debate regarding the optimal use of intravenous recombinant tissue plasminogen activators (rtPA) ( Alper 2015 ). It is estimated that for each person with a good stroke outcome at six months, another person would have symptomatic intracranial bleeding, and for every three to four people without neurological deficits at six months, there is an excess of one death after thrombolysis ( Appelros 2015 ; Brunstr\u00f6m 2015 ). The evidence is inadequate to conclude whether lower doses of thrombolytic agents are more effective than higher doses, whether one agent is better than another, or which route of administration is the best for treatment of people who have had an acute ischaemic stroke ( Wardlaw 2013 ). Evidence of harm Glycoprotein IIb\u2010IIIa inhibitors (abciximab and tirofiban) increase the risk of intracranial haemorrhage without evidence of any reduction in death or disability in stroke survivors ( Ciccone 2014 ). These data do not support their routine use in clinical practice. Abciximab contributed 89% of the total number of participants of the Cochrane Review ( Ciccone 2014 ). Anticoagulants (standard unfractionated heparin, low\u2010molecular\u2010weight heparins, heparinoids, oral anticoagulants, and thrombin inhibitors) as immediate therapy for acute ischaemic stroke are not associated with net short\u2010 or long\u2010term benefit. Reduced rate of recurrent stroke, deep vein thrombosis, and pulmonary embolism with anticoagulant therapy was offset by the increased risk of intracranial haemorrhage and extracranial bleeding. The data do not support the routine use of any of the currently available anticoagulants in acute ischaemic stroke ( Berge 2002 ; Sandercock 2008a ; Sandercock 2008b ). Long\u2010term anticoagulant therapy in people with presumed non\u2010cardioembolic ischaemic stroke or transient ischaemic attack was not associated with any benefit, but there was a significant bleeding risk ( Sandercock 2009 ). Tirilazad, an amino steroid inhibitor of lipid peroxidation, increased the combined end\u2010point of 'death or disability' in people with acute ischaemic stroke ( TISC 2001 ). Lubeluzole, an ion channel modulator of glutamate release that has a benzothiazole structure with potential neuroprotective properties, did not reduce death or dependency in acute ischaemic stroke patients. In contrast, it increased heart\u2010conduction disorders (Q\u2010T prolongation) ( Gandolfo 2002 ). Evidence of lack of benefit The evidence of the lack of benefit has accumulated for the following treatment options, which were tested in clinical trials and the results of which were systematically reviewed: corticosteroids ( Sandercock 2011 ); calcium antagonists ( Horn 2000 ); haemodilution ( Chang 2014 ); excitatory amino acid antagonists, including ion channel modulators and N\u2010methyl\u2010D\u2010aspartic acid (NMDA) antagonists ( Muir 2003 ); piracetam ( Ricci 2012a ); and a free radical trapping agent NXY\u2010059 ( Shuaib 2007 ). There is no evidence that colloids lead to lower odds of death or dependence after stroke compared with crystalloids ( Visvanathan 2015 ). Role in clinical practice There is still inadequate evidence from RCTs for the following antithrombotic agents: oral antiplatelet drugs other than aspirin (clopidogrel, ticlopidine, cilostazol, satigrel, sarpolgrelate, KBT 3022, iisbogrel) ( Sandercock 2014 ); and fibrinogen\u2010depleting agents (ancrod and defibrase) ( Hao 2012 ). The list of interventions of agents tested in clinical trials with subsequent Cochrane Reviews of results that documented inadequate evidence to establish a role in clinical practice includes: ginkgo biloba ( Zeng 2005 ); gamma aminobutyric acid (GABA) receptor agonists ( Liu 2016 ); percutaneous vascular interventions, including intra\u2010arterial thrombolysis with urokinase and pro\u2010urokinase ( O'Rourke 2010 ); sonothrombolysis ( Ricci 2012b ); glycerol ( Righetti 2004 ); mannitol ( Bereczki 2007 ); naftidrofuryl, a 5\u2010HT2 serotonergic antagonist ( Leonardi\u2010Bee 2007 ); theophylline or methylxanthine derivatives ( Bath 2004a ; Bath 2004b ); nitric oxide donors ( Bath 2002 ); blood pressure\u2010altering interventions ( BASC 2000 ; BASC 2001 ; Bath 2014 ); prostacyclin and its analogues ( Bath 2004c ); vinpocetine ( Bereczki 2008 ); gangliosides ( Candelise 2001 ); colony stimulating factors ( Bath 2013 ); or stem cells ( Boncoraglio 2010 ); Chinese herbal medicine Sanchi ( Chen 2008 ), puerarin ( Tan 2008 ), mailuoning ( Yang 2009 ), tongxinluo capsules ( Zhuo 2008 ); and the neuroprotective agent edaravone ( Feng 2011 ), which are widely used for ischaemic stroke in China. Cerebrolysin belongs to this category ( Ziganshina 2015 ). Description of the condition Ischaemic stroke occurs when the brain loses its blood and energy supply, resulting in damage to brain tissue; it is a brain equivalent of a heart attack. Most strokes (87%) are ischaemic ( AHA 2014 ). Worldwide every year 15 million people suffer a stroke: five and a half million people die and another five million are left permanently disabled, placing a burden on family and community ( WHO 2014 ). Stroke is one of the major causes of disability and mortality ( AHA 2014 ; Bonita 1992 ; WHO 2014 ). It is the third most common cause of death in the developed world after coronary disease and cancer. The World Health Organization (WHO) stroke statistics registered the number of deaths from stroke to be more than 200,000 in the Russian Federation, as well as in China and in India, with the highest number of 1,652,885 in China and 517,424 in Russia in 2002 ( WHO 2014 ). According to the Russian data there are between 400,000 to 450,000 cases of acute stroke registered in the Russian Federation annually ( Gusev 2003 ) with the incidence of 3.36 per 1000 population and standardised incidence of 2.39 (3.24 in men and 2.24 in women) per 1000 population ( Gusev 2013 ). The case fatality rate of stroke is 40.37% (61.4% for haemorrhagic stroke and 21.8% for ischaemic stroke). The north\u2010west regions had the highest stroke incidence of 7.43 per 1000, followed by some cities in middle areas of the country (5.37 per 1000) and the far east (4.41 per 1000) ( Gusev 2003 ; Vilenski\u012d 2006b ). The stroke recurrence rate is 30% ( Suslina 2009 ). Stroke survivors experience serious neurological disorders (loss of vision, speech or both; paralysis; and confusion) and these are not restored in 30% to 66% of cases six months after a stroke ( French 2007 ). In Russia, stroke is the number one cause of disability in adults: 32 cases per 100,000 population. By the end of one year 25% to 30% of stroke survivors develop dementia. Stroke presents a huge financial burden for the health system ( Martynchik 2013 ). Description of the intervention Cerebrolysin is a mixture of low\u2010molecular\u2010weight peptides and amino acids derived from pigs' brain tissue, which has potential neuroprotective and neurotrophic properties. Its manufacturer promotes it for multiple neurological conditions, and it is widely used in the treatment of acute ischaemic stroke in Russia, China, and other Asian and post\u2010Soviet countries. How the intervention might work The term 'neuroprotection' is used to describe the putative effect of interventions protecting the brain from pathological damage. In ischaemic stroke the concept of neuroprotection includes inhibition of pathological molecular events leading to calcium influx, activation of free radical reactions and cell death. Knowledge of pathophysiology in acute ischaemic stroke stimulated development of a number of potential neuroprotective agents. Many neuroprotective agents have proven to be efficacious in animal studies. Cerebrolysin is a mixture of low\u2010molecular\u2010weight peptides (80%) and free amino acids (20%) derived from pig brain tissue, with proposed neuroprotective and neurotrophic properties similar to naturally occurring growth factors (nerve growth factor, brain\u2010derived neurotrophic factor) ( Alvarez 2000 ; Fragoso 2002 ). Results of in vitro and animal studies of cerebrolysin have been traditionally used to suggest its potential for treating acute ischaemic neuronal damage ( Masliah 2012 ). For example, cerebrolysin was shown to be effective in tissue culture models of neuronal ischaemia dose\u2010dependently increasing neuronal survival ( Schauer 2006 ). In brain slices it counteracted necrotic and apoptotic cell death induced by glutamate ( Riley 2006 ). Cerebrolysin also demonstrated neuroprotective activity in a rat model of haemorrhagic stroke ( Makarenko 2005 ) and ischaemic stroke ( Zhang 2010 ), as well as spinal cord trauma ( Sapronov 2005 ). One randomised double blind placebo\u2010controlled trial showed no effect of cerebrolysin in acute haemorrhagic stroke on chosen efficacy measures (Barthel Index (BI), Unified Neurological Stroke Scale, and Syndrome Short Test (SST)) ( Bajenaru 2010 ). Why it is important to do this review Despite the effectiveness of neuroprotective agents in animal models of stroke, clinical trials of neuroprotective agents in humans have provided disappointing results ( European Ad Hoc Consensus 1998 ). More recent Cochrane Reviews of the effects of individual neuroprotective agents and pharmacological groups confirmed this ( Gandolfo 2002 ; Muir 2003 ; Ricci 2012a ; TISC 2001 ). Other means of neuroprotection are being sought. Cerebrolysin is well accepted by Russian and Asian physicians. It is widely used in the treatment of acute ischaemic stroke and other neurological disorders ( Chukanova 2005 ; Gromova 2006 ; Onishchenko 2006 ). Research data from observational studies and clinical trials of cerebrolysin in acute stroke or head injury, with most performed in Russia and China, have accumulated ( Chukanova 2005 ; Gafurov 2004 ; Gromova 2006 ; Ladurner 2005 ; Skvortsova 2004 ; Wong 2005 ). We carried out a Cochrane Systematic Review, which did not find sufficient evidence to support cerebrolysin use in practice ( Ziganshina 2010a ). Cerebrolysin, as assessed in a Cochrane Systematic Review for vascular dementia, may have positive effects on cognitive function and global function in elderly people with mild to moderate dementia, but the review authors do not recommend it for routine use in vascular dementia due to the limitations of the studies and the resulting review: small number of included trials, wide variety of treatment durations, and short\u2010term follow\u2010up ( Chen 2013b ). Cerebrolysin has also been proposed for treatment of people with Alzheimer's disease ( Fragoso 2002 ). Trials of cerebrolysin in acute haemorrhagic stroke have been assessed in a meta\u2010analysis ( Shu 2012 ), concluding on its safety and supporting implementation of new trials for definitive efficacy assessment. The previous versions of this Cochrane Review, based on one eligible trial only, did not find evidence of cerebrolysin benefit in acute ischaemic stroke ( Ziganshina 2010a ; Ziganshina 2015 ). More research data from clinical trials of cerebrolysin in acute ischaemic stroke have accumulated with cerebrolysin used for varying periods of time, majority of them for less than 14 days, as specified by the protocol of the review, and at various doses (10 mL, 30 mL, and 50 mL). We decided to use a more inclusive approach and to refine our inclusion criteria to allow inclusion of the accumulated research data. The aim of this Cochrane Review update is to establish whether the available evidence from controlled trials indicates if cerebrolysin is beneficial and safe for the treatment of acute ischaemic stroke.",
        "summary": "When cerebrolysin (usually 30 mL diluted to 100 mL volume daily for 10 days; range 10 to 50 mL for 10 to 21 days) started within 12 to 24 hours of ischemic stroke onset was compared with placebo in adults with a mean age of 60 to 69 years, moderate\u2010quality evidence showed no evidence of a reduction in all\u2010cause mortality at 10 to 21 days. However, moderate\u2010quality evidence showed an increase in the overall incidence of non\u2010fatal serious adverse events with cerebrolysin (on average, 30 vs 12 per 1000 people) but no apparent differences between groups in fatal serious adverse events, or when minor adverse events were included to give an overall incidence. Studies did not report om the combined outcome death and dependency or on quality of life."
    },
    "CD006432": {
        "query": "In people who have had a stroke, what are the effects of simultaneous bilateral training for improving arm function?",
        "document": "Background Stroke is the main cause of permanent and complex long\u2010term disability in adults and has implications for patients, caregivers, health professionals and society in general ( Feigin 2003 ; Kwon 2004 ; Langhorne 2003 ; van der Lee 1999 ). At present there is no routinely available curative treatment for stroke patients and therefore rehabilitation interventions are relied upon to maximise patient outcomes ( Langhorne 2003 ). Upper limb (arm) hemiparesis is widely reported in the literature as one of the primary impairments following stroke ( Johnson 2001 ; Page 2002 ; van der Lee 2001 ). While many patients recover ambulatory function after dense hemiplegia, restoration of arm motor skills is often incomplete ( Johnson 2001 ; Page 2001 ). It has been reported that the paretic arm remains without function in between 30% ( Heller 1987 ) to 66% ( Sunderland 1989 ; Wade 1983 ) of hemiplegic stroke patients, when measured six months post\u2010stroke. Furthermore, only 5% ( Heller 1987 ) to 20% ( Nakayama 1995 ) of individuals achieve complete functional recovery. Nevertheless, return of voluntary arm movements is one of the most important goals during stroke rehabilitation in order to avoid long\u2010term disability in activities of daily living (ADL), social and occupational activities, and depression ( Broeks 1999 ). The aim of rehabilitation is to reduce impairment and minimise disability ( Page 2001 ) and a number of interventions to achieve these aims and improve arm function after stroke have been suggested ( Barreca 2003 ; van der Lee 2001 ). The effectiveness of some of these interventions has been, or is in the process of being reviewed within other Cochrane systematic reviews: electromyographic (EMG) biofeedback ( Woodford 2004 ), electrostimulation ( Pomeroy 2006 ), electromechanical and robotic\u2010assisted arm training ( Merholz 2008 ), constraint\u2010induced movement therapy ( Sirtori 2003 ) and repetitive task training ( French 2006 ). However, rigorous systematic evaluation is still required to investigate the effectiveness of simultaneous bilateral training. Simultaneous bilateral training involves the execution of identical activities with both arms simultaneously but independently ( Mudie 2000 ). Beneficial effects of bilateral training are assumed to arise from an interlimb coupling effect, in which movement of the non\u2010paretic arm facilitates movements in the impaired limb ( Kelso 1979 ; Morris 2008 ; Swinnen 2002 ). Cauraugh 2008 and Stinear 2008 further suggest that bilateral practice of synchronous movements with the paretic and non\u2010paretic limbs allows activation of the intact hemisphere to facilitate activation of the damaged hemisphere through enhanced interhemispheric inhibition. Bilateral training is often combined with other interventions, such as electrostimulation or assistive technology, to assist the affected arm to undertake the simultaneous movements. Two reviews ( Cauraugh 2005 ; Stewart 2006 ) report favourable effects of bilateral training. These reviews, however, included studies other than randomised controlled trials (RCTs) and both acknowledge that there are inconsistent findings across bilateral movement studies. A further, more recent narrative review of bilateral training ( McCombe Waller 2008 ) acknowledges that bilateral studies have not shown improvements in all patients and that bilateral training has not been shown to be more beneficial than other training approaches. However this review was not systematic and included a range of study designs, including single case studies. We therefore sought to undertake a complete, up\u2010to\u2010date, systematic review of randomised controlled trials to determine the effects of bilateral training compared to no treatment, placebo or other interventions for improving arm function after stroke.",
        "summary": "Based on available evidence, simultaneous bilateral upper limb training does not seem effective for stroke survivors but the strength of evidence on which this conclusion is made is very limited. Randomized controlled trials including around 100 participants showed that there may be a moderate treatment effect in favor of bilateral training, in terms of motor impairments, compared with usual care. However, this did not translate into improved scores on upper limb functional scales or activities of daily living scales. Most analyses were underpowered and had substantial heterogeneity. People having simultaneous bilateral training had better scores on extended activities of daily living that people having unilateral training. There were no clear differences between groups in any other outcomes but this may have been because of lack of power in the analyses. Adverse event reporting was poor across all the included studies."
    },
    "CD004905": {
        "query": "How does multiple\u2010micronutrient supplementation for pregnant women affect infant outcomes?",
        "document": "Background Description of the condition Micronutrient deficiencies are common among women of reproductive age (15 to 49 years of age), especially those residing in low\u2010 and middle\u2010income countries where diets often lack diversity and fortified foods are less available ( Black 2013 ; FAO/WHO 2004 ). Infections and chronic illness can also contribute to micronutrient deficiencies by directly inhibiting nutrient absorption ( Bailey 2015 ). Micronutrient deficiencies are exacerbated during pregnancy due to increased requirements of the growing fetus, placenta and maternal tissues. An inability to fulfil the increased demands results in potentially adverse effects on the mother and the fetus ( Berti 2011 ). Additionally, there can be sustained intergenerational effects. Maternal malnutrition has been shown to affect both short\u2010term and long\u2010term outcomes for the offspring, including growth, neurodevelopment and cognition, and cardiometabolic, pulmonary, and immune function ( Gernand 2016 ). Up\u2010to\u2010date population\u2010level estimates are largely lacking for individual micronutrients due to measurement and cost challenges associated with collecting these indicators ( Gernand 2016 ). In addition, there are few data that have been disaggregated by age, parity, wealth status and other factors that can influence nutrition throughout pregnancy ( Gernand 2016 ). However, we do know that anaemia due to iron deficiency is one of the most prevalent micronutrient deficiencies globally. According to 2013 estimates, the worldwide prevalence of prenatal iron\u2010deficiency anaemia was 19.2% (95% confidence interval (CI) 17.1% to 21.5%; Black 2013 ). Anaemia during pregnancy has been found to be associated with increased risk of maternal mortality, perinatal mortality, and infants with low birthweight (LBW) ( Allen 2001 ; Christian 2010 ; Haider 2013 ; Murray\u2010Kolb 2013 ). Vitamin A is another important nutrient that, when deficient, can lead to night blindness. According to global estimates for the time period between 1995 and 2005, vitamin A deficiency, measured using night blindness and low serum retinol levels, affected 9.8 million (95% CI 8.7 to 10.8 million), and 19.1 million pregnant women (95% CI 9.30 to 29.0 million), respectively. This corresponds to 15.3% (95% CI 6.0% to 24.6%), of pregnant women being deficient in vitamin A ( Black 2013 ). Deficiency of vitamin A has been associated with poor birth and mortality outcomes; however, supplementation with vitamin A during pregnancy has demonstrated no beneficial effect on these outcomes ( McCauley 2015 ), but has been shown to reduce the risk of maternal anaemia, infection, and night blindness ( McCauley 2015 ). In the past decade, deficiency of vitamin D has also emerged as an important nutritional problem with high prevalence being reported in high\u2010income, as well as low\u2010income, populations ( Datta 2002 ; Ginde 2010 ; Sachan 2005 ). Iodine deficiency, often measured by urinary iodine, is also common among pregnant women. The median urinary iodine level in a nationally representative sample of pregnant women in Nepal was reported to be 134 mcg/L ( Benoist 2008 ), indicating insufficient iodine intake ( Andersson 2007 ). Severe iodine deficiency during pregnancy results in pregnancy loss, mental retardation and cretinism ( Dunn 1993 ). Although severe deficiency is now rare, mild to moderate deficiency continues to be a problem ( Andersson 2007 ). Deficiencies of other micronutrients are also common among pregnant women. According to the 2012 estimates, around 17% of the world\u2019s population have reduced dietary intake of zinc ( Wessells 2012 ). Zinc deficiency has been associated with complications of pregnancy and delivery such as pre\u2010eclampsia, premature rupture of membranes, and congenital abnormalities ( Black 2001 ; Caulfield 1998 ). However, a review of trials of zinc supplementation showed a reduction in the risk of preterm birth only ( Hess 2009 ; Ota 2015 ). Folic acid deficiency can lead to haematological consequences and congenital malformations; however, association with other birth outcomes is equivocal ( Black 2001 ; De\u2010Regil 2010 ). Concurrent deficiencies that could include vitamins A, D, E, riboflavin, B6, B12, folic acid, iron, and zinc have also been reported in studies conducted among pregnant women ( Jiang 2005 ; Pathak 2004 ). Deficiencies in other minerals such as magnesium, selenium, copper and calcium may also potentially be associated with complications of pregnancy, childbirth or fetal development ( Black 2001 ). Description of the intervention The World Health Organization (WHO) currently recommends iron and folic acid supplementation for women during pregnancy as part of routine antenatal care ( WHO 2012 ). The recommended dose of iron ranges from 30 mg to 60 mg. In areas where anaemia is a severe public health problem, defined as a prevalence of 40% or higher, a daily dose of 60 mg of iron is preferred. The standard dose of 60 mg of iron was first recommended in 1959 and was based on maternal requirements during pregnancy ( WHO 1959 ). Despite its provision as part of national antenatal care programmes for the last few decades in most low\u2010 and middle\u2010income countries, compliance with the supplement is low. Gastrointestinal side\u2010effects including constipation, nausea, vomiting, and diarrhoea are the most common complaints among women consuming high doses of iron ( Oriji 2011 ; Seck 2008 ). Supplementation with iron and folic acid during pregnancy has been found to be associated with reduction in the risk of maternal anaemia and infants with LBW ( Haider 2013 ; Pena\u2010Rosas 2015 ). To overcome other possible maternal micronutrient deficiencies, the United Nations Children's Fund (UNICEF), United Nations University (UNU) and the WHO, in 1999, agreed on the composition of a proposed multiple\u2010micronutrient (MMN) tablet ( UNICEF 1999 ). This UNIMMAP tablet provides one recommended daily allowance of vitamin A, vitamin B1, vitamin B2, niacin, vitamin B6, vitamin B12, folic acid, vitamin C, vitamin D, vitamin E, copper, selenium and iodine with 30 mg of iron and 15 mg of zinc for pregnant women. In contrast to the WHO recommendation, a lower dose of iron was recommended as the absorption of iron was expected to be enhanced due to vitamin C, vitamin A, and riboflavin, and given that the majority of pregnant women suffer from mild anaemia and the potential side\u2010effects associated with higher doses of iron. How the intervention might work Vitamins and minerals play critical roles in cellular metabolism, growth and maintenance of normal functioning of the human body. They are also important in many enzymatic processes, signal transduction and transcription pathways ( McArdle 1999 ; WHO 2004 ). Recent studies have suggested a possible benefit of multiple micronutrient supplementation for improving pregnancy outcomes through placental function, including modulation of inflammation and oxidative stress and vascular function ( Owens 2015 ; Richard 2017 ). Deficiencies of these micronutrients rarely exist in isolation. Additionally, because of their role at various levels in the biological pathways, it is difficult to assign a clinical or pre\u2010clinical condition to the deficiency of a single micronutrient ( McArdle 1999 ). Micronutrient deficiencies are also known to interact. Combining MMN in a single delivery mechanism has been suggested as a cost\u2010effective way to achieve multiple benefits. Why it is important to do this review The interest of the global research community in eliminating micronutrient deficiencies stems from their significant negative impact on the health of women and infants. The health effects during the fetal life may also have consequences later as an adult. Several trials have demonstrated that supplementation with MMN during pregnancy reduces the risk of micronutrient deficiencies ( Haider 2012 ). Findings from individual trials regarding the benefit on other maternal and pregnancy outcomes are inconsistent, as individual studies may not have statistical power to evaluate effects on these outcomes. Several meta\u2010analyses have systematically reviewed and synthesised the evidence of the effect of supplementation with multiple micronutrients, with the first such synthesis of evidence being an earlier version of this Cochrane Review ( Bhutta 2012 ; Haider 2006 ; Haider 2011 ; Haider 2012 ; Kawai 2011 ; Ramakrishnan 2012 ). On the basis of the evidence, supplementation with MMN during pregnancy has been recommended ( Bhutta 2008 ; Bhutta 2013 ). However, a consensus is yet to be reached regarding the replacement of iron and folic acid supplementation with MMN. Since the last update of this Cochrane Review ( Haider 2017 ), evidence from a few large trials has recently been made available, inclusion of which is critical to inform global policy. This review updates a previously published Cochrane Review on MMN supplementation during pregnancy that had demonstrated positive effect of supplementation on birth outcomes ( Haider 2017 ). The effects of supplementation with individual micronutrients during pregnancy have been evaluated in other Cochrane Reviews. The effect of MMN supplementation in HIV\u2010infected pregnant women has been evaluated in another Cochrane Review ( Siegfried 2012 ).",
        "summary": "For pregnant women, multiple\u2010micronutrient supplementation reduces low birthweight and may reduce the numbers of small\u2010for\u2010gestational\u2010age infants, preterm births, and stillbirths compared with supplementation with iron. Of note, all trials were conducted in low\u2010 and middle\u2010income countries. High\u2010certainty evidence shows that multiple micronutrients (most commonly iron and folic acid; vitamins A, C, D, E, B1, B2, B3, B6, and B12; and copper, selenium, zinc, and iodine) given orally to pregnant women either throughout pregnancy and/or up to 12 weeks after delivery reduce the number of low birthweight infants (187 vs 212 per 1000 infants; all results on average) when compared with iron, with or without folic acid. Researchers found little to no difference between groups regarding stillbirths and overall perinatal mortality (moderate\u2010 to high\u2010certainty evidence). Subgroup analyses show that multiple micronutrients led to lower perinatal mortality when started after 20 weeks, although event rates were low (\u2264 4%; three RCTs; 41,121 participants). Moderate\u2010certainty evidence shows that multiple micronutrients may slightly reduce the numbers of small\u2010for\u2010gestational\u2010age infants (310 vs 337 per 1000 infants) and preterm births (188 vs 197 per 1000 infants), although the latter appears to occur primarily among women with body mass index (BMI) < 20 kg/m 2 (three RCTs; 50,926 participants). Reviewers found no studies assessing other outcomes such as neurodevelopmental delay, side effects of multiple\u2010micronutrient supplementation supplements, or congenital anomalies."
    },
    "CD011045": {
        "query": "How does portion size or package or tableware size affect selection and consumption of food and tobacco?",
        "document": "Background Description of the condition Non\u2010communicable diseases, principally cardiovascular diseases, diabetes, certain forms of cancer and chronic respiratory diseases, accounted for an estimated 62% of all deaths worldwide in 2012 ( World Health Organization 2014a ), and globally the proportion of years of life lost as a result of non\u2010communicable diseases increased from 38% in 2000 to 47% in 2012 ( World Health Organization 2014b ). Major risk factors for these conditions are in part determined by patterns of behaviour that are in principle modifiable, including consumption of food, alcohol and tobacco products ( United Nations 2014 ). Identifying interventions that are effective in achieving sustained health behaviour change has therefore become one of the most important public health challenges of the 21 st century. Description of the intervention It is increasingly recognised that the physical environments that surround us can exert considerable influences on our health behaviour and that altering these environments may provide a catalyst for behaviour change ( Das 2012 ). In a recent scoping review, we described a class of interventions that involve altering the properties or placement of objects or stimuli within micro\u2010environments such as shops, restaurants, bars or homes, with the intention of changing health\u2010related behaviours ( Hollands 2013a ; Hollands 2013b ). The size of a portion or package is a modifiable property of food, alcohol and tobacco products that may influence their selection and consumption. In the case of food and alcohol products, the size or shape of an item of tableware used to consume such products may similarly influence their selection and consumption. Examples include the portion size of alcoholic beverages served in bars or of foods served in restaurants, at a buffet or in the home, such as portions of a dish served to restaurant customers ( Diliberti 2004 ), the size or shape of plates or glasses used to serve products ( Shah 2011 ), and the number or length of cigarettes in packets sold in shops ( Russell 1980 ). In this context, the intervention involves manipulation of the size or physical dimensions of a food, alcohol or tobacco product, its packaging or the tableware used in its consumption. Comparisons of interest are between products, packages or items of tableware that differ only in terms of these properties. How the intervention might work There are considerable influences on behaviour that are beyond individuals' deliberative control. Indeed, it has been suggested that most human behaviour occurs outside of awareness, cued by stimuli in environments and resulting in actions that may be largely unaccompanied by conscious reflection ( Marteau 2012 ; Neal 2006 ). This proposition has led to increasing policy and research attention being placed on interventions with mechanisms of action that are less dependent on the conscious engagement of the recipients, including interventions that involve altering properties of objects or stimuli within the small\u2010scale environments that surround and cue behaviour ( Hollands 2013a ). A number of mechanisms of action have been proposed to explain how the size of products may affect their consumption ( Herman 2015 ; Steenhuis 2009 ). It has been suggested that as the amount of a product made available for consumption is increased, individuals will continue to perceive each increasing amount as an appropriate quantity to consume. This phenomenon may be explained by several mediating factors including personal and social norms about what constitutes a suitable amount of a product to consume. Such norms can be influenced by the amounts that are presented for consumption, and larger portions of food have become increasingly prevalent, making it increasingly unlikely that smaller portions are viewed as normal or appropriate for a single serving ( Young 2002 ). There is also a tendency for individuals to engage most comfortably with a product as a single entity independent of its size. This 'unit bias' means that they are predisposed to consume the entirety of a product even as it changes size ( Geier 2006 ). In addition, the way in which products are presented can influence their consumption. The presentation of food and alcohol products often entails the use of tableware, such as plates, glasses or cutlery. Not only does the size of tableware have the potential to directly influence the amount of a product available for consumption ( Pratt 2012 ), but its physical dimensions can elicit various cognitive biases ( Wansink 2005 ), which may influence perceptions of quantity and in turn determine levels of consumption. Similarly, sub\u2010dividing a fixed portion of a food into smaller pieces also affects perceptions of quantity ( Scisco 2012 ). All of these mechanisms may also influence product selection (with or without purchasing), which is an important intermediate outcome in pathways to consumption. Extant research involving the experimental manipulation of portion, package or tableware size has focused on food (including non\u2010alcoholic beverage) products to a much greater extent than tobacco products ( Hollands 2013a ). Whilst the causal mechanisms of underlying potential effects of such manipulations on selection or consumption of tobacco may be assumed to be broadly similar to food, smokers are known to titrate their received dose of nicotine to regulate the level in the body, with the potential to attenuate the effects of interventions to alter the size of tobacco products ( Kozlowski 1986 ). Why it is important to do this review A recent scoping review of evidence for the effects of choice architecture interventions identified a substantial number of randomised controlled trials that have investigated the effects of exposure to different portion, package or tableware sizes on selection and consumption behaviours ( Hollands 2013a ). The majority of these studies focused on food products, but because both tobacco and alcohol use also involve the selection and consumption of products, similar interventions may have the potential to change these behaviours via similar mechanisms. To our knowledge, evidence from these studies has yet to be synthesised using rigorous systematic review methods that include assessment of risk of bias and investigation of potential effect modifiers, nor to encompass alcohol and tobacco use. As such, we do not yet have reliable estimates of the effects of altering the sizes of portions, packages or tableware on product selection and consumption, nor of the influence of factors that may modify any such effects. Both are necessary to inform the selection and design of effective public health interventions. Interventions that aim to reduce people's exposure to larger or smaller food portions, as opposed to those that involve providing information to encourage health behaviour change, may also have the potential to reduce health inequalities if they rely less on recipients' levels of literacy, numeracy and cognitive control, which have been found to be lower in population subgroups experiencing higher levels of social and material deprivation ( Kutner 2006 ; Marteau 2012 ; Spears 2010 ; Williams 2003 ). Despite evidence that behaviours with the potential to undermine health are socially patterned (for example, that people in lower socioeconomic groups tend to consume less fruit and vegetables ( Giskes 2010 )), potential differences in behavioural responses to product sizing interventions between socioeconomic subgroups remain unclear. Also, to our knowledge (prior to conducting this review), no studies of the effects of product size had been conducted in low or middle\u2010income (LMIC) country populations ( Hollands 2013a ). This review therefore includes a focus on identifying evidence for differential effects of exposure to different sizes of these products between socioeconomic subgroups (and between studies conducted in LMIC and high\u2010income countries (HIC)), highlight any identified gaps in this aspect of the evidence base, and seek to draw implications for the potential of such interventions to affect health inequalities. This systematic review is also timely given current interest in the topic within public health policy circles. There is evidence from the USA and Europe that portion sizes have been increasing since the 1970s ( Young 2002 ; Young 2012 ). There have also been recent attempts to regulate the size of products in order to reduce consumption levels and improve public health, such as New York City Mayor Michael Bloomberg's proposed ban on the sale of sugary drinks larger than 16 oz (473 ml) ( Gabbatt 2013 ). In the UK, there are recent examples of companies reducing the portion sizes of confectionery and sugary drinks as part of the Public Health Responsibility Deal in England. This systematic review can contribute to a better evidence\u2010based understanding of the potential impact of such policies.",
        "summary": "Larger portions or packages and larger individual units or tableware increase daily energy intake by a small to moderate amount for both children and adults. Moderate\u2010quality evidence shows that larger portions and packages and larger individual units or tableware increases (by a small to moderate extent) the quantities of food or tobacco consumed by people considered to be unrestrained eaters. Among 72 trials, 69 (96%) manipulated food products and three (4%) manipulated tobacco products. The reviewers calculated that daily energy intake from food consumed would be on average 247 kcal higher (14.3%) for adults and 95 kcal higher (5.7%) for children when larger portions or packages and larger individual units or tableware were used (moderate\u2010quality evidence). A similar pattern was observed when selection without purchase was assessed. Moderate\u2010quality evidence shows that larger portions and packages and larger individual units or tableware increases the quantities of food selected by people by a small to moderate amount. It was calculated that daily energy intake from food selected would be on average 188 kcal higher (10.9%) for adults (moderate\u2010quality evidence) and 63 kcal higher (3.8%) for children (low\u2010quality evidence) with larger portions and packages and larger individual units or tableware. Studies did not assess selection with purchase."
    },
    "CD006915": {
        "query": "In infants with bronchiolitis, is there randomized controlled trial evidence to support the use of heliox inhalation therapy?",
        "document": "Background Description of the condition Bronchiolitis, an acute inflammatory process of the small airways, is the leading cause of hospitalisation among infants in high\u2010income countries ( Hall 2013 ; Welliver 2003 ). Common symptoms include a runny nose, cough and dyspnoea often with bronchospasm, mucus production and wheezing. Apnoea may be the initial manifestation before other respiratory signs are present and is most common in premature infants ( Ralston 2009 ). Respiratory syncytial virus (RSV) is the most common pathogen isolated ( Mansbach 2012 ). Approximately 20% of all infants have RSV\u2010associated wheezing in the first year of life and 2% to 3% require hospitalisation as part of the management strategy ( AAP 2006 ). Young age (under six months of age) at the onset of the RSV season increases the risk of RSV hospitalisation ( Carbonell\u2010Estrany 2000 ; Figueras\u2010Aloy 2008 ; Law 2004 ; Liese 2003 ). Analysis of representative studies over the last 30 years found that the risk ratio of boys to girls is 1.4:1 ( Simoes 2003 ). More than 50% of hospitalisations caused by RSV infections are in infants and children with no known risk factors ( Boyce 2000 ). Even though there are many treatments, there is no evidence to endorse a specific treatment other than supportive care ( Davison 2004 ). Current evidence suggests that nebulised 3% saline may significantly reduce the length of hospital stay among infants hospitalised with non\u2010severe acute viral bronchiolitis and improve the clinical severity score in both outpatient and inpatient populations ( Zhang 2013 ). Supplemental oxygen and judicious fluid management remain the mainstays of therapy. Ventilatory support can be necessary in 7.5% of infants hospitalised for bronchiolitis with an overall clinical score of at least 4 on a scale of 0 to 10 ( Skjerven 2013 ). The use of heated, humidified, high\u2010flow nasal cannula (HFNC) therapy may reduce the need for invasive respiratory support thus potentially lowering costs, with clinical advantages and fewer adverse effects ( Beggs 2014 ; Bressan 2013 ; Hough 2014 ; Mayfield 2014 ; Milesi 2013 ). Premature or low birth weight infants, infants with bronchopulmonary dysplasia and patients with haemodynamically significant congenital heart disease merit special attention. Recipients of hematopoetic stem cell transplants have had especially high rates of severe disease.The relatively smaller airways of select infant groups places them at higher risk of respiratory failure and need for specialised management. For example, the percentage of patients requiring endotracheal intubation or positive pressure ventilation (PPV) is higher in infants with congenital heart disease, immunocompromised status ( Wang 1995 ), chronic lung disease or those born prematurely ( Meert 1990 ). Mortality rates of infants hospitalised in a paediatric intensive care unit (PICU) for RSV\u2010bronchiolitis range from 0% to 3% if patients have no risk factors, and 2.5% to 6% if they have at least one risk factor ( Chevret 2005 ; Prais 2003 ; Wang 1995 ). Mortality associated with RSV infection is relatively rare among young children in high\u2010income countries. The majority of deaths occur in infants with complex chronic conditions and in those with other acute conditions, such as sepsis, which could have contributed to their deaths ( Byington 2015 ). It was estimated that between 66,000 and 199,000 children younger than five years died from RSV\u2010associated acute lower respiratory infection in 2005, with 99% of these deaths occurring in low\u2010income countries ( Nair 2010 ). Description of the intervention In 1934, Barach first described the use of helium\u2010oxygen gas mixtures (heliox) for the treatment of upper airway obstruction ( Barach 1934 ). Heliox has subsequently been shown to be a useful adjunctive therapy in children with asthma ( Carter 1996 ; Kudukis 1997 ), bronchiolitis ( Gross 2000 ; Hollmann 1998 ; Paret 1996 ), upper airway obstruction ( Duncan 1979 ; Tobias 1997 ), acute respiratory distress ( Winters 2000 ), and in children with post\u2010extubation stridor ( Kemper 1991 ). Heliox may also increase the effectiveness of nasal continuous positive airway pressure (nCPAP) in the treatment of respiratory distress syndrome in premature infants ( Colnaghi 2012 ). Although heliox can be an effective treatment option, the existing evidence does not provide support for the administration of heliox mixtures to all emergency department patients with acute asthma ( Rodrigo 2010 ). How the intervention might work In bronchiolitis, breathing becomes more difficult due to an increased end\u2010expiratory lung volume, decreased lung compliance and relative upper airway obstruction with increased airway resistance. Infection of bronchiolar respiratory and ciliated epithelial cells produces increased mucus secretion, cell death and sloughing. This is followed by a peribronchiolar lymphocytic infiltrate and submucosal oedema ( AAP 2006 ; Piedimonte 2002 ). The combination of debris and oedema produces obstruction of the smaller airways. This critical narrowing results in turbulent flow and increased airway resistance. Decreased ventilation in affected areas causes ventilation/perfusion mismatching, resulting in hypoxia. During the expiratory phase of respiration, dynamic collapse of the airways produces a disproportionate decrease in airflow and resultant air trapping. Since bronchiolitis is associated with airway obstruction and turbulent gas flow, this disease process could theoretically be improved by heliox, which improves gas flow through high\u2010resistance airways because of the lower density of helium compared to air ( Gupta 2005 ; Panitch 2003 ). Helium is an inert gas with no intrinsic bronchodilatory or anti\u2010inflammatory properties. Helium has the lowest density of any gas other than hydrogen, which is unfortunately not medically useful because of its flammable properties. Helium acts as a 'carrier gas', resulting in lower resistance to gas flow allowing for increased bulk flow, increased oxygen flow and decreased work of breathing ( Wolfson 1984 ). Equally important is the fact that carbon dioxide diffuses through helium four to five times faster than through air, which aids ventilation and carbon dioxide removal. The effects of heliox are relatively rapid and therefore any significant clinical effects are expected to be seen within minutes. A one\u2010hour trial is usually enough to detect any beneficial effect of heliox ( Martinon\u2010Torres 2015 ). Thus, the clinician quickly knows if heliox therapy will be beneficial for an individual patient or whether it should be abandoned for other possible therapies. For non\u2010ventilated patients, it is also essential to ensure that there is no accidental contamination of the heliox mixture with air or oxygen. For instance, heliox administration via a standard high\u2010concentration reservoir mask leads to significant dilution by room air ( Standley 2008 ). The use of premixed heliox tanks with at least a 21% oxygen concentration avoids accidental administration of a hypoxic gas mixture. Manufacturers supply pre\u2010mixed bottles at different concentrations of helium/oxygen for clinical use. The administration of heliox during mechanical ventilation must be carried out with vigilance and accurate, continuous monitoring. Helium can interfere with the accuracy of pneumotachometer and ventilator function, which are typically calibrated for nitrogen instead of helium as the primary balance gas ( Berkenbosch 2003 ). New generation helium/oxygen administration systems have been developed to help circumvent these issues. Even if helium\u2010oxygen can be safely delivered by modifying standard ventilators according to guidelines in the literature, caution must be applied when using this approach ( Martinon\u2010Torres 2012 ). Why it is important to do this review The hypothesis of this review is that heliox inhalation is beneficial in the management of acute bronchiolitis as assessed by clinically relevant outcomes. In order to critically evaluate the clinical data, we undertook a systematic review of trials that use heliox for the treatment of bronchiolitis.",
        "summary": "High\u2010quality evidence suggests that the use of heliox therapy (a helium\u2010oxygen mixture) in infants with bronchiolitis admitted to the ICU did not reduce the need for mechanical ventilation, when compared with air or oxygen using the same protocol as that for heliox. There was also no apparent difference between groups in terms of mortality or rate of endotracheal intubation, however, the event rates were very low, and the analyses were likely to have been underpowered. Moderate quality evidence suggests that heliox does have a short\u2010term benefit (within 1 hour of starting treatment) in modestly improving respiratory scores (by approx. 1 point on a 10\u2010point scale)."
    },
    "CD012069": {
        "query": "What adverse events are associated with methylphenidate in children and adolescents with attention deficit hyperactivity disorder (ADHD)?",
        "document": "Background Description of the condition Attention deficit hyperactivity disorder (ADHD) is one of the most commonly diagnosed and treated childhood neurodevelopmental disorders ( Scahill 2000 ). The estimated prevalence in children and adolescents is between 3% to 8% ( Polanczyk 2007a ; Thomas 2015 ; Willcut 2012 ), depending on the classification system used, with boys two to four times more likely to be diagnosed than girls ( Schmidt 2009 ). Prevalences have remained stable over the past 30 years and do not appear to vary between countries ( Polanczyk 2014 ). Individuals with ADHD may show difficulties in attention and cognitive functions like problem\u2010solving, planning, orienting, flexibility, response inhibition and working memory, as well as impulsivity and hyperactivity ( Pasini 2007 ; Sergeant 2003 ). Furthermore, children and adolescents have difficulties handling affective components such as motivational delay and mood dysregulation ( Castellanos 2006 ; Nigg 2005 ; Schmidt 2009 ). The aetiology of ADHD involves genetic, environmental and social factors but is not yet completely understood. Family and twin studies have shown a high heritability of around 70% to 80% and with a substantial overlap between the two dimensions of hyperactivity/impulsivity and inattention and with no sex differences of heritability ( Franke 2012 ; Neale 2010 ). Furthermore, genetic factors may be involved in determining the persistence of ADHD into adulthood ( Faraone 2000 ; Franke 2012 ). Although family studies have shown high heritability, and there are many candidate genes that may be involved in the disorder ( Neale 2010 ), genome\u2010wide studies have yet to find any clear associations. Several studies have examined environmental risk factors for ADHD; however, researchers have not found any specific predictor for elevated risk. At the population level, poverty (families living under the poverty level) is more likely to be a feature among American children and adolescents diagnosed with ADHD ( CDC 2015 ). In a Swedish cohort of 811,803 individuals, low family income in early childhood was highly associated with ADHD ( Larsson 2014 ). Other potential risk factors for ADHD development include low birthweight ( Indredavik 2004 ; Van Lieshout 2015 ), prematurity ( Bhutta 2002 ; Burnett 2014 ; Elgen 2015 ), maternal exposure to tobacco ( Kovess 2015 ; Obel 2016 ), and exposure to chemical components like manganese and lead ( Hong 2014 ; Hong 2015 ). The diagnosis of ADHD is purely clinical, requiring recognition of excessive inattention, hyperactivity and impulsivity that interfere with or reduce the quality of social, academic, or occupational functioning ( APA 2013 ; WHO 1992 ). There are 18 core symptoms of ADHD according to the principal diagnostic classification systems: International Classification of Diseases \u2010 10th Revision (ICD\u201010; WHO 1992 ) and the Diagnostic and Statistical Manual of Mental Disorders (DSM) \u2010 4th Edition (DSM\u2010IV; APA 1994 ), \u2010 4th Edition \u2010 Text Revision (DSM\u2010IV\u2010TR; APA 2000 ), and \u2010 5th Edition (DSM\u20105; APA 2013 ). Both the DSM\u20105 and ICD\u201010 criteria require excessive inattention, hyperactivity, and impulsivity to be inconsistent with the developmental level and to be pervasive. The symptoms must be present in two or more settings and appear before the age of 6 years according to the ICD\u201010 ( WHO 1992 ), or 12 years according to the DSM\u20105 ( APA 2013 ), and they should also persist for at least six months. The DSM\u20105 modified the criteria for adolescents and adults older than 17 years of age, requiring fewer perceived symptoms and providing further descriptions to better identify typical ADHD symptoms in adolescents. Earlier versions of the DSM and the ICD\u201010 required that there be clear evidence of clinically significant impairment in social, academic, and occupational functioning ( APA 1994 ; APA 2000 ; APA 2013 ; WHO 1992 ), but the DSM\u20105 only requires that symptoms interfere with or reduce the quality of these domains ( APA 2013 ). Furthermore the ICD\u201010 and the DSM\u2010IV differ from the newer DSM\u20105 in excluding people with autism spectrum disorder; DSM\u20105 only excludes people during the course of schizophrenia or another psychotic or mental disorder. The diagnostic criteria describe three different subtypes or 'presentations' in the DSM\u20105, according to the predominant symptoms: 'predominantly inattentive type', 'predominantly hyperactive\u2010impulsive type', and 'combined type' \u2013 a combination of both hyperactive\u2010impulsive and inattentive symptoms. The DSM\u20105 acknowledges the absence of validity for these subtypes by renaming them predominantly inattentive, predominantly hyperactive\u2010impulsive, and combined presentations ( APA 2013 ; Willcut 2012 ). Children, adolescents, and adults with ADHD are at increased risk of a broad spectrum of co\u2010occurring conditions, which frequently result in negative outcomes later in life ( Newcorn 2008 ; Schmidt 2009 ). The Multimodal Treatment of Attention Deficit Hyperactivity Disorder (MTA) trial identified one or more comorbid disorders in almost 40% of the participants ( MTA 1999 ). These included oppositional defiant disorder, conduct disorder, depression, anxiety, tics, learning disorders, and verbal and cognitive difficulties ( Jensen 2001 ; Kadesj\u00f6 2001 ). ADHD has also been shown to co\u2010occur with bipolar disorder ( Perroud 2014 ). More recently, studies have confirmed such comorbidity ( Czamara 2013 ; Yoshimasu 2012 ), with some authors noting that excess weight and obesity are found in children with ADHD ( Cortese 2016 ). In a study with 1480 twin pairs from Sweden, researchers found that persistent hyperactivity or impulsivity symptoms of ADHD are associated with both early\u2010onset tobacco and alcohol use ( Chang 2012 ). Similarly, ADHD comorbidity with conduct disorder can lead to adverse outcomes in academic achievement, failure to complete high school, criminality, substance use disorder, and unemployment ( Erskine 2016 ). In addition, ADHD is associated with several harmful consequences. A cohort of participants with ADHD who were followed up to the age of 40 years demonstrated that these individuals have an elevated risk of criminality and a high risk of death before 40 years of age ( Koisaari 2015 ). Similarly, studies from health insurance plans demonstrated not only elevated risk of injury, but also higher indirect costs of those with an ADHD diagnosis compared to diagnosis of depression ( Hodgkins 2011 ). Recently, ADHD has been linked to increased premature mortality higher than 50%, compared to non\u2010ADHD patients, in a 24.9 million person\u2010years Danish cohort study ( Dalsgaard 2015b ). To ensure high standards in assessment, diagnosis and therapeutic practice, professional and national bodies have developed guidelines ( AAP 2011 ; CADDRA 2011 ; NCCMH 2009 ; Pliszka 2007a ; Scottish Intercollegiate Guidelines Network (SIGN) ). Psychosocial interventions are recommended initially for younger children and for mild to moderate symptoms ( AAP 2011 ; NCCMH 2009 ; Pliszka 2007a ). For more severe ADHD symptoms, stimulants, either alone or in combination with psychosocial interventions, may be necessary ( AAP 2011 ; CADDRA 2011 ; NCCMH 2009 ). Description of the intervention Stimulant medication, notably methylphenidate and dexamphetamine (or dextroamphetamine), together with the non\u2010stimulants atomoxetine (a non\u2010stimulant selective noradrenaline reuptake inhibitor) and guanfacine (an alpha 2A agonist), are considered the treatments of choice along with psychosocial treatments for children and adolescents with ADHD ( Greenhill 2006 ; NICE 2008 ; Pliszka 2007a ) . Globally, methylphenidate is the most commonly used drug prescribed for ADHD; it has been used in practice for more than 50 years ( Kadesj\u00f6 2002 ; NCCMH 2009 ) . Methylphenidate is used because it appears to have a favourable effect on reducing the core symptoms of excessive hyperactivity, impulsivity, and inattention in children and adolescents with ADHD. It is licensed for use in children aged six years and older. Rates of prescription of methylphenidate are high and increasing, standing at approximately 8% of children and adolescents under 15 years of age in the USA ( Akinbami 2011 ), and around 3% to 5% in Europe ( Bachmann 2017 ; Hodgkins 2013 ; Schubert 2010 ; Trecen\u00f5 2012 ; Zo\u00ebga 2016 ). ADHD medication appears to be discontinued in 13% to 64% of patients from all age groups ( Adler 2010 ) , but information of the continuity of these treatments from childhood or adolescence into adulthood is still lacking. Dexamphetamine is licensed for use in children aged three years and older. It is also available for use as mixed\u2010amphetamine salts (levoamphetamine plus dextroamphetamine) and as a pro\u2010drug of dexamphetamine, lisdexamphetamine. These have a longer duration of action than dextroamphetamine. Clinical choice of preparation by clinicians and families is based on a range of factors, including the presence of co\u2010occurring conditions, adverse events associated with the drug, issues regarding compliance, and the preference of the child and parents. Methylphenidate dose varies from patient to patient. The dose needs to be titrated individually in order to maximise benefits and minimise potential adverse events ( Stevenson 1989 ) . The daily therapeutic range of methylphenidate dosages varies from 5 mg to 60 mg, administered one to three times daily, depending on the release system (immediate, sustained, or extended release) and mode of administration (oral or transdermal) ( Pliszka 2007a ; Storeb\u00f8 2015 ) . The British National Formulary suggests that initial doses in children aged four to six years should be 2.5 mg twice daily. Where necessary, it should be increased at weekly intervals by 2.5 mg daily, to a maximum of 1.4 mg/kg daily (divided into two to three doses per day) ( BNF 2018 ). In children aged 6 years to 18 years, the initial dose may be 5 mg once or twice daily, increased, where necessary, at weekly intervals by 5 mg to 10 mg daily in two to three divided doses. Although methylphenidate is licensed to a maximum dose of 60 mg daily, it may be increased by 2.1 mg/kg daily in two to three divided doses (maximum 90 mg daily) under specialist supervision. The bioavailability of oral methylphenidate is 11% to 52%, with an approximate duration of action of 2 to 4 hours for immediate\u2010release methylphenidate, 3 to 8 hours for sustained\u2010release methylphenidate, and 8 to 12 hours for extended\u2010release methylphenidate ( Kimko 1999 ) . How the intervention might work The pharmacodynamics of methylphenidate are still not entirely clear. Methylphenidate has both dopamine and noradrenaline transporter\u2010binding affinity and binds to and blocks both transporters, leading to increased availability of noradrenaline and dopamine within the synaptic cleft ( Heal 2006 ; Volkow 1998 ; Volkow 2004 ; Volkow 2012 ). This is thought to increase the general firing rate via increased neurotransmission of dopamine and noradrenaline, which, in turn, has an effect on the prefrontal cortex \u2013 responsible for executive function \u2013 and is linked to sub\u2010performance of dopamine and noradrenaline functions associated with ADHD ( Arnsten 2005 ). As a result, patients can improve function (through symptom control) and experience several benefits such as improved attention and reduced hyperactivity\u2010impulsivity ( Barkley 1977 ; Barkley 1981 ; Barkley 1989 ; Connor 2002 ; Engert 2008 ; Schulz 2012 ; Shaw 2012 ; Solanto 1998 ), which may improve classroom functioning and academic learning ( Biederman 2003 ; Cox 2004 ; Evans 2001 ; Swanson 2004 ). Methylphenidate has also been correlated with a reduction of several harmful outcomes. In an extensive cohort of 710,120 individuals, including 4557 individuals diagnosed with ADHD before age 10 years, the use of methylphenidate was found to reduce emergency department visits by 46% and injuries by 44% ( Dalsgaard 2015a ). However, given the lack of sufficiently powered, well\u2010conducted randomised clinical trials (RCTs), it is not clear if these are genuine benefits or statistical artefacts ( Garattini 2016 ; Storeb\u00f8 2015 ). In a Swedish national register composed of 25,656 participants, Lichteinstein 2012 demonstrated a 32% and 41% reduction in criminality among men and woman respectively, when treated with medications for ADHD. In another study, the researchers found that medication was associated with a 58% risk reduction in serious transport accidents, and estimated that 41% to 49% of the accidents could have been avoided if ADHD male patients were in drug treatment ( Chang 2014a ). There have also been more recent reports of reductions in motor vehicle crashes in patients on methylphenidate ( Chang 2017 ). Similarly, ADHD drugs decreased injuries among 5\u2010 to 10\u2010year\u2010old children from 32% to 44%, when compared to ADHD children without treatment ( Dalsgaard 2015a ). It is also a general concern that ADHD medication treatment can lead to substance abuse. Contrary to this, the prescription of ADHD stimulants was associated with a 31% decrease in substance abuse ( Chang 2014b ). A similar concern was the association of ADHD treatment and suicide, but again, the treatment was correlated with a protective effect ( Chen 2014 ). Why it is important to do this review The most commonly reported adverse events associated with methylphenidate are headache, sleep problems, fatigue, and decreased appetite. Studies have indicated that methylphenidate also impairs both children's height and increases in weight ( Schachar 1997 ; Swanson 2004 ; Swanson 2009 ). Serious adverse events, such as psychosis and mood disorders, are reported to affect approximately 3% of children treated with methylphenidate ( Block 1998 ; Cherland 1999 ; MTA 1999 ; NICE 2009 ; Pliszka 1998 ). An observational study supports an association between the use of stimulants and sudden unexplained death among children and adolescents ( Gould 2009 ). The study showed an odds ratio (OR) of 7.4 (95% confidence interval (CI) 1.4 to 74.9) for use of stimulants, specifically methylphenidate, in children and adolescents with sudden death compared to aged\u2010matched, motor vehicle accident deaths ( Gould 2009 ). Further research is needed to determine whether these deaths are related to methylphenidate ( US FDA 2011 ). Reports of sudden death in adults taking methylphenidate are also a concern ( Jackson 2016 ). In an update ( US FDA 2011 ), the FDA found no evidence for increased risk of serious cardiovascular events in adults treated with ADHD medications, based on two epidemiological studies ( Cooper 2011 ; Habel 2011 ). Recent reviews of methylphenidate treatment have focused on its benefits only, as opposed to its harmful effects ( Charach 2013 ; Faraone 2002 ; Faraone 2006 ; Faraone 2010 ; Hanwella 2011 ; Maia 2017 ). Relatively few randomised clinical trials included in our Cochrane Review assessing methylphenidate versus placebo or no intervention reported adverse events ( Storeb\u00f8 2015 ). Given the worldwide increase in methylphenidate prescriptions to children and adolescents, the need for an evidence\u2010based risk profile for serious and non\u2010serious adverse events remains ( Bushe 2013 ; Cairns 2014 ). To expand our understanding of adverse events, particularly where these are rare or take time to become apparent, it is necessary to bolster the limited data from RCTs by including data from non\u2010randomised studies ( Storeb\u00f8 2015 ). Non\u2010randomised studies have a number of advantages; they are often larger (allowing for detection of rare events), have a broader range of participants (reflecting 'real\u2010life'), longer follow\u2010up times, and lower costs than RCTs ( Benson 2000 ; Hannan 2008 ; Silverman 2009 ). Non\u2010randomised studies may detect adverse events due to long\u2010term drug exposure, which would not be detected in relatively short RCTs ( Storeb\u00f8 2015 ). Some adverse events may be too uncommon to be detected in RCTs ( Loke 2011 ), and as a result, cohort studies, patient\u2010control studies, and even patient reports/series may be of value ( Reeves 2011 ). In fact, non\u2010randomised studies can estimate the adverse events of treatment as well as, and maybe even more comprehensively than, RCTs ( Vandenbroucke 2006 ). RCTs and non\u2010randomised studies investigating adverse events rarely find differences in risk estimates ( Golder 2011 ). In their review, Pitrou 2009 found that some RCTs provided no information on adverse events, and severity was often poorly defined. Only 13% of studies noted the reasons for patient withdrawal due to adverse events. Another study reported that only 18% of all paediatric RCTs published between 2006 and 2009 documented harms adequately according to CONSORT guidelines ( De Vries 2010 ). Our findings were similar ( Storeb\u00f8 2015 ), with only 17/185 RCTs (9.20%) reporting serious adverse events and approximately 60/185 RCTs (32.0%) reporting non\u2010serious adverse events. The main disadvantage, however, is that causality cannot be established in observational studies because the observed adverse event may be related to other factors. Nonetheless, in view of the poor reporting of adverse events in RCTs, non\u2010randomised studies may provide important data that would otherwise remain undetected ( Loke 2011 ; Vandenbroucke 2006 ). Such data from non\u2010randomised studies may help children, adolescents, families, clinicians, and policymakers understand the relative risks and benefits, leading to better informed choices regarding methylphenidate treatment. When we deal with serious adverse events, troublesome non\u2010serious adverse events, and/or prevalent non\u2010serious adverse events, we should remember the recommendations from regulatory authorities stating that P values are of very limited value as substantial differences (expressed as relative risk or risk differences) require careful assessment and will, in addition, raise concern, depending on seriousness, severity or outcome, irrespective of the P value observed ( EMA 2017 ). A non\u2010significant difference between treatments will not allow for a conclusion on the absence of a difference in safety. In other words, in line with general principles, a non\u2010significant test result should not be confused with the demonstration of equivalence ( EMA 2017 ).",
        "summary": "Compared with no methylphenidate for children and adolescents with ADHD, comparative studies suggest that methylphenidate is more commonly associated with serious adverse events and psychotic disorders, although event rates are extremely low in both groups (both 16 vs 12 per 1000 children). Methylphenidate is also more commonly associated with arrhythmia but is less commonly associated with ischemic stroke and heart failure (absolute values not available). Non\u2010comparative studies reported approximate proportions of people receiving methylphenidate experiencing a serious adverse event or psychotic disorder as 1%, arrhythmia 3%, heart failure 1%, hypertension 0.6%, and seizures, sudden death, and suicide all < 0.5%. All evidence was of very low certainty; therefore, no firm conclusions can be drawn."
    },
    "CD001893": {
        "query": "How does epidural local anesthetics compare with opioid\u2010based analgesia for improving outcomes in adults undergoing abdominal surgery?",
        "document": "Background This is an update of a previously published Cochrane review ( Jorgensen 2001 ). Description of the condition In 2011, nearly 29% of hospital stays and 48% of hospital costs in the United States (USA) involved operating room procedures ( http://www.hcup\u2010us.ahrq.gov/reports/statbriefs/sb165.jsp ). Among the 15 procedures most commonly performed in the USA were cholecystectomy and common bile duct exploration (129.4 per 100,000 population), abdominal and vaginal hysterectomy (99.4 per 100,000 population), colorectal resection (97.4 per 100,000 population), excision or lysis of peritoneal adhesions (97.4 per 100,000 population), appendicectomy (93.3 per 100,000 population) and oophorectomy (71.3 per 100,000 population). Thus abdominal surgery represents a significant proportion of hospital stays and costs. Gastrointestinal paralysis and postoperative pain are two major issues that need to be taken care of after abdominal surgery. Gastrointestinal paralysis following abdominal surgery may result in prolonged hospital stay and increased costs. Following laparotomy, laparoscopic cholecystectomy and colectomy, approximately 10.3% of patients will have an ileus ( Gan 2015 ). An ileus occurs more frequently in colectomy than cholecystectomy and more often when performed by laparotomy. Patients with ileus receiving opioids will have an increased length of hospital stay (ranging from 4.8 to 5.7 days), greater total costs (from USD 9,945 to USD 13,055) and a higher 30\u2010day all\u2010cause readmission rate (2.3% to 5.3% higher) compared with patients without an ileus ( Gan 2015 ). In 2000, the Joint Commission on Accreditation of Healthcare Organizations (JCAHO) suggested that pain should be considered as the fifth vital sign, and that under\u2010treatment of pain would constitute abrogation of a fundamental human right ( White 2007 ). After this statement was issued, an increase in the use of opioids for acute postoperative pain treatment was observed, as was an increase in their side effects ( White 2007 ). It was also noted that postoperative critical respiratory events were encountered more frequently during the first 24 hours after opioid therapy was introduced ( Ramachandran 2011 ). Description of the intervention Epidural anaesthesia or analgesia or both consist of injection of a local anaesthetic into the spine outside the dura mater. Epidural local anaesthetic may be used during abdominal surgery, sometimes as a replacement for general anaesthesia but most commonly as a supplement to general anaesthesia for surgery and for postoperative analgesia. How the intervention might work Epidural analgesia has been claimed to facilitate many of the steps through which a patient must go before returning to his or her preoperative functional level after a major abdominal surgery, including motility of the gastrointestinal tract ( Th\u00f6rn 1996 ). As summarized in their review, Holte and Kehlet considered that \"the pathogenesis of postoperative ileus is multifactorial, and includes activation of inhibitory reflexes, inflammatory mediators and opioids (endogenous and exogenous)\" ( Holte 2002 ). Epidural analgesia may promote a faster return to intestinal transit through various mechanisms including a reduction in opioid administration ( Guay 2006 ), a blockade of sympathetic gut innervation (creating a relative parasympathetic predominance) and a direct effect of systemic local anaesthetics ( McCarthy 2010 ). Thorn et al included 14 participants and demonstrated that the gastrointestinal electromyographic activity of participants who received epidurally administered bupivacaine was different from that of participants who received epidurally administered morphine ( Th\u00f6rn 1996 ). Oral acetaminophen absorption (as demonstrated by the area under the curve of acetaminophen blood concentrations from zero to 60 minutes) was also greater among participants who received bupivacaine than among those given morphine ( Th\u00f6rn 1996 ). Thus an epidural containing a local anaesthetic may promote faster gastrointestinal transit return than is attained with systemic opioids. Among undisturbed participants receiving patient\u2010controlled morphine analgesia after surgery, abnormal breathing patterns with cyclical airway obstruction are extremely common ( Drummond 2013 ). Provided that pain relief would be at least equivalent to that achieved with opioid therapy, decreasing the quantity of opioids administered ( Guay 2006 ) would make epidural analgesia with a local anaesthetic appear as an interesting alternative in the treatment of acute postoperative pain for the first days after abdominal surgery \u2010 the time when pain is most intense. Reducing the quantity of opioids administered after surgery may reduce the rare, but serious, adverse respiratory events associated with administration of opioids for the treatment of postoperative pain. Why it is important to do this review This is an update of a previously published Cochrane review ( Jorgensen 2001 ) in which review authors concluded that an epidural with a local anaesthetic reduced the time to return of gastrointestinal transit but with substantial heterogeneity. The effect of additional epidural opioid on gastrointestinal function was unclear. We undertook this review to look for new studies, to update methods and to re\u2010explore heterogeneity.",
        "summary": "Epidural local anesthesia (with or without opioids) seems to have benefits over opioid\u2010based analgesia in terms of postoperative return of gastrointestinal transit and pain control. However, most trials failed to report key methodological features and adverse effects were not assessed, making the drawing of any firm conclusion difficult. When epidural local anesthesia (with or without opioids) was compared with opioid\u2010based analgesia in adults undergoing a range of abdominal surgeries, postoperative gastrointestinal transit (time to first flatus (high\u2010quality evidence) and stool (low\u2010quality evidence) recovered much more quickly with epidural local anesthesia. In addition, pain control was better both at rest and on movement with epidural local anesthesia at 6 to 8 hours, 24 hours, 48 hours and 72 hours\u2019 post\u2010surgery; the difference in pain scores was moderate to large across the time points. No differences were detected between the two regimens in terms of vomiting (low\u2010quality evidence), gastrointestinal anastomotic leak (low\u2010quality evidence) or duration of hospital stay (very low\u2010quality evidence). It is worth noting that most trials included in these analyses failed to report key methodological features, which means they could be subject to bias. This would be of particular concern for subjective outcomes such as pain control. When subgroup analyses were conducted based on the type of surgery or opioid used, the results of the larger subgroups tended to reflect those of the overall analyses. Where no differences were detected for subgroups between the regimens, these tended to be the smaller, underpowered analyses. Adverse effects were not assessed in the review."
    },
    "CD006150": {
        "query": "How do adjunctive corticosteroids affect outcomes of Pneumocystis jiroveci pneumonia in patients with HIV?",
        "document": "Background Description of the condition With the introduction of highly active antiretroviral therapy (HAART) more than two decades ago, the incidence of Pneumocystis jiroveci pneumonia ( Stringer 2002 ) has decreased significantly in the Western hemisphere. However, PCP (the acronym stands for pneumocystis pneumonia) still remains one of the most common opportunistic infections in patients infected with the human immunodeficiency virus (HIV) ( Kaplan 2000 ). Among patients with HIV infection and PCP the mortality rate is 10% to 20% during the initial infection and this increases substantially with the need for mechanical ventilation ( Randall 2000 ). Description of the intervention Two to three days after starting anti\u2010PCP therapy, the respiratory situation of PCP patients often worsens because of increased inflammation in the lungs as a reaction to pneumocystis particles from killed organisms. Corticosteroids given in conjunction with anti\u2010PCP therapy may help to better control the inflammatory process. Therefore the corticosteroid treatment should be started as early as possible but within 72 hours after starting the PCP\u2010specific therapy. So far, there is no evidence about an optimal dose or duration of adjunctive corticosteroids. The following 21\u2010day oral regimen with prednisone has been recommended: 40 mg orally twice daily for days one to five, 40 mg once daily for days six to 10, and 20 mg once daily for days 11 to 21 ( Benson 2004 ; EACS 2013 ). If parenteral administration is necessary, it is recommended to use methylprednisolone at 75% of the respective prednisone dose ( CDC 2013 ). In children with severe PCP, it is recommended to start the corticosteroid treatment as early as possible but within 72 hours after diagnosis. The recommended dose of prednisone for children is 1 mg/kg of body weight twice daily for days one to five, 0.5 mg/kg once daily for days six to 10, and 0.5 mg/kg once daily for days 11 to 21 ( NIH 2013 ). The alternative regimen with methylprednisolone (intravenous) is 1 mg/kg/dose every six hours for days one to seven, 1 mg/kg/dose once daily for days eight to nine, 0.5 mg/kg/dose twice daily for days 10 to 11, and 1 mg/kg/dose once daily for days 12 to 16 ( NIH 2013 ). How the intervention might work Within two to three days of the initiation of anti\u2010PCP therapy, the health status of patients often worsens. It is presumed that the patient's alveolar\u2010arterial oxygen gradient and the inflammatory processes in the lungs increase as organisms are killed ( Sax 2012 ). Adjunctive corticosteroids administered with initiation of anti\u2010PCP therapy may reduce the inflammatory process and prevent this clinical worsening. Why it is important to do this review In 1990, an expert panel recommended the use of corticosteroids for HIV\u2010infected patients with PCP and substantial hypoxaemia (initial arterial oxygen partial pressure of < 70 mmHg or alveolar\u2010arterial gradient > 35 mmHg on room air) based on the evidence from five randomised controlled trials ( Consensus 1990 ). The studies used for the consensus statement still represent the basis for the current guidelines for the treatment of adults ( CDC 2013 ) and children ( NIH 2013 ). However, at the time of the first consensus statement, one trial was not yet completed ( Nielsen 1992 ), two trials had been stopped prematurely ( Gagnon 1990 ; Montaner 1990 ), and one trial was not published in full ( Clement 1989 ). In 1992, a systematic review qualitatively summarised the same incomplete data ( Sistek 1992 ).",
        "summary": "High\u2010quality evidence shows that compared with placebo or no intervention, adjunctive corticosteroids therapy decreases the overall mortality rate at one month (on average 138 vs. 247 per 1000 people die) and at three to four months (on average 152 vs.258 per 1000 people die) in HIV\u2010infected adults with Pneumocystis jiroveci pneumonia and substantial hypoxemia. Moderate\u2010quality evidence also shows that adjunctive corticosteroids decrease the need for mechanical ventilation (on average 62 vs.164 per 1000 people need to be ventilated). In a single trial of HIV\u2010infected infants with pneumonia and hypoxia, there was no apparent benefit of adjunctive corticosteroids in terms of in\u2010hospital mortality, however, this result should be taken with caution, as it is based on a very small number of children, and therefore too underpowered to detect a difference even if one was present. The review did not assess adverse effects of the adjunctive corticosteroids; these can include super\u2010infections, opportunistic infections or malignancies, hypertension, hyperglycemia, gastrointestinal bleeding or neuropsychiatric disturbances."
    },
    "CD010965": {
        "query": "What are the effects of vasopressin receptor antagonists (VRAs) for people with chronic non\u2010hypovolemic hypotonic hyponatremia?",
        "document": "Background Description of the condition Hypotonic hyponatraemia is a common condition, occurring in up to 60% of people admitted to hospitals, depending on the definition of hyponatraemia, the types of patients who are studied and the healthcare facility to which these patients are admitted ( Upadhyay 2009 ). Hypotonic hyponatraemia is usually defined as a serum sodium concentration < 135 mmol/L with an osmolality < 285 mOsm/kg ( Reynolds 2006 ). It develops when the body retains an excess of water relative to the amount of sodium. It can be caused by intrinsic kidney disease but usually results from incomplete suppression of vasopressin activity despite decreased tonicity of the plasma. In situations of decreased circulating blood volume, vasopressin release is increased in a physiologic response to maintain haemodynamic homeostasis. This occurs either with true volume depletion or with reduced effective arterial circulating volume, as seen in heart failure, liver cirrhosis or nephrotic syndrome. In the syndrome of inappropriate antidiuretic hormone secretion, the increased release of vasopressin is non\u2010haemodynamic and can have multiple causes including ectopic production of vasopressin by a variety of tumours ( Verbalis 2013 ). When plasma tonicity is low, water tends to enter the cells and causes them to swell. If blood sodium concentrations drop rapidly (within a 48 hour period), the swelling of brain cells may lead to brain oedema, brain stem herniation and eventually even death. Fortunately, when blood sodium concentrations drop more gradually, brain cells adapt to their hypo\u2010osmolar surroundings and prevent swelling by the transport of solutes from the intracellular to the extracellular compartments. As a consequence, immediate symptoms attributable to chronic hyponatraemia are usually less severe than for acute hyponatraemia ( Reynolds 2006 ). Nevertheless, people with chronic hyponatraemia have reduced attention and less stable gait than those without hyponatraemia ( Renneboog 2006 ). They fall more often and have increased risk of osteoporosis and bone fractures ( Arampatzis 2013 ; Hoorn 2011 ; Kinsella 2010 ; Renneboog 2006 ; Verbalis 2010 ). Finally, they stay in hospital longer and have an increased risk of death, even when sodium concentrations are only mildly decreased and underlying or comorbid conditions are adjusted for ( Wald 2010 ). Description of the intervention It is accepted that acute hypotonic hyponatraemia requires an immediate increase in serum sodium concentration to prevent severe neurologic complications ( Ellison 2007 ). What to do with chronic hypotonic hyponatraemia is less clear. Firstly, chronic non\u2010hypovolaemic hypotonic hyponatraemia has been treated under the assumption that increasing the sodium concentration improves important health outcomes; that patients live longer, feel better and are hospitalised less frequently. Although several observational studies have indicated an association between hyponatraemia and undesirable outcomes, it is still unclear whether correcting the hyponatraemia improves them ( Upadhyay 2009 ; Wald 2010 ). Secondly, once brain cells have adapted to their hypo\u2010osmolar environment, they become vulnerable to osmotic demyelination in case the hypo\u2010osmolar environment is restored. Although rare, osmotic demyelination is a devastating neurologic complication that may occur when the myelin sheath around pontine and extrapontine neurons breaks down after rapid rises in serum sodium concentration. It very rarely does if the increases stay below 8 to 12 mmol/L/24 h and 18 mmol/L/48 h \u2010 accepted limits depending on risk factors such as older age, malnutrition and alcohol abuse ( Adrogue 2012 ; Ellison 2007 ; Reynolds 2006 ). Treatment for chronic hypotonic hyponatraemia must balance the uncertain benefit of increasing the sodium concentration against the risk of complications due to overly rapid correction. How the intervention might work Whatever the underlying cause, chronic non\u2010hypovolaemic hypotonic hyponatraemia usually results from urine being insufficiently dilute to maintain serum osmolality within the normal range ( Adrogue 2000 ). Several treatment strategies can be used to try overcoming this ( Adrogue 2012 ; Ellison 2007 ; Verbalis 2013 ). Restriction of fluid intake aims to decrease the amount of free water needing excretion. Urea and mannitol improve electrolyte\u2010free water clearance by increasing urine osmolality and creating osmotic diuresis ( Lindner 2012 ). Loop diuretics, such as furosemide, bumetanide and ethacrynic acid, impair free\u2010water absorption in the collecting duct by reducing the hypertonicity of the renal medulla. Corticosteroids with a mineralocorticoid effect increase renal sodium retention by active reabsorption of sodium in the principal cells of the cortical collecting tubule. Demeclocycline, lithium, phenytoin and vasopressin receptor antagonists act by pharmacologically inhibiting the effect of antidiuretic hormone on the principal cells of the collecting duct, thereby limiting insertion of water channels in the luminal membrane and thus preventing free water reabsorption. Restriction of fluid intake aims to decrease the amount of free water needing excretion. Urea and mannitol improve electrolyte\u2010free water clearance by increasing urine osmolality and creating osmotic diuresis ( Lindner 2012 ). Loop diuretics, such as furosemide, bumetanide and ethacrynic acid, impair free\u2010water absorption in the collecting duct by reducing the hypertonicity of the renal medulla. Corticosteroids with a mineralocorticoid effect increase renal sodium retention by active reabsorption of sodium in the principal cells of the cortical collecting tubule. Demeclocycline, lithium, phenytoin and vasopressin receptor antagonists act by pharmacologically inhibiting the effect of antidiuretic hormone on the principal cells of the collecting duct, thereby limiting insertion of water channels in the luminal membrane and thus preventing free water reabsorption. As hyponatraemia with true volume depletion (chronic hypovolaemic hypotonic hyponatraemia) is treated by restoring volume with water and salt, we do not cover it in this review. Why it is important to do this review The benefits and harms of treatments for chronic non\u2010hypovolaemic hypotonic hyponatraemia have not been formally evaluated in a systematic review. Two systematic reviews have explored the efficacy and safety of vasopressin receptor antagonists (e.g. conivaptan, lixivaptan, satavaptan, tolvaptan) versus placebo, no treatment or fluid restriction ( Jaber 2011 ; Rozen\u2010Zvi 2010 ), but to our knowledge there has been no attempt to compare them with any other intervention or to compare any of the other interventions versus placebo or against one another. Both systematic reviews have found an early increase in serum sodium concentration, but no improvement in outcomes important to patients. Indeed, most randomised controlled trials (RCTs) have evaluated short\u2010term and surrogate outcomes only, making it difficult to adequately asses any expected benefit in the long\u2010term. Since the most recent systematic review was published, 12 additional RCTs comparing vasopressin receptor antagonists versus control have been completed, increasing the total sample size by at least 50%. Although outcomes are still mostly surrogate and short\u2010term, the largest study was terminated early due to a numeric imbalance in the number of early deaths in the experimental group ( FDA 2012 ). Two vasopressin receptor antagonists have received wide\u2010spread regulatory approval, but indications and permitted treatment durations vary among regions. Divergent interpretations of survival and harms data are a likely cause.",
        "summary": "For adults (median age 65 years) mostly with moderate chronic non\u2010hypovolemic hypotonic hyponatremia, moderate\u2010certainty evidence shows that use of VRAs improves serum sodium concentration (by 4.17 mmol/L) and increases the number of people with a response in serum sodium concentration within one to five days (72 vs 43 per 1000 people) and within six months (628 vs 252 per 1000 people). Most (23/25) trials used a placebo for comparison; one trial used usual care and another used fluid restriction. Use of VRAs may also provide benefit in terms of duration of hospital stay (on average, shorter by approximately two days with VRAs) and mental component scores on the Short Form (SF)\u201012 questionnaire (5\u2010point difference on a 100\u2010point scale), but evidence is of very low to low certainty. More people experienced thirst (absolute values not reported) and polyuria (14 vs 3 per 1000 people) with VRAs. Researchers observed no clear impact of VRAs on mortality, on cognitive function at one to six months, or on changes in physical component scores on the SF\u201012, but evidence for these outcomes is of very low to low certainty."
    }
}